{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "terminal-riding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.23.3\n",
      "PyTorch:      1.12.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import matplotlib\n",
    "# import sklearn\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from IPython.display import display\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "# print(\"pandas:     \", pd.__version__)\n",
    "# print(\"matplotlib: \", matplotlib.__version__)\n",
    "# print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"PyTorch:     \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c45c2f4d-78fb-4f0c-bc59-9f9922806260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed7e7753-2202-4b5e-9ff9-aa3cd1cbc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 项目本地数据集文件夹\n",
    "LOCAL_DATA_PATH = r\"D:\\Project-Workspace\\Python-Projects\\DataAnalysis\\local-datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-affiliation",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Pytorch-Transformer\n",
    "\n",
    "参考文档:\n",
    "+ [Pytorch tutorial -> Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "+ [Pytorch -> Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "\n",
    "这里介绍了Pytorch里 transformer 层的实现模块.\n",
    "\n",
    "主要有如下几个类：\n",
    "+ `nn.Transformer`：一步到位构建整个Transformer模型，底层使用的是下面的API\n",
    "+ `nn.TransformerEncoder`：Encoder层，包含了多个 `TransoformerEncoderLayer`\n",
    "+ `nn.TransformerDecoder`：Decoder层，包含了多个 `TransformerDecoderLayer`\n",
    "+ `nn.TransformerEncoderLayer`：单层Encoder\n",
    "+ `nn.TransformerDecoderLayer`：单层Decoder\n",
    "\n",
    "但是需要注意的是，上述各种层的**底部都是调用的`activation.py`中的`MultiheadAttention`类**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-agenda",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Layer\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention).\n",
    "\n",
    "它是`TransformerEncoderLayer`和`TransformerDecoderLayer`层中 **self-attention 层**的实现。  \n",
    "需要注意的是，pytorch源码中 `MultiheadAttention` 被放在了 Non-linear Activations 这一类中（并且是放在`activation.py`文件中的）。\n",
    "\n",
    "\n",
    "### 实例化参数\n",
    "`MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)`  \n",
    "+ `embed_dim`，整个MHA的输入/输出特征维度。  \n",
    "  **原始论文里，MHA的输入/输出特征维度可以是不一样的，但是在Pytorch的实现里，强制要求它们是一样的**（这个限制未来版本可能会取消）。\n",
    "+ `num_heads`，head数量，注意，分配到每个head的维度 = embed_dim/num_heads.\n",
    "+ `kdim`，自定义 key 中的特征维度，它是下面`forward()`方法里`key`对应的特征维度，默认下=`embed_dim`\n",
    "+ `vidm`，自定义 value 中的特征维度，`forward()`方法里`value`对应的特征维度，默认=`embed_dim`\n",
    "+ `bias`，\n",
    "+ `add_bias_kv`\n",
    "+ `add_zero_attn`\n",
    "+ `batch_first`，早期版本的pytorch中，会要求下面`forward()`方法里输入数据的各个维度顺序必须为：`(序列长度，batch_size, embedding_dimension)`，后来提供了这个`batch_first`参数，用于适应一下输入数据的维度变化.\n",
    "\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "`MultiheadAttention.forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)`\n",
    "+ `query`, `key`, `value`，MHA的三个输入矩阵——这三个输入向量会被用于生成对应的Q,K,V\n",
    "  + `query.shape` = $(L, N, E_q)$，`key.shape` = $(S, N, E_k)$，`value.shape` = $(S, N, E_v)$.   \n",
    "    + $N$ 是 batch size\n",
    "    + $E$ 是 embedding 维度，$E_q$=`embed_dim`, $E_k$=`kdim`, $E_v$=`vdim`\n",
    "    + $L$ 是**target**序列的长度——因为它决定了MHA输出的序列长度\n",
    "    + $S$ 是**source**序列的长度\n",
    "  + Transformer中MHA会**在3个地方使用，这3个地方会影响 query, key, value 的内容**\n",
    "    + Encoder中 query, key, value 都是来自于样本的**输入序列**，此时 $L=S$，`kdim == vdim`\n",
    "    + Decoder的 **Masked MHA** 中 query, key, value 都来自于样本的**输出序列**，此时 $L=S$, `kdim == vdim`\n",
    "    + Decoder的 **decoder-encoder attention**中，query 来自于样本的**输出序列**，key, value 都是**Encoder最后一层的输出**，此时可能 $L\\neq S$, `kdim != vdim`\n",
    "  + **有关序列维度的说明**\n",
    "    + 输入的 `query`, `key`, `value` 不是self-attention的输入，它们需要经过各自的投影矩阵 $W_Q \\in (E_q, E_q)$, $W_K \\in (E_d, E_k)$, $W_V \\in (E_d, E_v)$ 的转换，上面这3个参数矩阵的维度，就是MHA里初始化的维度\n",
    "    + 线性投影过程如下（这里把batch的维度提前了）：   \n",
    "    $q = query * W_Q^T \\in (N, L, E_d)$,   \n",
    "    $k = key * W_K^T \\in (N, S, E_d)$,   \n",
    "    $v = value * W_V^T \\in (N, S, E_d)$  \n",
    "    + 可以看出，参数的 `kdim` 和 `vdim` 只是用于设置投影矩阵 $W_K, W_V$ 的维度，这两个矩阵的另一个维度被设置成了 $E_q$=`embed_dim`，经过上述三个投影矩阵的变换之后，`query`, `key`, `value` 的维度都变成了 $E_q$，也就是 `embed_dim`\n",
    "    + 后续进行的MHA操作为：\n",
    "    $${\\rm softmax}(\\frac{q*k^T}{\\sqrt{E_d}}) * v$$\n",
    "    $$((N, L, E_d) * (N, E_d, S)) * (N, S, E_d) \\rightarrow (N, L, E_d)$$\n",
    "    调用的是`torch.bmm()`算子，会在每个批次上做上述操作。\n",
    "    + 可以看出，最终决定输出序列长度的是 `query` 的序列长度 $L$，而 `key` 和 `value` 的序列长度必须一致，它们会在中间计算过程中相互抵消掉。\n",
    "\n",
    "\n",
    "+ `key_padding_mask`：填充位置的掩码，shape=$(N, S)$   \n",
    "  + 对于一个batch的训练数据来说，batch size 为 $N$ 表示有 $N$ 个序列，但是**每个序列的长度（也就是单词个数）是不一样的**，`key`,`value`中的 $S$ 都是指最长序列的长度，其他的序列就需要对缺少的单词位置做填充(padding)，padding位置的词通常使用全为0的embeding 来表示。   \n",
    "  + 为了不让MHA对这些位置的词产生注意力，需要记录这些padding位置，每个batch都使用这样一个 $(N, S)$ 的矩阵来标识padding的位置，该矩阵的每一行对应batch中的一个序列，值为`[False, False, True, True]` 或者 `[0, 0, 1, 1]` 的形式，其中结尾`True`或者1标识的就是padding的位置。   \n",
    "  + 需要注意的是，这里padding矩阵实际生效位置是在`query`和`key`做完矩阵乘法得到shape=$(N,L,S)$的中间矩阵 P 之后才使用的，由于只有`key`使用了padding掩码，只需要在 $N$ 个序列中对 $S$ 这一个维度标识padding位置即可，所以使用的padding矩阵shape=$(N,S)$. pytorch中的实现就是对中间矩阵 P 的 $S$ 维度，按照 `[False,False,True,True]` 中的标记，对应`True`的位置赋值为`-inf`，从而在后续的softmax中，该位置得到的概率为0.\n",
    "\n",
    "\n",
    "+ `attn_mask`，**只在Decoder中使用**，主要是解码时不让当前位置的词和后面要预测的词序列进行Attention.   \n",
    "  + shape=$(L,S)$ 或者 $(N * num\\_heads,L,S)$，如果是 2D 的矩阵，那么batch中的每个序列都会使用这个，如果是 3D 的矩阵，则会指定每个batch 的掩码\n",
    "  + 它的生效位置也是在`query`和`key`相乘得到shape=$(N,L,S)$的中间矩阵 P 之后使用，此时batch中**每个**序列的输入`query`部分（shape=$(L,E)$）和`key`部分（shape=$(S,E)$）相乘得到了一个 $(L,S)$ 的矩阵 P，矩阵中列表示`query`里的每个词，行表示`key`中的每个词，如果不想让`query`中第 $i$ 个词看到`key`中第 $j$ 个词之后的内容，只需要让 `P[i, j:]` 这部分为 `-inf` 即可，因此这个掩码的shape=$(N,L,S)$.\n",
    "  + 如果是在Decoder中的**Masked MHA**这里使用，此时输入的`query`,`key`,`value`都是来源于同一个地方，`query`和`key`维度相等，此时 P 是一个方阵，并且右上三角均为`-inf` —— 这是好理解的部分，也是正常它要解决的问题部分。\n",
    "  + 如果是在Decoder中的**encoder-decoder attention**层使用，此时输入的`query`是来自于之前的Masked MHA层，属于输出序列的内容，但是`key`,`value`是来自于Encoder，属于输入序列的内容，此时才会出现`query`和`key`长度不等的情况——这种情况下虽然也可以使用掩码，但是它的含义就不那么好解释了。\n",
    "\n",
    "\n",
    "+ `need_weights`，bool，表示是否需要返回`attn_output_weigths`，默认为`True`\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值\n",
    "  + `attn_output`，shape= $(L, N, E_q)$，$L$ 由 输入的`query`序列长度决定，$E$=`embed_dim`.\n",
    "  + `attn_output_weights`，shape=$(N, L, S)$，这个权重应该是每个输入序列中，每个word的attention权重.\n",
    "  \n",
    "\n",
    "### 一些说明\n",
    "\n",
    "Pytorch中的Multi-Head Attention的实现和论文《All you need is transformer》中有一些不一样的地方，总结如下：\n",
    "\n",
    "+ pytorch在实现MHA时，强制要求了输入的特征维度和输出的特征维度必须一致，也就是实例化时的`embed_dim`参数，但是在原论文中没有这个限制。   \n",
    "原论文中，输入的特征维度是`query`里的embed_dim，输出的特征维度由`value`的embed_dim决定。\n",
    "\n",
    "+ pytorch中MHA实例化参数 `kdim`,`vdim` 和含义和原论文里的 $d_k$, $d_v$ 不一样，如下所示.   \n",
    "原论文中的 $d_k$, $d_v$ 指的是 query, key 经过参数矩阵 $W$ 变换之后的 embed_dim，而pytoch中的 `kdim`,`vdim` 指定的是输入的 `key`,`value` 中，没有经过 $W$ 变换之前的特征维度，$W$ 这个参数矩阵设置的时候，其中一个维度就是 `kdim` （或者 `vdim`），另一个维度就是 `embed_dim`，因此 `key`, `value` 经过 $W$ 变换之后，维度就变成了 `embed_dim` ——这是 pytorch强制规定的。\n",
    "\n",
    "<img src=\"images/multi-head-attention-formula.png\" width=\"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8b87b-8c61-4978-ae21-ad91a3dbed01",
   "metadata": {},
   "source": [
    "### 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "permanent-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim, num_heads = 512, 8\n",
    "batch_size, target_len, source_len = 5, 10, 20\n",
    "\n",
    "query = torch.rand(target_len, batch_size, embed_dim)\n",
    "key = torch.rand(source_len, batch_size, embed_dim)\n",
    "value = torch.rand(source_len, batch_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunrise-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 512])\n",
      "torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "print(attn_output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "civilian-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 512])\n",
      "torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "kdim, vdim = 256, 256\n",
    "key = torch.rand(source_len, batch_size, kdim)\n",
    "value = torch.rand(source_len, batch_size, vdim)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, kdim=kdim, vdim=vdim)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "print(attn_output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c975d9-a8ce-4564-be5a-d8f263baa926",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoder Layer\n",
    "\n",
    "### 单层Encoder\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)，它内部包含了一个 self-attention 层 + 一个 feedforward 层.\n",
    "\n",
    "`nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`  \n",
    "\n",
    "+ 实例化参数\n",
    "  + `d_model`，输入序列中，每个word的特征个数——它同时也决定了输出序列里每个word的特征个数\n",
    "  + `nhead`，multiheadattention中的head个数\n",
    "  + `dim_feedforward`，这个维度设置的是前馈神经网络的节点个数，前馈神经网络的输出节点数还是 `d_model`\n",
    "  + `dropout`，dropout比例，默认 0.1  \n",
    "\n",
    "\n",
    "+ 前向传播`forward(src, src_mask=None, src_key_padding_mask=None)`\n",
    "  + `src`，输入的sequence, `shape` = $(S, N, E)$\n",
    "  + `src_mask`，输入sequence的mask\n",
    "  + `src_key_padding_mask`，每个batch中src的padding矩阵  \n",
    "\n",
    "\n",
    "+ `forward()`方法返回值  \n",
    "  它返回的是和 `src` shape 相同的 tensor.\n",
    "\n",
    "\n",
    "Encoder的内部只有**一层**`MultiheadAttention`：  \n",
    "`Encoder.forward()`实际执行时，调用方式为`MultiheadAttention.forward(src, src, src)`——`src`会同时作为 query, key, value 传入 self-attention。\n",
    "\n",
    "\n",
    "\n",
    "### 多层Encoder\n",
    "\n",
    "`TransformerEncoder(encoder_layer, num_layers, norm=None)`\n",
    "+ 参数：\n",
    "  + `encoder_layer`，是一个`TransformerEncoderLayer`类的实例对象\n",
    "  + `num_layers`，指定堆叠的 Encoder 层数\n",
    "+ `forward()`方法和`TransformerEncoderLayer`的一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variable-jerusalem",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape:  torch.Size([32, 10, 512])\n",
      "out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model, num_heads, dim_ffn = 512, 8, 2048\n",
    "batch_size, source_len = 10, 32\n",
    "src = torch.rand(source_len, batch_size, d_model)\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=dim_ffn)\n",
    "out = encoder_layer(src)\n",
    "\n",
    "print(\"src.shape: \", src.shape)\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fed3cba-eca8-46d4-b6e5-df9ad8d636a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape:  torch.Size([32, 10, 512])\n",
      "out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "multi_encoder_layer = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=3)\n",
    "out = multi_encoder_layer(src)\n",
    "\n",
    "print(\"src.shape: \", src.shape)\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be2b79-8ce5-439d-88ba-f2837babc56f",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "\n",
    "### 单层Decoder\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer)\n",
    "\n",
    "`TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`   \n",
    "\n",
    "+ 实例化参数：和 Encoder 一致\n",
    "  + `d_model`，输入序列中，每个word的特征个数——它同时也决定了输出序列里每个word的特征个数\n",
    "  + `nhead`，multiheadattention中的head个数\n",
    "  + `dim_feedforward`，这个维度设置的是前馈神经网络的节点个数，前馈神经网络的输出节点数还是 `d_model`\n",
    "  + `dropout`，dropout比例，默认 0.1\n",
    "\n",
    "\n",
    "+ 前向传播`forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)`  \n",
    "  这个方法和 Encoder 不一样，它多了一个 memory. \n",
    "  + `tgt`，输入的序列，`shape`= $(S, N, E)$，\n",
    "  + `memory`，来自 encoder 层（通常是最后一层）的输出, `shape`=$(S, N, E)$.\n",
    "  + `tgt_mask`，\n",
    "  + `memory_mask`\n",
    "  + `tgt_key_padding_mask`，\n",
    "  + `memory_key_padding_mask`\n",
    "\n",
    "> `Decoder`内部会调用两层`MultiheadAttention`\n",
    "> 1. 第一层调用时为`MultiheadAttention.forward(tgt, tgt, tgt)`\n",
    "> 2. 第二层调用时为`MultiheadAttention.forward(tgt, memory, memory)`\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值  \n",
    "它返回的是和 `tgt` shape 相同的 tensor，\n",
    "\n",
    "\n",
    "### 多层Decoder\n",
    "\n",
    "`TransformerDecoder(decoder_layer, num_layers, norm=None)`   \n",
    "参数同 `TransformerEncoder` 类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coated-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape:  torch.Size([32, 10, 512])\n",
      "memory.shape:  torch.Size([32, 10, 512])\n",
      "decoder_out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model, nhead, dim_ffn = 512, 8, 2048\n",
    "batch_size, source_len = 10, 32\n",
    "tgt = torch.rand(source_len, batch_size, d_model)\n",
    "memory = torch.rand(source_len, batch_size, d_model)\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ffn)\n",
    "decoder_out = decoder_layer(tgt, memory)\n",
    "\n",
    "print(\"tgt.shape: \", tgt.shape)\n",
    "print(\"memory.shape: \", memory.shape)\n",
    "print(\"decoder_out.shape: \", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13f1996-8cdc-4d6d-9aa2-20ab185944ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape:  torch.Size([32, 10, 512])\n",
      "memory.shape:  torch.Size([32, 10, 512])\n",
      "decoder_out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "multi_decoder_layer = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=3)\n",
    "decoder_out = multi_decoder_layer(tgt, memory)\n",
    "\n",
    "print(\"tgt.shape: \", tgt.shape)\n",
    "print(\"memory.shape: \", memory.shape)\n",
    "print(\"decoder_out.shape: \", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8397f18f-d31a-4472-bd28-1c61e3767e47",
   "metadata": {},
   "source": [
    "## Transformer类\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer)\n",
    "\n",
    "`nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=<function relu>, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None)`\n",
    "\n",
    "+ 实例化参数\n",
    "  + `d_model`，输入/输出的embedding dim，默认512\n",
    "  + `nhead`\n",
    "  + `num_encoder_layers`\n",
    "  + `num_decoder_layers`\n",
    "  + `dim_feedforward`\n",
    "  + `dropout`\n",
    "  + `activation`\n",
    "  \n",
    "+ 正向传播`forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)`\n",
    "  + `src`，shape=$(S,N,E)$\n",
    "  + `tgt`，shape=$(T,N,E)$\n",
    "  + `src_mask`，shape=$(S,S)$\n",
    "  + `tgt_mask`，shape=$(T,T)$\n",
    "  + `memory_mask`，shape=$(T,S)$\n",
    "  + `src_key_padding_mask`，shape=$(N,S)$\n",
    "  + `tgt_key_padding_mask`，shape=$(N,T)$\n",
    "  + `memory_key_padding_mask`，shape=$(N,S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dfc62-cc2a-4dc0-a87b-91d75927bc3a",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "# Huggingface\n",
    "\n",
    "[Huggingace公司](https://huggingface.co/docs)（下面简称HF）提供了围绕Transformer架构的一系列工具，我常用的有如下几个部分：\n",
    "+ [Datasets](https://huggingface.co/docs/datasets/index)，提供了一系列的数据集封装，方便下载常用的训练数据，同时也封装了一些数据预处理操作\n",
    "+ [Tokenizer](https://huggingface.co/docs/tokenizers/index)，为NLP提供了分词处理的pipeline，底层是由Rust实现的，分词速度非常快，适用于生产环境\n",
    "+ [Transformer](https://huggingface.co/docs/transformers/index)，基于transformer架构实现的一系列模型，比如BERT等\n",
    "\n",
    "\n",
    "此外，HF还提供了许多详细的文档资源：\n",
    "+ **[Course](https://huggingface.co/course/chapter0/1?fw=pt) ——KEY**，HF提供的一个教程，详细介绍了如何使用上述HF-ecosystem的各种package，这个很有用\n",
    "+ [Models](https://huggingface.co/models)，模型仓库，提供了一系列任务的预训练模型，可以配合`transformer`包使用\n",
    "+ [Dataset](https://huggingface.co/datasets)，数据集仓库，提供许多用于训练的数据集，可以配合`datasets`包使用\n",
    "\n",
    "\n",
    "下面是对上述我常用的3个包的整理总结."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65566a18-b616-474b-b6da-e3596dcc94fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "## Dataset\n",
    "\n",
    "`datasets`包可以方便的下载常用的数据集，并且封装了一些数据处理操作.\n",
    "\n",
    "详细介绍可以参考：\n",
    "+ github仓库 [datasets](https://github.com/huggingface/datasets)\n",
    "+ 官方文档 [Datasets](https://huggingface.co/docs/datasets/index)\n",
    "+ 官方教程文档 [Course --> The datasets library](https://huggingface.co/course/chapter5/1?fw=pt)：这个作为入门介绍比较容易接受\n",
    " \n",
    "huggingface官方提供了一个数据集的托管共享平台：[datasets hub](https://huggingface.co/datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4daba1f-6aa0-4302-8381-456230c904a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### dataset介绍\n",
    "\n",
    "这部分的内容主要来自于官方文档 [datasets: Conceptual Guides](https://huggingface.co/docs/datasets/about_arrow).\n",
    "\n",
    "**一个dataset对应于一个文件夹**（[Conceptual Guides --> Build and load](https://huggingface.co/docs/datasets/about_dataset_load)），其中主要包含了下面两部分内容：\n",
    "1. 数据集本身，通常以JSON，csv，parquet等格式存储\n",
    "2. 处理数据集的Python脚本——**可选** \n",
    "\n",
    "一般datasets-hub上提供的数据集，基本都有脚本，**脚本中会从URL下载原始数据集，进行一些数据处理，然后生成便于使用的数据集格式（Arrow格式）**，并且此脚本会返回一个`DatasetBuilder`对象，记录数据集的元数据信息。\n",
    "\n",
    "datasets中提供的数据一般会以Apache-Arrow的格式存储，这是一种常用于存储大数据的列存储格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db8857-86f2-41f3-80b3-83ae968c1fbf",
   "metadata": {},
   "source": [
    "### 数据集加载\n",
    "\n",
    "参考文档 [datasets: Tutorials --> Load a dataset from the Hub](https://huggingface.co/docs/datasets/load_hub#load-a-dataset-from-the-hub).\n",
    "\n",
    "`datasets` 模块提供了如下的函数用于加载数据：\n",
    "\n",
    "+ `list_datasets()`，以list的形式列出可用的数据集（实际上是处理数据集的脚本），不过**推荐直接去 dataset-hub 里查找**，因为数据集实在是太多了，这个返回结果会很大，很卡。\n",
    "\n",
    "+ `load_dataset_builder()`，加载数据集的builder\n",
    "  + 内部实例化一个`dataset_module_factory`对象，**会触发下载数据集中的Python脚本和元数据的操作，但不会去下载实际的数据集**\n",
    "  + 返回对应的`DatasetBuilder`对象，记录数据集的元数据信息\n",
    "  + **这个是最基础的函数，下面的大部分函数都会先调用这个**\n",
    "\n",
    "\n",
    "+ `get_dataset_config_info()`，加载数据集的配置信息\n",
    "  + 内部调用`load_dataset_builder()`\n",
    "  + 返回`DatasetBuilder`对象的`.info`属性   \n",
    "\n",
    "\n",
    "+ `get_dataset_config_names()`，返回数据集的配置信息\n",
    "  + 有些数据集会有**多个类型的子数据集(sub-datasets)，**这些子数据集被称为 *configurations*，可以用这个函数查看指定数据集含有哪些子数据集\n",
    "  + 内部也会实例化一个`dataset_module_factory`对象    \n",
    "\n",
    "\n",
    "+ `get_dataset_infos()`，查看数据集信息.  \n",
    "  + 内部调用`get_dataset_config_names()`\n",
    "  + 返回一个dict         \n",
    "\n",
    "\n",
    "+ `get_dataset_split_names()`，获取数据集中split信息，**一个split就是数据集的子集，比如训练集，测试集等**\n",
    "  + 内部调用`get_dataset_config_info()`         \n",
    "\n",
    "\n",
    "+ **`load_dataset()`——KEY**，最重要的函数，内部会执行如下几件事（参见[接口文档](https://huggingface.co/docs/datasets/package_reference/loading_methods#datasets.load_dataset)）：\n",
    "  1. 调用`load_dataset_builder()`获取数据集的元数据和Python脚本\n",
    "  2. 从Python脚本中的URL下载原始数据集，并进行处理             \n",
    "\n",
    "\n",
    "\n",
    "`load_dataset()`接受的参数如下（大部分参数也可用于`load_dataset_builder()`）\n",
    "+ `path`：数据集的path或者name，有如下几种情况：\n",
    "  + 本地路径：\n",
    "    + 文件夹中只有数据文件，则基于文件类型(csv,txt,json等)使用通用的builder，包含文件夹下的所有的数据\n",
    "    + 指向从huggingface-hub中已经下载好的对应数据集的处理脚本，则会运行该脚本，创建对应的builder对象\n",
    "  + 数据集名称：\n",
    "    + 指向huggingface-hub中的数据名称，然后按照上面的方式处理，如果hub上的该数据只有数据，就使用通用的builder，如果含有处理脚本，就使用对应的处理脚本\n",
    "    \n",
    "  注意，**如果是指向数据处理脚本，就不会再下载数据集的元数据信息了**\n",
    "  \n",
    "+ `name`：指定 data configuration 的名称\n",
    "+ `data_dir`：指向 data configuration 的位置\n",
    "+ `data_files`：指向数据文件的位置\n",
    "+ `cache_dir`：缓存位置，默认为 `~/.cache/huggingface/datasets`\n",
    "\n",
    "+ `split`：获取数据集的哪个子集\n",
    "  + 不指定时，返回该数据集的所有split，封装成一个`DatasetDict`\n",
    "  + 指定时，返回的是`Dataset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a366e8b7-1c66-446b-9518-8298d21e596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder, get_dataset_config_names, get_dataset_config_info, \\\n",
    "    get_dataset_infos, get_dataset_split_names,  load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b911c6-797c-4369-a6c1-99b365311f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回的 list 太长了，其实没啥用\n",
    "# datasets_list = list_datasets()\n",
    "# print(len(datasets_list))\n",
    "# datasets_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855439bc-2f58-47fc-ab23-deeaa7c59051",
   "metadata": {},
   "source": [
    "#### 加载在线数据集\n",
    "\n",
    "这里以 wikitext 这个数据集为例，该数据集包含多个类型的子数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9877a6f-7c4b-4898-af70-17933324dc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 如果只使用如下的数据集名称，那么第一次会从 huggingface-hub 下载该数据集的元数据信息和数据处理脚本（wikitext.py）\n",
    "# 后续操作虽然不用再次下载数据处理脚本，但是也会联网，速度比较慢\n",
    "# path = 'wikitext'\n",
    "\n",
    "# 推荐的操作是，下载了数据处理脚本之后，path 直接指定本地的数据脚本，这样后续的各种操作会快很多\n",
    "path = os.path.join(LOCAL_DATA_PATH, r'huggingface\\wikitext.py')\n",
    "print(os.path.exists(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01ce8d65-2815-40b4-aa2b-a6a49cff2cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikitext-103-v1',\n",
       " 'wikitext-2-v1',\n",
       " 'wikitext-103-raw-v1',\n",
       " 'wikitext-2-raw-v1']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不要一开始就直接查看 builder，会报错，因为该数据集中有多个子数据集配置，所以还需要指定子数据集的配置\n",
    "# builder = load_dataset_builder(path)\n",
    "\n",
    "# 先查看该数据集有哪些配置\n",
    "# 这里可能会下载数据集的处理脚本和元数据信息\n",
    "get_dataset_config_names(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b12205-005f-4754-b587-5d18a980793a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wikitext-103-v1': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\\n License.\\n', citation='@misc{merity2016pointer,\\n      title={Pointer Sentinel Mixture Models},\\n      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},\\n      year={2016},\\n      eprint={1609.07843},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='wikitext', config_name='wikitext-103-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1295579, num_examples=4358, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=545142639, num_examples=1801350, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1154755, num_examples=3760, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip': {'num_bytes': 190229076, 'checksum': '242ba0f20b329cfdf1ccc61e9e9e5b59becf189db7f7a81cd2a0e2fc31539590'}}, download_size=190229076, post_processing_size=None, dataset_size=547592973, size_in_bytes=737822049), 'wikitext-2-v1': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\\n License.\\n', citation='@misc{merity2016pointer,\\n      title={Pointer Sentinel Mixture Models},\\n      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},\\n      year={2016},\\n      eprint={1609.07843},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='wikitext', config_name='wikitext-2-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1270951, num_examples=4358, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=10918134, num_examples=36718, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1134127, num_examples=3760, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip': {'num_bytes': 4475746, 'checksum': '92675f1d63015c1c8b51f1656a52d5bdbc33aafa60cc47a218a66e7ee817488c'}}, download_size=4475746, post_processing_size=None, dataset_size=13323212, size_in_bytes=17798958), 'wikitext-103-raw-v1': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\\n License.\\n', citation='@misc{merity2016pointer,\\n      title={Pointer Sentinel Mixture Models},\\n      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},\\n      year={2016},\\n      eprint={1609.07843},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='wikitext', config_name='wikitext-103-raw-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1305092, num_examples=4358, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=546501673, num_examples=1801350, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1159292, num_examples=3760, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip': {'num_bytes': 191984949, 'checksum': '91c00ae287f0d699e18605c84afc9e45c192bc6b7797ff8837e5474655a33794'}}, download_size=191984949, post_processing_size=None, dataset_size=548966057, size_in_bytes=740951006), 'wikitext-2-raw-v1': DatasetInfo(description=' The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\\n Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\\n License.\\n', citation='@misc{merity2016pointer,\\n      title={Pointer Sentinel Mixture Models},\\n      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},\\n      year={2016},\\n      eprint={1609.07843},\\n      archivePrefix={arXiv},\\n      primaryClass={cs.CL}\\n}\\n', homepage='https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/', license='Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)', features={'text': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='wikitext', config_name='wikitext-2-raw-v1', version=1.0.0, splits={'test': SplitInfo(name='test', num_bytes=1305092, num_examples=4358, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=11061733, num_examples=36718, dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1159292, num_examples=3760, dataset_name='wikitext')}, download_checksums={'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip': {'num_bytes': 4721645, 'checksum': 'ef7edb566e3e2b2d31b29c1fdb0c89a4cc683597484c3dc2517919c615435a11'}}, download_size=4721645, post_processing_size=None, dataset_size=13526117, size_in_bytes=18247762)}\n"
     ]
    }
   ],
   "source": [
    "# 也可以使用下面这个方法查看数据集信息，但是返回的内容比较多，速度也比较慢\n",
    "info = get_dataset_infos(path)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5969e0de-ca31-4860-af24-248cacfe2ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'validation']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据有哪些分割子集，必须要带上配置信息，也就是具体的子数据集\n",
    "get_dataset_split_names(path, 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14c14020-8f21-4b91-9efc-c82b648df342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder.__class__:  <class 'datasets_modules.datasets.wikitext.a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126.wikitext.Wikitext'>\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集的 Builder，这里也要带上配置信息\n",
    "builder = load_dataset_builder(path, name='wikitext-2-raw-v1')\n",
    "print('builder.__class__: ', type(builder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8617c750-f26d-4b96-b1dd-3777dbef06fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder.name:\n",
      "wikitext\n",
      "----------------\n",
      "builder.config_id:\n",
      "wikitext-2-raw-v1\n",
      "----------------\n",
      "builder.config:\n",
      "WikitextConfig(name='wikitext-2-raw-v1', version=1.0.0, data_dir=None, data_files=None, description='Raw level dataset: the raw tokens before the addition of <unk> tokens. They should only be used for character level work or for creating newly derived datasets.')\n",
      "----------------\n",
      "builder.cache_dir:\n",
      "C:\\Users\\Daniel-ZHANG\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\n"
     ]
    }
   ],
   "source": [
    "# 查看 Builder 的各种属性\n",
    "print('builder.name:')\n",
    "print(builder.name)\n",
    "print('----------------')\n",
    "print('builder.config_id:')\n",
    "print(builder.config_id)\n",
    "print('----------------')\n",
    "print('builder.config:')\n",
    "print(builder.config)\n",
    "print('----------------')\n",
    "print('builder.cache_dir:')\n",
    "print(builder.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab6db96a-e6cb-4f32-975d-fbbb73273272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builder.info.__class__: \n",
      "<class 'datasets.info.DatasetInfo'>\n",
      "-----------------------\n",
      "builder.info.builder_name:\n",
      "wikitext\n",
      "-----------------------\n",
      "builder.info.config_name:\n",
      "wikitext-2-raw-v1\n",
      "-----------------------\n",
      "builder.info.description:\n",
      " The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\n",
      " Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\n",
      " License.\n",
      "\n",
      "-----------------------\n",
      "builder.info.features:\n",
      "{'text': Value(dtype='string', id=None)}\n",
      "-----------------------\n",
      "builder.info.dataset_size:\n",
      "13526093\n",
      "-----------------------\n",
      "builder.info.download_size:\n",
      "4721645\n",
      "-----------------------\n",
      "builder.info.splits:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test': SplitInfo(name='test', num_bytes=1305088, num_examples=4358, dataset_name='wikitext'),\n",
       " 'train': SplitInfo(name='train', num_bytes=11061717, num_examples=36718, dataset_name='wikitext'),\n",
       " 'validation': SplitInfo(name='validation', num_bytes=1159288, num_examples=3760, dataset_name='wikitext')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 Builder 的 info信息\n",
    "print(\"builder.info.__class__: \")\n",
    "print(builder.info.__class__)\n",
    "print('-----------------------')\n",
    "\n",
    "# 查看数据集描述信息\n",
    "print('builder.info.builder_name:')\n",
    "print(builder.info.builder_name)\n",
    "print('-----------------------')\n",
    "print('builder.info.config_name:')\n",
    "print(builder.info.config_name)\n",
    "print('-----------------------')\n",
    "print('builder.info.description:')\n",
    "print(builder.info.description)\n",
    "print('-----------------------')\n",
    "print('builder.info.features:')\n",
    "print(builder.info.features)\n",
    "print('-----------------------')\n",
    "print('builder.info.dataset_size:')\n",
    "print(builder.info.dataset_size)\n",
    "print('-----------------------')\n",
    "print('builder.info.download_size:')\n",
    "print(builder.info.download_size)\n",
    "print('-----------------------')\n",
    "print('builder.info.splits:')\n",
    "builder.info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a9191d-6ebd-48b1-aec6-b15b9c707102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb7a55060094e11acd459826d873880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b44763a5224b74ac55caa953823f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3040de615d44ad5a834b990070dd08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f9eaacbd2344a0bbf53fe766540d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikitext downloaded and prepared to C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038fefa70f964070ae81386e6d2b5801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 不指定数据集的 split 时，会加载所有的 split，返回 DatasetDict\n",
    "# 如果 cache_dir 里没有数据，那么这里第一次会下载数据集\n",
    "data = load_dataset(path, 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "758471b1-a93f-4775-bd24-79d95ccaf8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e29036e98174637973c891823cb31fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(path, 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8a6ed56-c240-4703-81cf-04f6c57df2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.__class__:  <class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"data.__class__: \", data.__class__)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bfcd449-4f45-4d32-81d6-b56b7e059a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定加载数据集的某个split\n",
    "data_train = load_dataset(path, 'wikitext-2-raw-v1', split='train')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d71c77a8-a77c-4caf-a29d-c5cc32cb2a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\\\wikitext-train.arrow'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.cache_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8a243-4b9b-4523-8ee6-d18ea6745cb7",
   "metadata": {},
   "source": [
    "#### 加载本地自定义数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26ea853f-745d-4dac-a3ca-6c900a737f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "custom_data_path = os.path.join(LOCAL_DATA_PATH, 'rankingcard.csv')\n",
    "print(os.path.exists(custom_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97f81557-0344-4dcc-9f0e-6dcde6f0b241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines  age  \\\n",
       "0      1                 1                              0.766127   45   \n",
       "1      2                 0                              0.957151   40   \n",
       "2      3                 0                              0.658180   38   \n",
       "3      4                 0                              0.233810   30   \n",
       "4      5                 0                              0.907239   49   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                     2   0.802982         9120.0   \n",
       "1                                     0   0.121876         2600.0   \n",
       "2                                     1   0.085113         3042.0   \n",
       "3                                     0   0.036050         3300.0   \n",
       "4                                     1   0.024926        63588.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                               13                        0   \n",
       "1                                4                        0   \n",
       "2                                2                        1   \n",
       "3                                5                        0   \n",
       "4                                7                        0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                             6                                     0   \n",
       "1                             0                                     0   \n",
       "2                             0                                     0   \n",
       "3                             0                                     0   \n",
       "4                             1                                     0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(custom_data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9fded0f-d5bc-4be8-8322-564c2242d59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-30075934ae74f7bd\n",
      "Found cached dataset csv (C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/csv/default-30075934ae74f7bd/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9fa5ff8fee4749b69c6c77b4838415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 下面会在 custom_data_path 路径下自动创建一个同名的文件夹\n",
    "ds = load_dataset('csv', data_files=custom_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "966e29dc-849f-4f19-9141-6aa08d60ca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'SeriousDlqin2yrs', 'RevolvingUtilizationOfUnsecuredLines', 'age', 'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b9d01-c4e7-4490-bb08-eb37ae27e561",
   "metadata": {},
   "source": [
    "#### 手动创建\n",
    "\n",
    "指的是从pandas.DataFrame或者dict对象中手动创建。\n",
    "\n",
    "此时可以调用`Dataset`类的如下类方法：\n",
    "+ `from_dataframe()`\n",
    "+ `from_dict()`\n",
    "+ `from_generator()`\n",
    "+ `from_parquet()`\n",
    "+ `from_list()`\n",
    "+ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efbee8d4-0f89-4135-a0ae-8fc7480d20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'col-1': [1, 2, 3],\n",
    "    'col-2': ['a', 'b', 'c']\n",
    "}\n",
    "data_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efc7d291-ef47-45f6-9e9b-4ca9b033dbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['col-1', 'col-2'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1 = Dataset.from_dict(data_dict)\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e07b990e-2787-45e2-a0de-f3eb0e4b5829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['col-1', 'col-2'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = Dataset.from_pandas(data_df)\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00cd677d-36ef-4d3c-b3f7-7ee8615b3a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryTable\n",
       "col-1: int64\n",
       "col-2: string\n",
       "----\n",
       "col-1: [[1,2,3]]\n",
       "col-2: [[\"a\",\"b\",\"c\"]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f4f2f34-b4d7-4b03-9144-b94765f6e47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1.cache_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec45a95-c7e5-443a-9eff-df411063161e",
   "metadata": {},
   "source": [
    "### 数据集处理\n",
    "\n",
    "\n",
    "上面加载数据集后，返回的 `Dataset` 对象，提供了一系列操作数据和处理数据的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6f7a69e-a97f-4be9-b6d0-1356fba26a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Daniel-ZHANG/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join(LOCAL_DATA_PATH, r'huggingface\\wikitext.py')\n",
    "builder = load_dataset_builder(path, name='wikitext-2-raw-v1')\n",
    "data_train = load_dataset(path, 'wikitext-2-raw-v1', split='train')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d90e1d-8114-4d96-bc1b-af01cfc7614c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92778b2b-f28d-4688-b2d1-6d37b85f2479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36718"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a50d847-b329-4394-811e-aa3a2c154590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f3843c2-5904-4ef5-a7b1-dfc6feffc2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffca2fa6-ef3d-424f-961f-d5a3f1449973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85113321-ff2d-4891-997b-bbcf9468a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.table.MemoryMappedTable"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31696ef7-aa4f-4d24-a955-822cd1b4e788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "item = data_train[0]\n",
    "print(item.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f5aea-38f2-44fa-afab-f9686c3cd1f2",
   "metadata": {},
   "source": [
    "#### 数据集切片、选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b5d3246-0871-4d1c-a307-6479d2cb471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取第 1 个样本\n",
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fdd15344-8a2e-47c0-911d-129c85ef8aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "36718\n"
     ]
    }
   ],
   "source": [
    "# 获取 text 特征\n",
    "text = data_train['text']\n",
    "print(text.__class__)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "abd75e01-c07b-4d0a-8db9-5629462353aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "19be5866-5549-40d3-96b5-b215ea92d6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' = Valkyria Chronicles III = \\n',\n",
       "  '',\n",
       "  ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       "  \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接切片\n",
    "data_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515e309-0fba-4c93-96ed-daf88c27ebe9",
   "metadata": {},
   "source": [
    "#### Map操作\n",
    "\n",
    "支持以自定义函数 按行 或者 按batch 的方式，处理数据集."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f93fd48-e0bb-4dd5-904d-7f360bd69877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ed6d7-f341-4fb6-b6ed-fd182a877551",
   "metadata": {},
   "source": [
    "+ 按行 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3e014b-8363-450a-807a-e8d6c35b54d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc7496266ee46e99b2b0fe7dc4aa26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36718 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 只取每个样本的前10个字符\n",
    "def example_head(example):\n",
    "    example['head'] = example['text'][:5]\n",
    "    return example\n",
    "\n",
    "ds_1 = data_train.map(example_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64085e11-e4f4-41db-ba43-d7ab904e24c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fd935f9-2bcb-45d9-9158-2554e21bd632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['', ' = Valkyria Chronicles III = \\n', ''],\n",
       " 'head': ['', ' = Va', '']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7fd09-40ac-4615-aba7-12c64cb4c1e6",
   "metadata": {},
   "source": [
    "+ 按batch 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58d85897-2234-4b54-a481-be8446bbe07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31688bcf74694e66b23bb0bb21b28dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Batch'>\n",
      "3\n",
      "-------------------------------------------\n",
      "<class 'datasets.arrow_dataset.Batch'>\n",
      "3\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def example_chunk(examples):\n",
    "    # examples 现在是一个 batch 的数据\n",
    "    print(examples.__class__)\n",
    "    print(len(examples['text']))\n",
    "    # print(examples)\n",
    "    # print(examples['text'])\n",
    "    # chunks 的长度必须要和 examples 一致，因为 chunks 中元素对应于 examples 中的每个样本\n",
    "    batch_size = len(examples['text'])\n",
    "    # 这里只是简单的记录每个 batch 中各个样本的长度\n",
    "    chunks = [[len(example) for example in examples['text']] for _ in range(batch_size)]\n",
    "    # print(chunks)\n",
    "    print('-------------------------------------------')\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "ds_2 = data_train.select([0,1,2,3,4,5]).map(example_chunk, batched=True, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41b7e81c-7fed-49a6-917c-6380f04d3798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0458cb3-9c84-4af8-aba9-e5ff4857f3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e6e45b5-6e51-49ce-84cd-a380d464ed76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '', 'chunks': [0, 30, 0]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f6464df-b09f-4afa-865d-61d2f72c279b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' = Valkyria Chronicles III = \\n', 'chunks': [0, 30, 0]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "972e18ba-408c-48e3-bb9a-b7724671ba06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n',\n",
       " 'chunks': [706, 524, 574]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbc6e3-0f3e-4c2b-aed6-b1308c226b62",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Tokenizer\n",
    "\n",
    "官方参考文档： \n",
    "+ [Tokenizer](https://huggingface.co/docs/tokenizers/index)\n",
    "+ [Course --> 6. The Tokenizer Library](https://huggingface.co/course/chapter6/1?fw=pt)\n",
    "\n",
    "\n",
    "一个tokenizer里包含了如下4个步骤（[Tokenizer -> The Tokenization pipeline](https://huggingface.co/docs/tokenizers/pipeline#the-tokenization-pipeline)）：\n",
    "1. **Normalization**：单词正则化，比如大小写处理，词形还原等\n",
    "2. **Pre-tokenization**：对单词进行简单的分词处理\n",
    "3. **Model**：使用更加细致的分词模型，比如WordPiece等\n",
    "4. **Postprocessor**：对分词后的序列进行后处理，添加BERT模型所需要的特殊token，比如`[CLS]`,`[SEP]`,`[PAD]`等\n",
    "\n",
    "上述4个步骤对应于`tokenizer`包里的4个模块，也是通过这4个模块来组成一个pipeline的，每个模块常用的选择可以参考官方文档 [Tokenizer -> Components](https://huggingface.co/docs/tokenizers/components#components)。\n",
    "\n",
    "此外，通常 tokenizer 还提供了一个 **Decoding** 步骤，用于将上面得到的词的id转换成原有的text。\n",
    "\n",
    "\n",
    "注意：\n",
    "> `transformer`包中各种模型使用的tokenizer的fast实现，就依赖于`tokenizer`包，但是slow版本的实现并不依赖。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbc61c-8173-4315-862e-4a0a4a90f794",
   "metadata": {},
   "source": [
    "### 分词模型\n",
    "\n",
    "参考文档：\n",
    "+ [Course --> 6. The tokenizers library -> Normalization and pre-tokenization](https://huggingface.co/course/chapter6/4?fw=pt#normalization-and-pre-tokenization)\n",
    "+ [Course --> 6. The tokenizers library -> Byte-Pair Encoding tokenization](https://huggingface.co/course/chapter6/5?fw=pt#byte-pair-encoding-tokenization)\n",
    "+ [Course --> 6. The tokenizers library -> WordPiece tokenization](https://huggingface.co/course/chapter6/6?fw=pt#wordpiece-tokenization)\n",
    "+ [Course --> 6. The tokenizers library -> Unigram tokenization](https://huggingface.co/course/chapter6/7?fw=pt#unigram-tokenization)\n",
    "\n",
    "主要分为如下几类：\n",
    "+ Byte-Pair Encoding(BPE)\n",
    "+ WordPiece\n",
    "+ Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59a244-d673-4d45-886a-99d49a39613c",
   "metadata": {},
   "source": [
    "### tokenizer使用\n",
    "\n",
    "`Tokenizer`类的属性如下：\n",
    "+ `normalizer`属性，pipeline中使用的`Normalizer`类\n",
    "+ `pre_tokenizer`属性，pipeline中使用的`PreTokenizer`类\n",
    "+ `model`属性，...\n",
    "+ `post_processor`属性，...\n",
    "+ `decoder`属性，....\n",
    "+ `padding`属性，返回一个dict，记录当前的padding配置\n",
    "+ `truncation`属性，返回一个dict，记录当前的截断配置\n",
    "\n",
    "`Tokenizer`类的常用方法如下：\n",
    "+ `from_pretrained()`，加载已训练好的token\n",
    "+ `encode(sequence, pair = None, is_pretokenized = False, add_special_tokens = True)`：将word转成token id\n",
    "+ `encode_batch()`：批量转换\n",
    "+ `decode(ids, skip_special_tokens = True)`：将token id 转回 word\n",
    "+ `decode_batch()`：批量转换\n",
    "+ `token_to_id()`：\n",
    "+ `id_to_token()`：\n",
    "+ `train(files, trainer = None )`：训练分词器\n",
    "+ `train_from_iterator(iterator, trainer = None )`：训练分词器\n",
    "+ `save()`：保存分词器\n",
    "\n",
    "上面`encode()`方法返回的是`Encoding`对象，该对象是dict的子类，有如下属性和方法：\n",
    "+ `ids`：每个token的ID\n",
    "+ `tokens`：具体的token\n",
    "+ `attention_mask`：用于告诉Bert等模型，哪些位置（值为1）的词需要进行attention操作\n",
    "+ `type_ids`：输入为成对句子（`pair=`参数）时，记录前后两个句子的token归属\n",
    "+ `word_ids`：每个token属于哪个word的索引，在分词模型为subword时起作用\n",
    "+ `words`，同`word_ids`，这个属性后面会被取消"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "426b88d6-39a4-42ed-ab30-8cdebaf414bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8864dac9-e876-4d11-9e0e-22f24b93a211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.Tokenizer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5138913-eedf-41ab-846d-657d9f15a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.normalizer:      <tokenizers.normalizers.BertNormalizer object at 0x000002173CAC4D70>\n",
      "tokenizer.pre_tokenizer:   <tokenizers.pre_tokenizers.BertPreTokenizer object at 0x000002173CAC4CB0>\n",
      "tokenizer.model:           <tokenizers.models.WordPiece object at 0x000002173CB36930>\n",
      "tokenizer.post_processor:  <tokenizers.processors.TemplateProcessing object at 0x000002173CA7C960>\n",
      "tokenizer.decoder:         <tokenizers.decoders.WordPiece object at 0x000002173CA7CA50>\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.normalizer:     ', tokenizer.normalizer)\n",
    "print('tokenizer.pre_tokenizer:  ', tokenizer.pre_tokenizer)\n",
    "print('tokenizer.model:          ', tokenizer.model)\n",
    "print('tokenizer.post_processor: ', tokenizer.post_processor)\n",
    "print('tokenizer.decoder:        ', tokenizer.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efb9eb36-af60-426f-8d06-6ad0dd6e593e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.Encoding"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "pair = \"The second sentence.\"\n",
    "encoding = tokenizer.encode(example, pair=pair)\n",
    "type(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97a921df-095b-41cf-83b6-a1599285f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding.ids:  [101, 2026, 2171, 2003, 25353, 22144, 2378, 1998, 1045, 2147, 2012, 17662, 2227, 1999, 6613, 1012, 102, 1996, 2117, 6251, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print('encoding.ids: ', encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43c3c69-5c86-4e8d-818c-60a673a5ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding.tokens:  ['[CLS]', 'my', 'name', 'is', 'sy', '##lva', '##in', 'and', 'i', 'work', 'at', 'hugging', 'face', 'in', 'brooklyn', '.', '[SEP]', 'the', 'second', 'sentence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print('encoding.tokens: ', encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6415a23a-57a1-4178-a1aa-487c14f2cd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding.attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print('encoding.attention_mask: ', encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b0f01d0-fc8d-4bb5-937a-b7c9582df4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding.type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print('encoding.type_ids: ', encoding.type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "358916e9-80a3-4f76-9495-57326d8ee794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding.word_ids:  [None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None, 0, 1, 2, 3, None]\n"
     ]
    }
   ],
   "source": [
    "print('encoding.word_ids: ', encoding.word_ids)\n",
    "# print('encoding.words: ', encoding.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b2630-adbf-4234-b081-7276ca7630bd",
   "metadata": {},
   "source": [
    "### 分词器的两类实现\n",
    "\n",
    "huggingface官方在两个地方提供了分词类：\n",
    "+ `tokenizer` 包，这里提供的分词类**底层是用Rust实现**的，分词速度非常快，这里实现的分词类被称为 fast-tokenizer.\n",
    "+ `transformer` 包里也提供了**纯python实现**的分词类，但是该分词类的效率不高，在处理大规模语料时，速度很慢.  \n",
    "\n",
    "因此，`transformer`包里每个模型的分词类都有两种实现，一个是包内自带的纯python实现的分词类，另一个就是依赖于`tokenizer`包的fast-tokenizer。\n",
    "\n",
    "有关这两类分词的对比可以参考如下官方文档：\n",
    "+ [Course --> 6. The tokenizers library -> Fast tokenizers' special powers](https://huggingface.co/course/chapter6/3?fw=pt#fast-tokenizers-special-powers)\n",
    "+ [Course --> 6. The tokenizers library -> Fast tokenizers in the QA pipeline](https://huggingface.co/course/chapter6/3b?fw=pt#fast-tokenizers-in-the-qa-pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f3021b-33a1-4540-b8c6-eba83bc207c7",
   "metadata": {},
   "source": [
    "这两类分词器的实现有一些区别，fast-tokenizer分词类提供的功能多一些，能够进行更加细致的处理。   \n",
    "\n",
    "比如上面返回的`Encoding`对象里的`word_ids`属性，就记录了分成subword之后，每个subword对应的原有完整词的索引：`[3,3,3]` 对应于`['sy', '##lva', '##in']`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16849aa-cad3-4622-8513-54a1f0a14b4f",
   "metadata": {},
   "source": [
    "### 训练tokenizer\n",
    "\n",
    "这部分主要参考官方 Course 教程的如下几部分：\n",
    "+ [Course --> 6. The tokenizers library -> Training a new tokenizer from an old one](https://huggingface.co/course/chapter6/2?fw=pt)，从已有的tokenizer进行迁移训练。\n",
    "+ [Course --> 6. The tokenizers library -> Building a tokenizer, block by block](https://huggingface.co/course/chapter6/8?fw=pt)，介绍了如何训练一个自己的tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363fd159-a956-4d50-9a48-95ba6744690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder, load_dataset\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers, models, processors, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f2424be-e270-43cb-b17c-1832ab3e4369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Drivi\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5198fc86-24e1-4ea4-a0cb-d42b770159cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Drivi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 36718\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'.\\datasets\\huggingface\\wikitext.py'\n",
    "builder = load_dataset_builder(path, name='wikitext-2-raw-v1')\n",
    "data_train = load_dataset(path, 'wikitext-2-raw-v1', split='train')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d273458c-a2d8-4747-8851-e312ec2ebbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集组织成生成器，每次返回一个batch的数据\n",
    "def get_training_corpus_batch(text_data):\n",
    "    for i in range(0, len(text_data), 1000):\n",
    "        yield text_data[i: i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7fd3526-dfce-43fb-84f1-f7c5668a4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 实例化一个 Tokenizer 类，使用的sub-word分词模型是 WordPiece\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "53a078f8-546d-405d-acf5-4e7ecb1c4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 规范化（Normalization）步骤：分词，去除大小写，词形还原等\n",
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "# 或者可以手动拼凑更加细节的控制\n",
    "# tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0091a58-1d8a-4c6b-9520-9674eac4116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 预处理（pre-tokenization）步骤：\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "# 或者手动进行精细化处理\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1e8fd36-07a0-4403-b7ab-cba3ac160ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 配置sub-word分词模型的训练器\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a0ed114d-526b-4194-9d66-bba5cecd44d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 使用语料进行训练\n",
    "tokenizer.train_from_iterator(get_training_corpus_batch(data_train), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6247284d-9e2c-49fc-8e8e-4c816951c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 后处理（post-processing）流程：也就是在句子开头加上 [CLS]，句子中间和末尾加上 [SEP]\n",
    "# 首先获取 [CLS] 和 [SEP] 的 token_id\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "# 然后增加后处理流程\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad063fa5-2189-4d20-b400-456d0ecf0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 保存训练的 tokenizer\n",
    "tokenizer.save(r\".\\datasets\\huggingface\\wikitext-2-raw-v1_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b1f47-8f28-4dd2-beb9-19d994a35e6d",
   "metadata": {},
   "source": [
    "+ 读取已训练的分词器，测试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c653960c-aca4-4204-ab03-5b85649e79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(r\".\\datasets\\huggingface\\wikitext-2-raw-v1_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26da755-b132-4320-b2c6-985c3e2aaf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'my', 'pre', '-', 'tok', '##eni', '##zer', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "------------------------------------\n",
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '.', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "single_sen = \"Let's test my pre-tokenizer.\"\n",
    "pair_sen = \"Let's test this tokenizer...\", \"on a pair of sentences.\"\n",
    "\n",
    "single_encoding = tokenizer.encode(single_sen)\n",
    "pair_encoding = tokenizer.encode(*pair_sen)\n",
    "\n",
    "print(single_encoding.tokens)\n",
    "print(single_encoding.type_ids)\n",
    "print(single_encoding.attention_mask)\n",
    "print('------------------------------------')\n",
    "print(pair_encoding.tokens)\n",
    "print(pair_encoding.type_ids)\n",
    "print(pair_encoding.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839cd6a6-e891-4537-af49-5abc9e3e58cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Transformer\n",
    "\n",
    "参考：\n",
    "+ 官方网站 [huggingface-transformer](https://huggingface.co/docs/transformers/index)\n",
    "+ github地址 [huggingface/transformers](https://github.com/huggingface/transformers)\n",
    "\n",
    "`transformer`包基于transformer结构，实现了针对如下任务的各种SOTA模型:\n",
    "+ **NLP**：包括BERT, DistilBERT, GPT2 等等，可以用于文本分类，NER，问答对，机器翻译等\n",
    "+ CV：计算机视觉相关的模型\n",
    "+ Audio：语音处理的模型\n",
    "+ Multimodal：多模态的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d2c48-e3be-4d67-bddf-156b63a4b0d7",
   "metadata": {},
   "source": [
    "### 基本框架\n",
    "\n",
    "有关transformer包的设计理念可以参考官方文档 [Conceptual Guides -> Philosophy](https://huggingface.co/docs/transformers/philosophy).\n",
    "\n",
    "transformer 包为了保持简洁，尽量减少了抽象接口，每个模型只有3个必要的抽象部分，这里**以NLP任务**为例：\n",
    "\n",
    "1. [Configuration](https://huggingface.co/transformers/main_classes/configuration.html)  \n",
    "基础的类是 `PretrainedConfig`（好像也只有这一个类），用于封装各个模型所需要的参数配置，同时也可以方便的 **导出/导入** 参数配置。  \n",
    "不同的模型有自己的配置类，但是都是继承于此类。\n",
    "\n",
    "2. [Models](https://huggingface.co/transformers/main_classes/model.html)  \n",
    "基础的类有如下三个，用于构建模型：\n",
    "   + `PreTrainedModel`，pytorch实现的模型都继承了这个类，继承于 `torch.nn.Module` 类.\n",
    "   + `TFPreTrainedModel`，Tensorflow2.0实现的模型都继承了这个类，继承于 `tf.keras.Model` 类.\n",
    "   + `FlaxPreTrainedModel`.    \n",
    ".\n",
    "\n",
    "3. 预处理类.   \n",
    "对于NLP任务来说是 [Tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html)，包含两个基础的类，用于将文本特征转换成transformer能处理的序列特征：\n",
    "   + `PreTrainedTokenizer`，这个是纯Python实现，速度比较慢\n",
    "   + `PreTrainedTokenizerFast`，这个是基于Rust实现，速度比较快，依赖于上面的`tokenizer`包      \n",
    "   \n",
    "   对于视频或音频任务来说，是feature extrator类。\n",
    "   \n",
    "\n",
    "上面这三个基础模块，都有如下两个方法用于**导入/导出**对应的配置：\n",
    "+ `.from_pretrained()`，导入预训练模型的 config/model/tokenizer\n",
    "  + `pretrained_model_name_or_path`，指定预训练模型的名称或者本地路径\n",
    "  + `cache_dir`，指定模型的缓存位置\n",
    "  + `force_download`，bool，指定是否强制下载模型\n",
    "  + `local_files_only`，bool，指定是否只使用本地模型\n",
    "  + `mirror`，指定下载的镜像地址\n",
    "\n",
    "> **`PretrainedConfig`类及其子类，可以直接用于实例化各种模型，但是不能用于实例化预处理类（比如Tokenizer）**.\n",
    "\n",
    "+ `.save_pretrained()`，导出训练好模型的 config/model/tokenizer\n",
    "\n",
    "**第一次使用这些语句的时候，如果本地没有对应的模型，会下载这些模型**。\n",
    "  \n",
    "在上面的3个基础子模块之上，还提供了如下几个子模块工具：\n",
    "\n",
    "+ [Pipepline](https://huggingface.co/transformers/main_classes/pipelines.html)  \n",
    "用于快速使用模型，是对上面三个类的封装\n",
    "+ [Trainer](https://huggingface.co/transformers/main_classes/trainer.html)  \n",
    "用于快速训练或者 fine-tune 模型\n",
    "\n",
    "\n",
    "transformer源码中，所有的模型结构都存放在 `src/transformers/models`文件夹下，每个模型对应于一个文件夹（比如BERT对应的就是`bert`文件夹），每个模型的文件夹内，主要有如下几个`.py`文件，其中的 `xxx` 是对应模型的名称：\n",
    "+ **`configuration_xxx.py`——KEY**，编写了该模型对应的`PretrainedConfig`子类，比如BERT就是`BertConfig`。\n",
    "+ **`modeling_xxx.py`——KEY**，存放了pytorch实现的模型结构，要用到的模型类都放在这里面\n",
    "+ `modeling_tf_xxx.py`，存放tensorflow实现的模型结构\n",
    "+ `modeling_flax_xxx.py`\n",
    "+ **`tokenization_xxx.py`——KEY**，用于 分词的 实现类（纯python实现），比如BERT就是`BertTokenizer`类\n",
    "+ `tokenization_xxx_fast.py`，快速分词的实现类，依赖于`tokenizer`包\n",
    "+ `convert_*.py`，用于将其它配置文件转换成对应的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332d6e1-dc72-4738-a48b-8de346e2dd3c",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85357b79-c5cc-4dbd-9bca-cc3adc2d4b27",
   "metadata": {
    "tags": []
   },
   "source": [
    "----------------\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "有关 tokenizer 的使用，可以参考如下官方教程\n",
    "+ [Tutorials --> Preprocess -> NLP](https://huggingface.co/docs/transformers/preprocessing#nlp)\n",
    "+ [Conceptual Guide --> Glossary -> Model inputs](https://huggingface.co/docs/transformers/glossary#model-inputs)\n",
    "+ 接口文档[Transformer-Tokenizer](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/tokenizer)\n",
    "\n",
    "`Tokenizer`类是处理transformer结构输入的主要类，它用于将文本类的数据转换成transformer结构能够接受的序列数据.  \n",
    "\n",
    "所有的模型里，Tokenizer 都有两种实现：\n",
    "  1. 基于python代码的实现，基类为`PreTrainedTokenizer`，在`transformer`包里用python实现的\n",
    "  2. 基于Rust Library的**快速**实现，基类为`PreTrainedTokenizerFast`，**这些快速分词类的实现依赖于`tokenizer`包**\n",
    "\n",
    "\n",
    "上述两个类又都是基于`PreTrainedTokenizerBase`类，该类实现了分词处理的主要方法，具体功能如下所示：\n",
    "  + 对文本进行分词，并将分词后的word映射到对应词典的id\n",
    "  + 训练分词模型，比如BERT使用的WordPiece分词法\n",
    "  + 生成特殊的Tokens，比如mask等\n",
    "  + 对序列进行截断，填充等特殊操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c53935-20c0-4855-b681-11d15478e1aa",
   "metadata": {},
   "source": [
    "#### 实例化Tokenizer\n",
    "\n",
    "`PreTrainedTokenizer`类虽然有实例化参数，但是通常会**使用`.from_pretrained()`方法从文件中读取配置，生成对应的Tokenizer对象**。  \n",
    "\n",
    "注意，**Tokenizer类不能使用`PreTrainedConfig`类及其子类来实例化**。\n",
    "\n",
    "通常该对象会有如下属性：\n",
    "+ `name_or_path`：读取的配置路径\n",
    "+ `vocab_file_names`：字典文件名\n",
    "+ `vocab_size`：单词字典的大小\n",
    "+ `vocab`：单词字典，是一个OrderedDict\n",
    "+ `ids_to_tokens`：id和token的映射字典，也是一个OrderedDict\n",
    "+ `wordpiece_tokenizer`：底层使用的分词器\n",
    "+ `max_model_input_sizes`\n",
    "+ `max_len_single_sentence`\n",
    "+ `model_max_length`\n",
    "+ **`is_fast`：是否为快速分词类**\n",
    "\n",
    "此外，还有一些特殊token的字符和id，比如：\n",
    "+ `pad_token`, `pad_token_id`\n",
    "+ `sep_token`, `sep_token_id`\n",
    "+ `cls_token`, `cls_token_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29397f8-88b2-49c1-8ef2-27a57cfe868e",
   "metadata": {},
   "source": [
    "#### `PreTrainedTokenizer`\n",
    "\n",
    "该类是所有**slow tokenizer**的基类，有如下常用的方法：\n",
    "\n",
    "##### `__call__()`\n",
    "\n",
    "参数如下：\n",
    "+ `text`，输入的句子或者句子序列，可接受的输入类型有：\n",
    "  + `str`，单个句子\n",
    "  + `List[str]`，多个句子。注意，如果这里是 list of word 的形式，那么需要设置`is_split_into_words=True`。\n",
    "  + `List[List[str]]`，一个batch的句子\n",
    "+ `text_pair`，**可选**，可接受的输入类型同`text`，**这里传入的句子会和 `text` 里的句子组成一对**，常用于QA或者翻译任务中。\n",
    "+ `text_target`：这个是用于翻译之类的任务中，`text`对应的目标序列\n",
    "+ `text_pair_target`\n",
    "+ `max_length`：设置可接受的最大句子长度\n",
    "+ `padding`，补全策略，有如下几种输入：\n",
    "  + `True` or `longest`，使用padding，此时按照最长的句子进行padding\n",
    "  + `max_length`，按照初始化指定的`max_length`参数或者模型的`model_max_length`填充\n",
    "  + `False` 或者 `do_not_pad`，不做padding，这是**默认值**。此时返回的数据中，一个batch中的各个句子会有不同的长度。\n",
    "+ `truncation`，指定是否截断\n",
    "+ `stride`\n",
    "+ `return_tensors`：返回值的类型\n",
    "  + `tf`：返回`tf.constant`对象\n",
    "  + `pt`：返回`torch.Tensor`对象\n",
    "  + `np`：返回`numpy.ndarray`对象\n",
    "\n",
    "\n",
    "`__call__()`方法返回的是一个`BatchEncoding`类对象，封装了返回的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a86fd-9ece-4273-aeeb-3f1e31ab5552",
   "metadata": {},
   "source": [
    "##### `tokenize()`\n",
    "将文本转换成分词后的 tokens，返回的是 `List[str]`，表示的是文本对应 **分词后** 的 tokens .  \n",
    "注意，**这个不能处理句子对，只能处理单个句子**！\n",
    "\n",
    "  \n",
    "##### `encode()`\n",
    "将文本转换成 sequences of ids.\n",
    "+ 它接受的参数和 `__call__()` 方法一样\n",
    "+ 返回的是 `List[int]`，表示的是文本分词后 tokens 对应的 tokenized ids.\n",
    "\n",
    "\n",
    "##### `decode()`\n",
    "用于将分词后的 tokens id 转成分词后的文本，并且去除其中的特殊token.\n",
    "\n",
    "\n",
    "##### 其他方法\n",
    "\n",
    "+ `convert_ids_to_tokens()`\n",
    "+ `conver_tokens_to_ids()`\n",
    "+ `conver_tokens_to_string()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea52052-17c1-4ec0-9ef0-f0fa787ed350",
   "metadata": {},
   "source": [
    "#### `PreTrainedTokenizerFast`\n",
    "\n",
    "快速分词的实现基类，它依赖于`tokenizer`包。\n",
    "\n",
    "相比与`PreTrainedTokenizer`类，它的初始化方法里，多了两个参数：\n",
    "+ `tokenizer_object`：是一个`tokenizer.Tokenizer`对象，它是快速分词底层使用的分词器。   \n",
    "  **如果使用自己训练的快速tokenizer时，就需要使用这个参数来实例化一个transformer使用的分词对象**。\n",
    "+ `tokenizer_file`：指定`tokenizer.Tokenizer`保存的json对象\n",
    "\n",
    "它提供的接口和`PreTrainedTokenizer`类似，多了如下的两个方法：\n",
    "\n",
    "+ `batch_decode()`\n",
    "+ `train_new_from_iterator()`\n",
    "\n",
    "**注意，`__call__()`方法返回的仍然是`BatchEncoding`对象，而不是`Tokenizer`类返回的`Encoding`对象**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c1b74-201a-423c-91f5-34d87f8da653",
   "metadata": {},
   "source": [
    "#### BatchEncoding对象\n",
    "\n",
    "它有如下几个重要属性：\n",
    "+ `data`：一个dict，存放分词后的数据，有如下重要的key:\n",
    "  + `input_ids`，分词后每个 token 对应 vocabulary 的 index —— 这个被**作为输入模型的数据**.\n",
    "  + `attention_mask`，表明填充的 token 位置，**填充位的 token 对应于 0**.  \n",
    "  因为一个batch中不同的句子长度不一样，短一些的句子会被padding成同样的长度，此时这些padding的位置需要被记录下来，用于告知模型这部分只是填充使用，不需要参与self-attention。\n",
    "  + `token_type_ids`：**可选**，通常用于**问答对**的句子中，它也是一个{0,1}掩码的形式，0表示属于问题的句子，1表示属于答案的句子\n",
    "  + `labels`：**可选**，翻译任务中会用到\n",
    "  \n",
    "+ `encoding`：`tokenizer.Encoding`类，对于fast-tokenizer来说，会存放一些特殊的信息\n",
    "+ `is_fast`：是否为fast-tokenizer的结果\n",
    "  \n",
    "  \n",
    "  \n",
    "常用的一些方法如下：\n",
    "+ `to(device)`\n",
    "+ `sequence_ids(batch_index=0)`：返回指定batch里，句子所属部分的指示值：\n",
    "  + 对于 `[CLS]`, `[SEP]` 这几个特殊的 token，返回 `None`\n",
    "  + 对于属于第一个句子的 token，返回 0\n",
    "  + 对于属于第二个句子的 token，返回 1   \n",
    "\n",
    "  主要用于成对句子\n",
    "\n",
    "+ `tokens(batch_index=0)`：返回指定batch里，句子进行分词后的token\n",
    "+ `words(batch_index=0)`：返回句子中每个token的位置索引\n",
    "+ `word_ids(batch_index=0)`：这个和`words()`一样，后续会被取代。\n",
    "\n",
    "> token 和 word 区别在于，使用wordPiece之类的分词模型时，对于一些特殊的词，比如 `Titan RTX`，划分得到的token是: `'titan', 'rt', '##x'`，但是原始的word是 `'titan', 'rtx'`，此时的word_ids 就是用来表示 token 和 原来的 word 之间的对应关系：`[0, 1, 1]`，两个 1 表示 `rt`, `##x` 对应的是原来的一个 word。\n",
    "\n",
    "+ `token_to_word()`：指定位置的 token 对应于原始文本中的word索引\n",
    "\n",
    "+ `word_to_tokens()`：指定位置的 word 对应的 token 的起止索引，因为一个word可能被分成多个token，所以是复数\n",
    "\n",
    "+ `token_to_sequence()`：指定位置的 token 属于sequence中的哪个部分，只返回 {0, 1}\n",
    "\n",
    "+ `token_to_chars()`\n",
    "\n",
    "+ `word_to_chars()`\n",
    "\n",
    "+ `char_to_token()`\n",
    "\n",
    "+ `char_to_word()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2348242-9b99-4261-af3d-0573fae5ad7b",
   "metadata": {},
   "source": [
    "#### 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5ceb1c-c54d-4398-9323-d37ef43e46fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "insured-place",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc19405-01f0-4f7d-9f4e-da2cab7c115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 BERT tokenizer 的设置\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "model_path = r\"bert-pretrained-models\\bert-base-uncased\" # windows\n",
    "# model_path = r\"bert-pretrained-models/bert-base-uncased\" # mac\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_path, local_files_only=True)\n",
    "tokenizer_fast = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_path, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1272a-58d5-4e1f-b393-5f0dcef243f7",
   "metadata": {},
   "source": [
    "+ 需要注意，这里的tokenizer类型不是`tokenizer.Tokenizer`类，只有fast里的`.backend_tokenizer`是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558bdd8f-dc26-470a-979e-67472fdf8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.__class__:        <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "tokenizer.basic_tokenizer:  <transformers.models.bert.tokenization_bert.BasicTokenizer object at 0x0000019C0F00CEB0>\n",
      "tokenizer_fast.__class__:   <class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "tokenizer_fast.backend_tokenizer:  <tokenizers.Tokenizer object at 0x0000019C0D78B090>\n"
     ]
    }
   ],
   "source": [
    "print('tokenizer.__class__:       ', tokenizer.__class__)\n",
    "print('tokenizer.basic_tokenizer: ', tokenizer.basic_tokenizer)\n",
    "print('tokenizer_fast.__class__:  ', tokenizer_fast.__class__)\n",
    "\n",
    "# 快速版本没有 basic_tokenizer 属性, 只有 backend_tokenizer\n",
    "# print('tokenizer_fast.basic_tokenizer: ', tokenizer_fast.basic_tokenizer)\n",
    "print('tokenizer_fast.backend_tokenizer: ', tokenizer_fast.backend_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf330e3-c290-487a-998c-0aa5283dc1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.vocab_size:  30522\n",
      "tokenizer_fast.vocab_size:  30522\n"
     ]
    }
   ],
   "source": [
    "# 查看分词表大小\n",
    "print(\"tokenizer.vocab_size: \", tokenizer.vocab_size)\n",
    "print(\"tokenizer_fast.vocab_size: \", tokenizer_fast.vocab_size)\n",
    "\n",
    "# 查看分词器所使用的的词表， 这个打印会很长\n",
    "# tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b9056f-154d-46b1-aeea-81d0b9b00355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] : [100, 102, 0, 101, 103]\n",
      "[CLS] :  101\n",
      "[MASK] :  103\n",
      "[PAD] :  0\n"
     ]
    }
   ],
   "source": [
    "# 查看特殊的 tokens 和对应的 id\n",
    "print(tokenizer.all_special_tokens, ':', tokenizer.all_special_ids)\n",
    "print(tokenizer.cls_token, ': ',tokenizer.cls_token_id)\n",
    "print(tokenizer.mask_token, ': ', tokenizer.mask_token_id)\n",
    "print(tokenizer.pad_token, ': ', tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b5a795c-3c6e-4459-b7e3-32e5e57cf548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] : [100, 102, 0, 101, 103]\n",
      "[CLS] :  101\n",
      "[MASK] :  103\n",
      "[PAD] :  0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fast.all_special_tokens, ':', tokenizer_fast.all_special_ids)\n",
    "print(tokenizer_fast.cls_token, ': ',tokenizer_fast.cls_token_id)\n",
    "print(tokenizer_fast.mask_token, ': ', tokenizer_fast.mask_token_id)\n",
    "print(tokenizer_fast.pad_token, ': ', tokenizer_fast.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b80d2-264b-4e78-82b3-9a917af15c07",
   "metadata": {},
   "source": [
    "+ 对单个句子或者一个batch的句子进行分词  \n",
    "此时所有句子分词后得到的`token_type_ids` 都是0，因为它们属于同一个句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb467534-a310-4669-86ab-9c0484d106d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_words:  ['a', 'titan', 'rt', '##x', 'has', '24', '##gb', 'of', 'vr', '##am']\n",
      "tokenized_ids:    [101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102]\n",
      "decoded_tokens:   [CLS] a titan rtx has 24gb of vram [SEP]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A Titan RTX has 24GB of VRAM\"\n",
    "\n",
    "# 查看分词后的 tokens\n",
    "tokenized_words = tokenizer.tokenize(sentence)\n",
    "print(\"tokenized_words: \", tokenized_words)\n",
    "\n",
    "# 查看分词后的 tokens id\n",
    "tokenized_ids = tokenizer.encode(sentence)\n",
    "print(\"tokenized_ids:   \", tokenized_ids)\n",
    "\n",
    "# 从 token id 映射回 token\n",
    "decoded_tokens = tokenizer.decode(tokenized_ids)\n",
    "print(\"decoded_tokens:  \", decoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a063a11-2daf-4f94-a093-3e58b0e1ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoding.__class__:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "input_ids:       [101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102]\n",
      "attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 直接调用 __call__() 方法，可以得到分词后的所有信息，一步到位\n",
    "batch_encoding = tokenizer(sentence)\n",
    "print('batch_encoding.__class__: ', batch_encoding.__class__)\n",
    "print(\"input_ids:      \", batch_encoding['input_ids'])\n",
    "print(\"attention_mask: \", batch_encoding['attention_mask'])\n",
    "print(\"token_type_ids: \", batch_encoding['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cb7bf9e-586b-4f0f-b18f-4880101d4758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoding.__class__:  <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "input_ids:       [101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102]\n",
      "attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 使用 fast 版本分词\n",
    "batch_encoding = tokenizer_fast(sentence)\n",
    "print('batch_encoding.__class__: ', batch_encoding.__class__)\n",
    "print(\"input_ids:      \", batch_encoding['input_ids'])\n",
    "print(\"attention_mask: \", batch_encoding['attention_mask'])\n",
    "print(\"token_type_ids: \", batch_encoding['token_type_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebabe8-b7fa-414b-a01b-32dfacbc21a1",
   "metadata": {},
   "source": [
    "+ 多个长度不一样的句子要使用 padding，并且由 attention_mask 区分padding的词和正常词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "conscious-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1.length:  18 ; s2.length:  18\n",
      "s1['input_ids']:       [101, 2023, 2003, 1037, 2460, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s2['input_ids']:       [101, 2023, 2003, 1037, 2738, 2936, 5537, 1012, 2009, 2003, 2012, 2560, 2936, 2084, 1996, 5537, 1015, 102]\n",
      "s1['attention_mask']:  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s2['attention_mask']:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s1 = \"this is a short sentence\"\n",
    "s2 = \"This is a rather longer sequence. It is at least longer than the sequence 1\"\n",
    "\n",
    "batch_encoding = tokenizer([s1, s2], padding=True)\n",
    "\n",
    "print(\"s1.length: \", len(batch_encoding['input_ids'][0]), \"; s2.length: \", len(batch_encoding['input_ids'][1]))\n",
    "print(\"s1['input_ids']:      \", batch_encoding['input_ids'][0])\n",
    "print(\"s2['input_ids']:      \", batch_encoding['input_ids'][1])\n",
    "print(\"s1['attention_mask']: \", batch_encoding['attention_mask'][0])\n",
    "print(\"s2['attention_mask']: \", batch_encoding['attention_mask'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0cc47-bc49-4a25-95b2-ceb5cf1fbedf",
   "metadata": {},
   "source": [
    "+ 对**成对句子**进行分词  \n",
    "此时`toke_type_ids`就起作用了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed433250-3718-4b55-a87e-c244c13821c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  [101, 2023, 2003, 1037, 2460, 6251, 102, 2023, 2003, 1037, 2738, 2936, 5537, 102]\n",
      "token_type_ids :  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s1 = \"this is a short sentence\"\n",
    "s2 = \"This is a rather longer sequence\"\n",
    "\n",
    "# 注意，此时传入的方式，不是 list，这两个句子作为一个样本，第一个是 text, 第二个是 text_pair\n",
    "batch_encoding = tokenizer(text=s1, text_pair=s2, padding=True)\n",
    "print('input_ids : ', batch_encoding['input_ids'])\n",
    "print('token_type_ids : ', batch_encoding['token_type_ids'])\n",
    "print('attention_mask : ', batch_encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c9bf0f4-89cb-4e37-913c-85d64e49769b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] this is a short sentence [SEP] this is a rather longer sequence [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_encoding['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04d7b926-1304-4ffe-a8e6-4686f5d88556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  [[101, 2023, 2003, 1037, 2460, 6251, 102, 2023, 2003, 1037, 2738, 2936, 5537, 102], [101, 2023, 2003, 1037, 2460, 6251, 102, 2023, 2003, 1037, 2738, 2936, 5537, 102]]\n",
      "token_type_ids :  [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention_mask :  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 传入一个batch的句子对\n",
    "text_batch = [s1, s1]\n",
    "text_pair_batch = [s2, s2]\n",
    "\n",
    "batch_encoding = tokenizer(text=text_batch, text_pair=text_pair_batch, padding=True)\n",
    "\n",
    "print('input_ids : ', batch_encoding['input_ids'])\n",
    "print('token_type_ids : ', batch_encoding['token_type_ids'])\n",
    "print('attention_mask : ', batch_encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49dd96-6a50-4cab-afc5-4a9c8990ea3f",
   "metadata": {},
   "source": [
    "+ `BatchEncoding`对象的常用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "120d73c2-2dac-47ad-9b07-0e369e2b9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "s1 = \"A Titan RTX has 24GB of VRAM\"\n",
    "s2 = \"This is a rather longer sequence than sequence s1\"\n",
    "batch_encoding = tokenizer_fast(text=s1, text_pair=s2, padding=True)\n",
    "print(batch_encoding.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53e6b67b-0e0a-4675-a34b-8bed156f8215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "# 查看指定批次的句子的 前后关系，0 表示属于第一个句子，1表示属于第二个句子\n",
    "print(batch_encoding.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a63c79fc-bd6f-48a3-806b-34386cf927a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'a', 'titan', 'rt', '##x', 'has', '24', '##gb', 'of', 'vr', '##am', '[SEP]', 'this', 'is', 'a', 'rather', 'longer', 'sequence', 'than', 'sequence', 's', '##1', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 查看指定批次的句子分词后的token\n",
    "# 这里只有一个样本，所以传入的batch_index只能是0\n",
    "print(batch_encoding.tokens(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0549d144-98bc-45be-bf50-cc5c059693c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 2, 3, 4, 4, 5, 6, 6, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_ids 表示的是上面划分的token，对应与原来文本中的哪个词的index，比如 2, 2 表示上面的 rt, ##x 在原始文本中是同一个word。\n",
    "batch_encoding.word_ids(0)\n",
    "# 下面的会被取代\n",
    "# batch_encoding.words(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1284d117-3d9f-4687-a68b-14d00c708609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里只有 1 个记录，所以传入的被当做 token_index，返回的是对应index的token，对应于原始文本中的word的index\n",
    "batch_encoding.token_to_word(batch_or_token_index=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20ad0183-7e73-4306-ad87-7b630ba3ebeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenSpan(start=3, end=5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 给出 第 2 个位置的word: RTX 对应的 token 起止位置\n",
    "batch_encoding.word_to_tokens(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79be51f2-c9f1-431a-badc-82d16f94a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 返回第 n 个 位置的 token，属于哪个序列\n",
    "print(batch_encoding.token_to_sequence(5))\n",
    "print(batch_encoding.token_to_sequence(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6af667f-7ec7-4fb6-a2fd-73dd972aa5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharSpan(start=12, end=15)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_encoding.token_to_chars(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b661f1-b46f-4ad2-8e76-b9b323f14f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----------------\n",
    "\n",
    "### Model\n",
    "\n",
    "transformer的Model基类`PreTrainedModel`有如下的一些属性或方法：\n",
    "+ `.base_model`，返回当前使用的模型，是`torch.nn.Module`类对象\n",
    "+ `get_input_embeddings()`，返回Embedding层，也是一个`nn.Module`类对象\n",
    "+ `get_output_embeddings()`，返回输出层的 embeddings。\n",
    "\n",
    "\n",
    "+ 可用的模型列表见官网 [Pretrained models](https://huggingface.co/transformers/pretrained_models.html).\n",
    "  + BERT模型（名称）有：\n",
    "    + bert-base-uncased\n",
    "    + bert-base-cased\n",
    "    + bert-large-uncased\n",
    "    + bert-large-cased\n",
    "    + bert-base-chinese  \n",
    ".    \n",
    "\n",
    "  + Distil-Bert模型（名称）有：\n",
    "    + distilbert-base-uncased\n",
    "    + distilbert-base-cased\n",
    "  \n",
    "  这些模型都比较大，可以提前下载下来。\n",
    "\n",
    "我一般常用的两个模型是 **BERT** 和 **Distil-BERT**，所以下面会着重关注这两个模型里的实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92032d02-a793-406e-980c-96a48711b739",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BERT\n",
    "[BERT Models](https://huggingface.co/transformers/model_doc/bert.html) 包含如下类（以pytorch为例）\n",
    "+ `BertConfig`，配置类.  \n",
    "位于`configuration_bert.py`文件中，该文件中也只有这一个配置类\n",
    "+ `BertTokenizer`，实现 `WordPiece` 分词的类.  \n",
    "位于`tokenization_bert.py`文件中，该文件中还有 `BasicTokenizer` 和 `WordpieceTokenizer` 两个类，不过`BertTokenizer`会调用它们.\n",
    "+ `BertForPreTrainingOutput`，记录BERT模型的输出.\n",
    "+ `BertModel`，最基本的 BERT 模型类，它返回的是 BERT 模型的原始结果，包括隐藏层状态.\n",
    "+ `BertForPreTraining`，在 `BertModel`的输出上加了一层处理，以下的几个模型都是如此.\n",
    "+ `BertForMaskedLM`\n",
    "+ `BertForNextSentencePrediction`\n",
    "+ `BertForSequenceClassification`\n",
    "+ 还有其他的一些基础工具类和其他任务对于的BERT模型类，这里就不介绍了。   \n",
    "\n",
    "注意：\n",
    "> huaggingface-transformer 里，**并没有使用pytorch内部提供的nn.Transformer相关的模型，而是自己重新写了一套实现逻辑**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a45b4-81c8-450e-b8a3-e74591fe8cde",
   "metadata": {},
   "source": [
    "#### BertConfig\n",
    "\n",
    "官方文档 [BertConfig](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertConfig).   \n",
    "封装了Bert的配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49445f6-1524-4db2-bb2f-5333ecff5fc5",
   "metadata": {},
   "source": [
    "#### 底层实现类\n",
    "\n",
    "huggingface-transformer的实现里，除了上述几个可以直接用于任务的模型类之外，还有一些底层的实现类没有对外暴露，这些实现类是上述模型的基础。\n",
    "\n",
    "<img src=\"images/transformer-bert.png\" width=\"50%\" align=\"center\">    \n",
    "\n",
    "\n",
    "+ `BertEmbeddings`：\n",
    "  + 用于将输入的 `input_ids`，`token_type_ids`，`position_ids` 转成词向量，并进行相加（原论文中是这么做的）  \n",
    "  .\n",
    "\n",
    "\n",
    "+ **`BertSelfAttention`-KEY**：\n",
    "  + 实现transformer的 self-attention 操作，也就是 ${\\rm softmax}(\\frac{ {\\rm query} \\cdot {\\rm key}^T}{\\sqrt{d_k}}) \\cdot {\\rm value}$ 这一部分.  \n",
    "  + 这里的self-attention操作，既可以是 encoder 里的 self-attention，也可以是 decoder 里的 self-attention\n",
    "  + 如果是 decoder 里的 self-attention，又分为两种情况：\n",
    "    + decoder的 Masked-self-attention（第一层），此时 q,k,v 都来自于输出序列的同一个\n",
    "    + decoder的 cross-self-attention（第二层），此时 q,k 来自于 encoder 的输出，v 来自于上一层的 encoder\n",
    "  + 为了兼容上述3种情况，它的`forward`方法里，可以传入的参数很多，用于控制具体实现哪里的self-attention.\n",
    "  + **输出最多为 长度=3 的 tuple**：`(context_layer, attention_probs, past_key_value)`\n",
    "    + `context_layer` 是 self-attention 最后的计算结果\n",
    "    + `attention_probs` 是 ${\\rm softmax}(\\frac{ {\\rm query} \\cdot {\\rm key}^T}{\\sqrt{d_k}})$ 的结果\n",
    "    + `past_key_value` 是一个 长度=2 的 tuple：`(key, value)`，存放的是 decoder 中第二层的 cross-self-attention 需要传入的 key,value.    \n",
    "  .\n",
    "\n",
    "\n",
    "+ `BertSelfOutput`：实现transformer里的 self-attention 之后的 **Add & Norm** 操作\n",
    "  \n",
    "+ `BertAttention`：封装了 `BertSelfAttention` + `BertSelfOutput`，形成一个 **Attention** 层\n",
    "  + 输出：和 `BertSelfAttention` 相同，只是对 `context_layer` 进行了 Add & Norm 处理，其他保持不变   \n",
    "  .\n",
    "\n",
    "\n",
    "+ `BertIntermediate`: 内部是一个 linear 层 + 激活函数，也就是 Attention 层后面的 **FeedForward** 全连接层\n",
    "+ `BertOutput`：这个和 `BertSelfOutput` 没有区别，只是为了实现 FeedForward 层后面的 **Add & Norm** 操作\n",
    "\n",
    "\n",
    "+ **`BertLayer`-KEY**：封装了 `BertAttention` + `BertAttention`(可选) +  `BertIntermediate` + `BertOutput`，构成 encoder 或者 decoder 里的一层.    \n",
    "  它提供了如下两个参数：\n",
    "  + `config.is_decoder`：为 True 时，作为 decoder 使用，输出 decoder 的结果，否则作为 encoder 使用，输出 encoder 的隐藏层状态\n",
    "  + `config.add_cross_attention`：为 True 时，会再添加一个 `BertAttention` 层，也就是 decoder 内部中间的 self-attention 层，称为 **cross-attention**   \n",
    "  \n",
    "  **输出：长度=4 的tuple**: `(context_layer, attention_probs, cross_attention_probs, past_key_value)`\n",
    "  + `context_layer` 是 输出的状态向量，它可能经过了两个 `BertAttention` 层\n",
    "  + `attention_probs` 是第一个 `BertAttentnion` 层的输出\n",
    "  + `cross_attention_probs` 是第二个 `BertAttentnion`(cross-attention) 层输出的\n",
    "  + `past_key_value` 现在可能是一个 长度=4 的tuple: `(key, value, cross_attention_key, cross_attention_value)`，前两个是第一个self-attention的输出，后两个是 cross-attention的输出.    \n",
    "  .\n",
    "\n",
    "\n",
    "+ `BertEncoder`：内部是多个作为 encoder 使用的 `BertLayer` 层   \n",
    "**输出**是一个 `BaseModelOutputWithPastAndCrossAttentions` 类，里面封装了如下结果：\n",
    "  + `last_hidden_state`：最后一个 encoder 的输出，也就是最后的 `context_layer`\n",
    "  + `hidden_states`：以 tuple 的形式记录了所有 encoder 的输出\n",
    "  + `attentions`：以 tuple 的形式记录了所有 encoder 的 `attention_probs`\n",
    "  + `cross_attentions`: 以 tuple 的形式记录了所有decoder的 `cross_attention_probs`\n",
    "  + `past_key_values`：以 tuple 的形式记录了所有decoder的 `past_key_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83326319-0d1b-4a8a-a806-d203f2c07454",
   "metadata": {},
   "source": [
    "#### 输出结构\n",
    "\n",
    "除了上述属于 transformer 架构的组件类，还提供了一些专门用于 BERT 架构的输出组件类。\n",
    "\n",
    "+ **`BertPooler`-KEY**：内部是一个 Linear + tanh激活函数，也就是一个全连接层，但是它**只会处理 `BertLayer` 输出的第一个 token 的隐藏层状态**。   \n",
    "对应 BERT 的 Masked-Language-Model 训练方式里，第一个 token `[CLS]` 的输出，**它不会改变隐藏层状态向量的维度**.\n",
    "\n",
    "+ `BertPredictionHeadTransform`：内部是一个 全连接层(Linear+激活函数) + LayerNorm，它会处理 `BertLayer` 输出的所有序列的隐藏层状态，相当于对 `BertLayer` 或者 `BertEncoder` 的输出做一次 FeedForward 处理，它**不会改变隐藏层状态向量的维度**。\n",
    "\n",
    "+ **`BertLMPredictionHead`-KEY**：内部是一个 `BertPredictionHeadTransform` + Linear，其中线性变换会改变隐藏层向量维度：`hidden_size` --> `vocab_size`。  \n",
    "名称中的\"LM\"应该指的是Language Model，也就是说这一层是为了用于语言模型对transformer-encoder的输出进行的变换。\n",
    "\n",
    "+ `BertOnlyMLMHead`：内部就是 `BertLMPredictionHead`，没有加其他的操作。  \n",
    "专门用于使用 Masked-Language-Model(MLM) 方式训练时获取transformer-encoder的输出。\n",
    "\n",
    "+ `BertOnlyNSPHead`：内部是一个 Linear变换，但是维度从 `hidden_size` -> 2。   \n",
    "专门用于使用 Next Sentence Prediction(NSP) 方式训练时将第一个token `[CLS]` 转成一个 表示二分类结果的向量。  \n",
    "它前面通常会有一个 `BertPooler` 层，用于获取第一个token `[CLS]` 的隐藏层状态向量。\n",
    "\n",
    "+ `BertPreTrainingHeads`：内部是 `BertLMPredictionHead` + 等价于`BertOnlyNSPHead`的 Linear变换.  \n",
    "这个应该是用于同时采用 MLM + NSP 方式训练BERT时，同时获取两者的输出.\n",
    "\n",
    "有了上述的 transformer 基础组件类 和 BERT 组件类的支持，就可以构建下面使用的 BERT 模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-senator",
   "metadata": {},
   "source": [
    "#### BertModel\n",
    "\n",
    "最基本的BERT架构，既可以作为 transformer-encoder 或者 transformer-decoder 使用，输出原始的隐藏层状态。\n",
    "\n",
    "查看源码时，会发现它的初始化方法里，有3个层：\n",
    "1. `self.embeddings = BertEmbeddings(config)`  \n",
    "2. `self.encoder = BertEncoder(config)`  \n",
    "这一层内部是`self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])`，其中的`BertLayer`\n",
    "3. `self.pooler = BertPooler(config) if add_pooling_layer else None`\n",
    "\n",
    "\n",
    "+ 初始化的一些重要参数：\n",
    "  + `is_decoder`\n",
    "  + `add_cross_attention`\n",
    "  + `add_pooling_layer`，是否添加最后的 `BertPooler` 层，默认为True\n",
    "  \n",
    "  上述参数中，除了`add_pooling_layer`，其他参数都是封装在 `BertConfig` 类中的。\n",
    "\n",
    "\n",
    "+ 前向传播方法：  \n",
    "`forward(input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)`\n",
    "  + `input_ids`：`shape=(batch_size, sequence_length)`，每个sequence是一个 list of int，其中每个值是该word在词典中的indice，经过embedding层之后，每个word的indice会被转换成word embedding的词向量。\n",
    "  + `attention_mask`，用于表示该位置的词语是否需要be attended，主要是处理padding的词语，这些padding的词语不需要参与计算。\n",
    "  + `token_type_ids`，用于区分成对句子中两个句子，为 {0, 1} 掩码\n",
    "  + `position_ids`，位置编码，可选\n",
    "  + `head_mask`，\n",
    "  + `inputs_embeds`，输入的embedding向量，`shape=(batch_size, sequence_length, hidden_size)`，如果不想传入`input_ids`经过内部的`BertEmbedding`转成向量，就可以在这里传入自定义的embedding表示向量，注意，这个**不能和 `input_ids` 同时使用**.\n",
    "  + `encoder_hidden_states`，作为decoder里的cross-attention使用时，需要的encoder输出就从这个参数传入\n",
    "  + `encoder_attention_mask`，\n",
    "  + `past_key_values`，包含了前一个attention block了计算的 key 和 value —— 这个参数的使用有点迷惑。\n",
    "  + `output_attentions`，是否输出所有attention layer 的结果，默认 False\n",
    "  + `output_hidden_states`，是否输出所有attention layer的隐藏层状态，默认 False\n",
    "\n",
    "\n",
    "`forward()`方法的返回值是如下的对象（在`return_dict=True`时）：\n",
    "+ `BaseModelOutputWithPoolingAndCrossAttentions`类对象，它有如下属性：\n",
    "  + `last_hidden_state`，shape=`(batch_size, sequence_length, hidden_size)`，最后一层的输出\n",
    "  + `pooler_output`，shape=`(batch_size, hidden_size)`，最后一层中 `[CLS]` token 对应的输出，经过了 `BertPooler` 的处理\n",
    "  + `hidden_states`，只有当`output_hidden_states=True`时才会返回.  \n",
    "    + 返回的是一个 长度= 1 + hidden_layers_num 的tuple, 每个元素对应于一层的隐状态（包括了 Embedding 层），\n",
    "    + 每层的隐状态 shape=`(batch_size, sequence_length, hidden_size)`\n",
    "  + `attentions`，只有当`output_attentions=True`时才会返回.  \n",
    "    + 返回一个 长度=hidden_layers_num 的tuple，对应于 每一层 的attention，也就是 `BertSelfAttention` 输出的 `attention_probs`\n",
    "    + 每一层的attention.shape=`(batch_size, num_heads, sequence_length, sequence_length)`，\n",
    "  + `cross_attentions`，只有当`output_attentions=True`时才会返回.  \n",
    "    + 返回一个 长度=hidden_layers_num 的tuple，对应于 每一层 的 cross-attention，也就是 `BertLayer`里设置`add_cross_attention=True`时输出的`cross_attention_probs`\n",
    "    + 每一层的attention.shape=`(batch_size, num_heads, sequence_length, sequence_length)`\n",
    "  + `past_key_values`，只有当`use_cache=True` 时才会返回\n",
    "  \n",
    "如果`return_dict=False`，那么返回的就是一个 tuple，其中包含的具体元素要看配置."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841cd289-c307-4257-a0dc-d8d15035ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tired-metadata",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "useful-vacuum",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at .\\bert-pretrained-models\\bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.__class__:     <class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "tokenizer.__class__:  <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "model.__class__:      <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"./bert-pretrained-models/bert-base-uncased\"   # mac\n",
    "model_name = r\".\\bert-pretrained-models\\bert-base-uncased\"    # windows\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, local_files_only=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, local_files_only=True)\n",
    "# 使用 config 对象初始化 BERT模型，但是不能 config 来初始化 tokenizer\n",
    "model = BertModel(config)\n",
    "# 或者\n",
    "# model = BertModel.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "\n",
    "print(\"config.__class__:    \", config.__class__)\n",
    "print(\"tokenizer.__class__: \", tokenizer.__class__)\n",
    "print(\"model.__class__:     \", model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "resistant-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1996,  2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,\n",
      "          2314,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[CLS] after stealing money from the bank vault [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] the bank robber was seen fishing on the mississippi river bank. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 单个句子\n",
    "# sentence = [\"After stealing money from the bank vault\", \"the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "# 两个句子\n",
    "sentence = [\"After stealing money from the bank vault\", \"the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "# return_tensors 指定返回的结果为 torch.Tensor，这样就不用做转换了，\n",
    "# 注意，必须要设置 padding，否则得到的 tensor 维度不一样，无法转成 tensors\n",
    "sentence_encode = tokenizer(sentence, padding=True, return_tensors='pt')\n",
    "\n",
    "print(sentence_encode)\n",
    "print(tokenizer.decode(sentence_encode['input_ids'][0]))\n",
    "print(tokenizer.decode(sentence_encode['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15c26e6d-220d-4973-aa6e-79e974af6a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.22.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看下 BertConfig 里的配置\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "anticipated-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将加载的预训练模型置于 evaluation 状态，这样会关闭其中的 dropout\n",
    "model.eval()\n",
    "\n",
    "# 预测时，使用 .no_grad() 方式会加快计算\n",
    "with torch.no_grad():\n",
    "    # 可以直接将 tokenizer 得到的输出作为输入，只要使用拆包技巧就行\n",
    "    outputs = model(**sentence_encode, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "outputs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "higher-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个句子: batch_size=2, 每个句子序列的长度 sequence_length=14, \n",
    "# 最后一层的 hidden_size=768 —— 这个由预训练模型的配置决定\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relevant-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对应于 [CLS] token 的 embedding，2 个句子，所以返回了两个\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "frank-consolidation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看每一个隐藏层的状态\n",
    "print(len(outputs.hidden_states))\n",
    "outputs.hidden_states.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "wrong-allergy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding 层的 隐状态\n",
    "outputs.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "isolated-order",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个 self-attention 层的 隐状态\n",
    "outputs.hidden_states[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "baking-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention 的信息\n",
    "print(len(outputs.attentions))\n",
    "outputs.attentions.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "protected-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 14, 14])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个 self-attention 层的 atttention shape\n",
    "# 2 个句子 batch_size=2, 12 个 heads, sequence_length=14, sequence_length=14\n",
    "outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-attitude",
   "metadata": {},
   "source": [
    "+ 也可以用 切片 的方式，直接从 `outputs` 中获取前两个的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "smoking-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 14, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# t1 对应于 last_hidden_state, t2 对应于 pooler_output\n",
    "t1, t2 = outputs[:2]\n",
    "print(t1.__class__)\n",
    "print(t1.shape)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "saving-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# return_dict=False，那么返回的就是一个 tuple\n",
    "outputs = model(**sentence_encode, output_hidden_states=True, output_attentions=True, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lined-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.__class__:  <class 'tuple'>\n",
      "outputs.len:        4\n"
     ]
    }
   ],
   "source": [
    "print(\"outputs.__class__: \", outputs.__class__)\n",
    "print(\"outputs.len:       \", len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "specific-cache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-trailer",
   "metadata": {},
   "source": [
    "#### BertForPreTraining\n",
    "\n",
    "在 BertModel 的输出后，增加了一层 `BertPreTrainingHeads()`，如果要自己训练一个BERT模型，从这个类出发比较方便.\n",
    "\n",
    "通过分析源码可以发现，它（并行）做了如下两件事：\n",
    "1. 调用 `nn.Linear()` 将 BertModel 输出的 `[CLS]` token （也就是pooled_output） 转换成 2 维向量，用于表示二分类的值——用于 Next Sentence 的训练方式\n",
    "2. 调用 `BertLMPredictionHead()` ，对 BertModel 最后一层的输出 last_hidden_state 进行处理——用于做 MaskLanguageModel 的训练    \n",
    "内部处理逻辑如下：\n",
    "   + 调用 `BertPredictionHeadTransform()` 对 last_hidden_state 做 线性变换 + 激活函数 + LayerNormalization，输出的 shape 不变\n",
    "   + 调用 `nn.Linear()` 将上一步得到的向量从 hidden_size 转换成 vocab_size 维度的向量\n",
    "\n",
    "\n",
    "+ `forward()`方法，参数相比于 `BertModel.forward()` 多了如下两个：\n",
    "  + `label`, shape=`(batch_size, sequence_length)`，用于计算 masked language modeling Loss 的标记.   \n",
    "  其中的取值范围应为 `[-100, 0, ..., config.vocab_size]`，也就是比 `input_ids` 的范围多了一个 -100, -100 表示对应位置的token 会被 mask，后续计算loss时，被标记为 -100 的token会被忽略掉。\n",
    "  + `next_sentence_label`, shape=`(batch_size,)`，用于计算 next sequence prediction(分类问题) Loss 的标记.   \n",
    "  取值只有 `{0, 1}`\n",
    "    + 0 表示 后一句 是连着 前一句\n",
    "    + 1 表示 后一句 是随机抽取的\n",
    "\n",
    "\n",
    "+ 返回值是一个封装的 `BertForPreTrainingOutput()` 对象（`return_dict=False`时）\n",
    "  + `loss`，shape=`(1,)`，保存了 MaskLM 的 Loss 和 next sentence prediction 的 **Loss 之和**——不懂这里为什么要求和。\n",
    "  + `prediction_logits`，shape=`(batch_size, sequence_length, config.vocab_size)`，记录了softmax之前的score\n",
    "  + `seq_relationship_logits`, shape=`(batch_size, 2)`，记录了 next sequence prediction 在 softmax 之前的 score\n",
    "  + `hidden_states`，就是 `BertModel` 的 hidden_states\n",
    "  + `attentions`，`BertModel`的对应输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "honey-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latter-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ./BERT/bert-pre-trained-models/bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.__class__:     <class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "tokenizer.__class__:  <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "model.__class__:      <class 'transformers.models.bert.modeling_bert.BertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./BERT/bert-pre-trained-models/bert-base-uncased\"\n",
    "# model_name = \"./BERT/bert-pre-trained-models/distilbert-base-uncased/\"\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, local_files_only=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, local_files_only=True)\n",
    "model = BertForPreTraining.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "print(\"config.__class__:    \", config.__class__)\n",
    "print(\"tokenizer.__class__: \", tokenizer.__class__)\n",
    "print(\"model.__class__:     \", model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "paperback-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1996,  2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,\n",
      "          2314,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 两个句子\n",
    "sentence = [\"After stealing money from the bank vault\", \"the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "# return_tensors 指定返回的结果为 torch.Tensor，这样就不用做转换了，\n",
    "# 注意，必须要设置 padding，否则得到的 tensor 维度不一样，无法转成 tensors\n",
    "sentence_encode = tokenizer(sentence, padding=True, return_tensors='pt')\n",
    "print(sentence_encode)\n",
    "\n",
    "outputs = model(**sentence_encode, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "killing-professional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForPreTrainingOutput"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjacent-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cultural-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 30522])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "closing-jaguar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.seq_relationship_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-retreat",
   "metadata": {},
   "source": [
    "#### BertForSequenceClassification\n",
    "\n",
    "这个类专门用于 序列分类，它的内部很简单：\n",
    "1. `BertModel()` 输出 pooled_output(`[CLS]` token 对应的 hidden_state)\n",
    "2. `nn.Dropout()` 处理 pooled_output\n",
    "3. `nn.Linear()` 映射到 多分类的类别 logits\n",
    "\n",
    "\n",
    "+ `forward()`方法\n",
    "\n",
    "+ 返回的是`SequenceClassifierOutput()`类对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ce60a-505d-42d4-8119-2bc8a09da311",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Distil-BERT\n",
    "\n",
    "[DistilBERT Models](https://huggingface.co/transformers/model_doc/distilbert.html) 包含如下类:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2964f75b-6333-4bec-8ca7-3fa158453640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720c3938-0bcc-41d9-8ccb-b5545232fd2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## 训练与Fint-Tune\n",
    "\n",
    "huggingface官方提供了一个使用transformer的 [Course](https://huggingface.co/course/chapter0/1?fw=pt)，这个教程写的很不错。\n",
    "\n",
    "如果想自己训练一个模型，可以参考其中的如下内容：\n",
    "+ [Course --> 6. The Tokenizer Library](https://huggingface.co/course/chapter6/1?fw=pt)\n",
    "  + [Training a new tokenizer from an old one](https://huggingface.co/course/chapter6/2?fw=pt#training-a-new-tokenizer-from-an-old-one)\n",
    "  + [Building a tokenizer, block by block](https://huggingface.co/course/chapter6/8?fw=pt#building-a-tokenizer-block-by-block)\n",
    "+ [Course --> 7. Main NLP Task](https://huggingface.co/course/chapter7/1?fw=pt)\n",
    "  + [Fine-tuning a masked language model](https://huggingface.co/course/chapter7/3?fw=pt#finetuning-a-masked-language-model)\n",
    "  + [Training a causal language model from scratch](https://huggingface.co/course/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch)\n",
    "  \n",
    "  \n",
    "除了上述两个教程之外，huggingface在`transformer`模块里，还提供了如下类作为辅助工具：\n",
    "+ `DataCollator`：用于补齐填充数据\n",
    "+ `Trainer`：封装训练过程，**只适用于PyTorch**框架"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd55dbb-6da6-4ec6-88b2-cfb035b8d896",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "### DataCollator\n",
    "\n",
    "官方文档：\n",
    "+ [API：Data Collator](https://huggingface.co/docs/transformers/main_classes/data_collator)\n",
    "\n",
    "DataCollator 用于从 list of dataset elements 中生成一个 batch 的数据，对该batch的数据进行如下的一些操作：\n",
    "+ 对batch中的每个样本序列进行对齐padding操作 —— 注意，**这个序列对齐只针对某个batch，不同batch的序列长度可以是不一样的**\n",
    "+ 用于 Masked Language Model 时，对输入的序列进行 Mask 处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104331fc-f424-4885-8402-00728a70e949",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DefaultDataCollator\n",
    "默认的collator，**它会检测 dataset 中是否有 `labels` 或者 `label_ids` 这两个特征，有的话，会单独处理**。\n",
    "\n",
    "DataCollator的输入是 list of dict-like 的内容，输出是一个 dict-like，每个 key 是一个特征，value 是 list，对应于该特征下的 batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2913bf29-a7a7-4738-9cc8-aaecc07ea5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([1, 2, 3]),\n",
       " 'col-1': tensor([10, 11, 12]),\n",
       " 'col-2': tensor([20, 21, 22])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以 DefaultDataCollator 为例\n",
    "from transformers import DefaultDataCollator\n",
    "dc = DefaultDataCollator()\n",
    "\n",
    "data_list = [\n",
    "    {'label': 1, 'col-1': 10, 'col-2': 20},\n",
    "    {'label': 2, 'col-1': 11, 'col-2': 21},\n",
    "    {'label': 3, 'col-1': 12, 'col-2': 22}\n",
    "]\n",
    "\n",
    "dc_data = dc(data_list)\n",
    "dc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908c577-2e06-47d1-ba27-c5ad1f5d2b19",
   "metadata": {},
   "source": [
    "#### DataCollatorWithPadding\n",
    "\n",
    "用于动态对每个 batch 的样本进行 padding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91c2bb-3b90-4e67-b11c-a3c1aa9c5d49",
   "metadata": {},
   "source": [
    "#### DataCollatorForLanguageModeling\n",
    "\n",
    "用于对 Language Model 的输入进行处理，比如对于BERT 的输入会执行 随机Mask 的操作.\n",
    "\n",
    "注意：\n",
    "> 如果使用了 WordPiece 之类的分词器，获得 subword 时（比如 `hug`,`##ging` 之类的词），这个类 Mask 的是 subword，不会 Mask 整个word （这里是`hugging`）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8163c4c-7dc8-4fdd-9fd9-e0ff96c1a8e8",
   "metadata": {},
   "source": [
    "#### DataCollatorForWholeWordMask\n",
    "\n",
    "用于对 Language Model 的输入进行处理，比如对于BERT 的输入会执行 随机Mask 的操作.\n",
    "\n",
    "即使是使用 Subword 分词器，这个类 Mask 的还是整体的word ，不过要求其中的 subword 必须以指定的符号为前缀（比如`##`）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018520f6-91e7-4e24-a390-6382f84288b0",
   "metadata": {},
   "source": [
    "-----\n",
    "### Trainer\n",
    "\n",
    "官方文档：\n",
    "+ [Transormer --> Main Class --> Trainer](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.Trainer)：API文档\n",
    "+ [Transformer --> Tutorial --> Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer)：`transformer`包的教程，比较详细，除了介绍`Trainer`的使用外，还有 Keras 和原始 pytorch 的训练代码\n",
    "+ [Cource --> 3. Fine-Tuning a pretrained model --> Fine-tuning a model with the Trainer API](https://huggingface.co/course/chapter3/3?fw=pt#fine-tuning-a-model-with-the-trainer-api)：Course里的Trainer教程，可以作为上面教程的补充\n",
    "\n",
    "\n",
    "Trainer-API的使用主要分为两个部分：\n",
    "1. 使用`TrainingArguments`类封装所有的超参数，具体参数可以查看该类的API文档 [Trainer --> TrainingArguments](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "2. 实例化`Trainer`类\n",
    "\n",
    "Trainer-API 主要提供了如下 **两组** 共 **4 个类**：\n",
    "+ `TrainingArguments` + `Trainer`，用于普通模型的训练\n",
    "+ `Seq2SeqTrainingArguments` + `Seq2SeqTrainer`：用于序列模型的训练，这两个类分别继承了上面的类，`Seq2SeqTrainingArguments`扩充了几个属性，`Seq2SeqTrainer` 重写了几个父类方法\n",
    "\n",
    "如果要自定义一些训练流程，可以继承`Trainer`类，然后重写其中的如下方法：\n",
    "+ `trainning_step()`\n",
    "+ `compute_loss()`\n",
    "+ `evaluate()`\n",
    "+ `prediction_step()`\n",
    "+ `predict()`\n",
    "\n",
    "需要注意的是，Trainer-API是专门为`transformer`包里的模型定制的训练流程，所以继承重写上述方法的时候，需要遵循一些约定，否则Trainer-API会出现意想不到的结果\n",
    "\n",
    "\n",
    "> **稍微看了下`Trainer`的源码，发现其中添加了许多逻辑，比如参数检查，最大batch_size的检查，各种回调方法，多GPU的支持，梯度修剪等等。  \n",
    "我的感觉是，这个API可以用，但是不要滥用，其中封装了许多对于初学者来说不必要的逻辑。  \n",
    "不过如果作为研究的对象还不错，其中有许多优化求解的操作，应该值得研究。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09736c9-6677-40be-b415-2067578a2764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}