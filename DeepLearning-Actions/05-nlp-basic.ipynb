{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5399e89-e882-4d28-a9a6-641573aabddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.21.5\n",
      "pandas:      1.4.3\n",
      "matplotlib:  3.5.2\n",
      "sklearn:     1.1.1\n",
      "seaborn:     0.11.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# from plotly import graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "# import plotly\n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "from time import time\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"pandas:     \", pd.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"seaborn:    \", sns.__version__)\n",
    "# print(\"plotly:     \", plotly.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c171c0f-5954-467b-a750-2315e2ffc9f2",
   "metadata": {},
   "source": [
    "# NLP任务概述\n",
    "\n",
    "介绍NLP领域的基本概念和基本任务。  \n",
    "\n",
    "NLP领域常见的基本任务如下：  \n",
    "+ **分词**（segment）\n",
    "+ **词性标注**（part-of-speech tagging）\n",
    "+ **命名实体识别**（NER, Named Entity Recognition）\n",
    "+ 句法解析（syntax parsing）\n",
    "+ 指代消解（anaphora resolution）\n",
    "+ **关键词提取**\n",
    "+ **文本向量化**\n",
    "+ **词向量** --- KEY\n",
    "\n",
    "其中黑体表示这里会涉及的部分。\n",
    "\n",
    "在上述基础任务之上，有许多的顶层任务可以研究，比如：\n",
    "+ 情感识别\n",
    "+ 文本分类\n",
    "+ 问答系统\n",
    "\n",
    "以下主要对NLP中的**基础任务**做一些总结和介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb7a33-cf3e-406a-b63d-3acd5cac7705",
   "metadata": {},
   "source": [
    "# 分词\n",
    "\n",
    "在文本分析的语境中，数据集通常被称为语料库（corpus），每个由文本表示的样本被称为文档（document）。\n",
    "\n",
    "所有NLP任务，首先绕不开的两步如下：\n",
    "1. 分词（tokenization），比如英文可以按照空格划分，划分之后的每个词被称为词例（token）\n",
    "2. 分词过后，就可以构建所有文档中出现的单词集合——词表（Vocabulary）\n",
    "\n",
    "对于中英文来说，分词这一步骤的侧重点不太一样。\n",
    "\n",
    "## 英文分词\n",
    "\n",
    "对于英文来说，划分成token这一步比较容易，因为英文天然有空格作为划分依据。   \n",
    "\n",
    "英文分词要面临的主要问题是**单词会有单复数、各种时态**等形式，处理这个问题可以通过**用词干（word stem）表示每个单词来解决**，通常有两种方式：\n",
    "1. 基于规则的启发法（比如删除常见的后缀），通常将其称为**词干提取（stemming）**\n",
    "2. 使用由已知单词形式组成的字典（明确的且经过人工验证的系统），通常称为**词形还原（lemmatization）**\n",
    "\n",
    "一般说来，**词形还原相比于词干提取更加好用**。\n",
    "\n",
    "此外，英文分词还有停用词、标点符号去除的步骤。\n",
    "\n",
    "\n",
    "## 中文分词\n",
    "对于中文分词来说，词的划分是个比较麻烦的问题，一般有如下3类方法：\n",
    "+ 基于规则的分词\n",
    "+ 基于统计的分词\n",
    "+ 混合分词\n",
    "\n",
    "中文分词的主要工具是jieba分词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239a20a-88be-4669-b24c-57576937413c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed670e-d8d5-4f09-908e-ac081e784c15",
   "metadata": {},
   "source": [
    "# 词性标注与命名实体识别\n",
    "\n",
    "## 词性标注\n",
    "\n",
    "词性标注最朴素简单的一个方法是从语料库中**统计每个词所对应的高频词性，将其作为默认词性**，不过显然还有提升空间。\n",
    "\n",
    "主流的方法是，**将句子的词性标注作为一个序列标注问题**来解决，这样就有两类方法来进行处理：\n",
    "1. 传统模型，比如隐马尔科夫链，条件随机场的方法；\n",
    "2. 深度学习的Seq2Seq模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf922f-04ea-4150-ae4b-61c55a278b51",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29de0e-b455-47e4-a360-00a0e8c914fc",
   "metadata": {},
   "source": [
    "## 命名实体识别\n",
    "\n",
    "NER的目的是识别出语料中的专有名词，比如人名、地名、组织机构等命名实体，不过这个领域当前并不是一个热门的研究方向。\n",
    "\n",
    "NER的研究方法也分为如下3类：\n",
    "+ 基于规则\n",
    "+ 基于统计，也就是基于人工标注的语料，**将命名实体识别任务作为序列标注问题**来解决——这是当前比较主流的做法\n",
    "+ 混合方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d818c-6bf2-442f-b2eb-46f0a0bf6876",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081afd91-a949-4774-9ea1-5f6bf1c8d1fd",
   "metadata": {},
   "source": [
    "# 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe4a18d-a9d8-43f7-a388-128c12260c0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49486544-a990-49ab-905f-ab94a4de2c13",
   "metadata": {},
   "source": [
    "# 文本向量化\n",
    "\n",
    "文本向量化指的是将一段文本转换成向量的表示形式，该文本的主要信息都包含在向量中，主要有如下两个类型的方式：\n",
    "+ 基于计数的表示：汇总统计文本中出现的单词，形成单词表，作为特征列，基于单词的频数构建特征，比如词袋表示和TF-IDF\n",
    "+ 基于词向量的表示：也就是将文本中的单词转成词向量，用这一系列的词向量序列表示文本，这里的重点是单词转词向量，见下一节\n",
    "\n",
    "这一节主要介绍**基于计数的表示法**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abb4e0-c23a-4a17-a2f6-d43eacd1f9e7",
   "metadata": {},
   "source": [
    "## 词袋表示\n",
    "\n",
    "词袋(Bag-of-words)是最基本的文本向量化方法，就是舍弃输入文本中的大部分结构，如章节、段落、句子和格式，**只计算语料库中每个单词在每个文本中的出现频次** .  \n",
    "\n",
    "Python中，对文本进行词袋特征提取常用的两个方式为：\n",
    "1. 使用`collection.Counter`手动构建词袋；\n",
    "\n",
    "\n",
    "2. 使用`sklearn.feature_extraction.text.CountVectorizer()`自动构建词袋\n",
    "\n",
    "`CountVectorizer()`，用于将文本转换成count. \n",
    "+ 参数\n",
    "  + `input='content'`，输入的类型，可以是文本，文件，字符串等\n",
    "  + `strip_accents=None`, 移除 accent 的方式\n",
    "  + `lowercase=True`, 是否转成小写\n",
    "  + `preprocessor=None`, 自定义预处理过程\n",
    "  + `tokenizer=None`,  自定义分词器\n",
    "  + `stop_words=None`, 停用词集合\n",
    "  + `token_pattern='(?u)\\b\\w\\w+\\b'`, 分词的正则表达式\n",
    "  + `ngram_range=(1, 1)`, tuple (min_n, max_n), n-gram的范围\n",
    "  + `analyzer='word'`, 取值 {‘word’, ‘char’, ‘char_wb’}, 分词的粒度\n",
    "  + `max_df=1.0`, 超过此阈值的词会被忽略，有两种设置方式\n",
    "    + 0~1 之间的float\n",
    "    + int，\n",
    "  + `min_df=1`, 单词最小出现次数阈值\n",
    "  + `max_features=None`, int，最大的特征值数量，按照频次排序，取top\n",
    "  + `vocabulary=None`, 自定义词典\n",
    "  + `binary=False` \n",
    "+ 属性\n",
    "  + `vocabulary_`：dict\n",
    "  + `stop_words_`：set\n",
    "  + `fixed_vocabulary_`: boolean\n",
    "+ 常用方法\n",
    "  + `fit()`,`transform()`\n",
    "  + `get_stop_words()`\n",
    "  + `get_feature_names()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445cc32a-ffa8-454d-8744-be4a052e0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0551fef-a46b-475f-91b4-1ae2e0f6989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6162a93-388a-4b14-9ddc-6ff54ca0baf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "count_vec.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de04ddd8-4f14-425b-8707-76fed0667565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取 单词字典表\n",
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb75ba0-e5a3-4b89-b175-c3c93839feeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取字典中的词，顺序就是上述字典表定义的顺序\n",
    "count_vec.get_feature_names_out()\n",
    "# 旧版本的API\n",
    "# count_vec.get_feature_names()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f23163-f09d-418c-a1d9-3def0f1e03a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取停用词\n",
    "count_vec.stop_words_\n",
    "# 或者\n",
    "# count_vec.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b90936-c13c-4d01-ac58-43d24318825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接得到的 X 是个稀疏矩阵\n",
    "X = count_vec.transform(corpus)\n",
    "print('X type: ', type(X))\n",
    "\n",
    "# 需要进行转换\n",
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab89d8a-3bb1-4bc1-8133-7033dd6cb8f1",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cace44-10cc-427b-ab14-a42e31d63b4c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff50cdf-438d-4ead-817c-a278e71e04ca",
   "metadata": {},
   "source": [
    "# 词向量-KEY\n",
    "\n",
    "词向量（Word-Embedding）是NLP非常重要和基础的一步，几乎后续所有的NLP顶层任务都依赖于词向量构建的质量，所以这个话题相关的研究非常之多，有用的一些参考资料如下：\n",
    "+ [博士论文《基于神经网络的词和文档语义向量表示方法研究》](https://licstar.net/archives/687)：来斯惟的这篇博士论文是我看过的最为详细的介绍词向量历史和概念的资料，非常值得推荐\n",
    "+ [秒懂词向量Word2vec的本质](https://zhuanlan.zhihu.com/p/26306795)：这里介绍了不少词向量相关的资料，也很不错\n",
    "+ [word2vec Parameter Learning Explained](http://arxiv.org/abs/1411.2738)：这篇文章介绍的也很详细，非常严谨\n",
    "+ [The Illustrated Word2vec](http://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "在详细介绍词向量之前，需要澄清一个概念，词向量又称词嵌入（Word-Embedding）：\n",
    "+ 广义上来说，是**基于分布假说（上下文相似的词，其语义也相似）** 的词表示方法统称；\n",
    "+ 狭义来说，特别指的是基于浅层神经网络构建的分布式词表示，这个狭义的用法比较多，所以下面**无特别说明，使用的是这个狭义的解释**\n",
    "\n",
    "基于神经网络实现的 Word-Embedding 是一个广泛的概念，实现这一概念的技术有很多种，最为出名的就是Google的Word2Vec，所以常常有人把Word-Embedding和Word2Vec混为一谈，这样不太严谨，实际上，除了Word2Vec，还有其他实现Word-Embedding的方式，比如 GloVe, wordRank, FastText (Facebook)等。\n",
    "\n",
    "\n",
    "基于分布假说的词表示方法，根据建模的不同，常见的有如下3种：\n",
    "1. 基于计数（Count-Based）的分布式表示，也被称为基于矩阵的分布式表示，比如 GloVe 词向量\n",
    "2. 基于聚类的分布表示——这个其实用的不多\n",
    "3. 基于神经网络的分布表示——这个用的最多\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e3727-6152-434c-9432-a3567739c308",
   "metadata": {
    "tags": []
   },
   "source": [
    "## one-hot\n",
    "\n",
    "one-hot表示就是汇总所有的文本语料，得到其中出现的单词词典 $V$，然后将每个单词表示成一个长度为 $|V|$ 维的向量，对应词出现的位置为 1 .\n",
    "\n",
    "这个表示方式产生了大量的稀疏特征和稀疏矩阵，效率很低，不过这种表示法常常是后续词向量的一个出发点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6dbc0d-6c2e-46d5-8cb8-a89f631a6363",
   "metadata": {},
   "source": [
    "## Word-Embedding基础模型\n",
    "\n",
    "### 神经网络语言模型NNLM\n",
    "\n",
    "### C&W模型\n",
    "\n",
    "### CBOW模型\n",
    "\n",
    "\n",
    "### Skip-Gram模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae68ad-2713-435f-95e6-e9386a42c165",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8b88f-b016-412f-828d-abe4cc0cec70",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e299fc-7704-4fa5-a3fc-2a96561d0f9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a17e9-572f-4c64-9c35-ede48bc159b9",
   "metadata": {},
   "source": [
    "## 上下文相关的词向量\n",
    "\n",
    "上一节中的词向量有一个问题，就是同一个词，在不同的句子中（上下文），可能有不同的含义，比如下面的两个句子中的bank意思是不一样的：\n",
    "+ They stood on the river **bank** (河岸) to fish.\n",
    "+ Have you paid that money to the **bank** (银行) yet ?   \n",
    "\n",
    "为了解决这类问题，发展出了**上下文相关的词向量（Contextualized Word-Embedding）**，这类模型通常是基于Seq2Seq模型在大量的文本语料上训练得到的，比较有名的有：\n",
    "+ ELMO (Embeddings from Language Model)\n",
    "+ BERT (Bidirectional Encoder Representations from Transformers)\n",
    "+ ERNIE (Enhanced Representation through Knowledge Integration)\n",
    "+ GPT (Generative Pre-Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3bfcb-08ad-44e7-9270-6c9bb0bda8af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca825d8-e3b1-4be6-8be5-48519b2bab69",
   "metadata": {},
   "source": [
    "# NLP常用工具包\n",
    "\n",
    "NLP基础任务中，常用的Python工具包有如下3个：\n",
    "+ NLTK：老牌的NLP处理工具，主要是针对英语，提供了分词，词性标注，NER和语法分析树等功能，在早期的NLP学术研究中用的比较多\n",
    "  + 纯Python写成，生产环境下速度比较慢\n",
    "  + 没有现代深度学习模型的支持\n",
    "+ spaCy：一个用于高级自然语言处理的开源软件库 —— KEY\n",
    "  + 和NLTK功能基本重合\n",
    "  + 使用Python和CPython混合写成，生产环境下速度更快，适合工业开发\n",
    "  + 提供了GPU支持\n",
    "  + 提供预训练的Pipeline支持\n",
    "+ gensim：主要针对文本的主题模型生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92068fb1-20f4-4724-9b99-04d477d42b1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## NLTK\n",
    "\n",
    "可供参考的官方文档：\n",
    "+ [NLTK](https://www.nltk.org/#)：官网首页\n",
    "+ [NLTK Python Module Index](https://www.nltk.org/py-modindex.html)：API结构\n",
    "\n",
    "\n",
    "1. 分词.  \n",
    "NLTK提供了`nltk.tokenize`这个模块，主要是如下两个分词**函数**（不是类）\n",
    "  + `nltk.tokenize.sent_tokenize(text, language='english')`  \n",
    "用于分割句子\n",
    "  + `nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)`  \n",
    "用于分割单词\n",
    "\n",
    "\n",
    "2. 词干提取.  \n",
    "NLTK提供了`nltk.stem`这个模块.\n",
    "  + `nltk.stem.porter.PorterStemmer()`类，封装了Porter提取算法\n",
    "  + `nltk.stem.snowball`模块里提供了非英语类词干的提取方法\n",
    "  + `nltk.stem.wordnet.WordNetLemmatizer()`类，提供了词形还原的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b6f26e-3444-4bff-b1d2-859006543412",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "官方文档 [spaCy](https://spacy.io/) —— spaCy的官方文档写的挺好的。\n",
    "\n",
    "spaCy的使用和NLTK完全不同，NLTK大体上还是面向过程的调用函数方式来完成任务，而spaCy是采用的pipeline方式完成.  \n",
    "\n",
    "spaCy中，训练好的模型被称为 **trained pipeline**，这些pipilines实际上也是单独的一个python package。\n",
    "\n",
    "spaCy的官网中，提供了一个有关文本处理术语的简单教程 [spacy-101](https://spacy.io/usage/spacy-101)，可以看一看。\n",
    "\n",
    "spaCy的使用步骤如下：\n",
    "1. 载入语言模型.  \n",
    "每个语言模型里，基于所选的语言，封装了一系列对文件进行处理的Pipeline.\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "```\n",
    "2. 将需要处理的文本传递给语言模型，生成一个`Doc`对象——它包含了一系列对文本进行处理的步骤，封装成一个Pipeline，其中第一步就是分词\n",
    "```python\n",
    "doc = nlp(\"Text to be process\")\n",
    "```\n",
    "3. 调用`Doc`对象的各种方法，获取不同的内容."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7997ba-05cc-42c8-803d-0f4d2916ae4b",
   "metadata": {},
   "source": [
    "## gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70a04-40d8-4301-a87c-171ad2e023e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis(Python3.8.12)",
   "language": "python",
   "name": "data-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}