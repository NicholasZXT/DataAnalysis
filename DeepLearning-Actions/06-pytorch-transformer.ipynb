{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d15e404a-ba0a-41d8-8d5d-5dcfcace6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.24.3\n",
      "PyTorch:      1.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# import sklearn\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from IPython.display import display\n",
    "# plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "# print(\"pandas:     \", pd.__version__)\n",
    "# print(\"matplotlib: \", matplotlib.__version__)\n",
    "# print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"PyTorch:     \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b046b5-a241-40da-86bd-f9c362d5b210",
   "metadata": {},
   "source": [
    "# Pytorch-Transformer\n",
    "\n",
    "参考文档:\n",
    "+ [Pytorch tutorial -> Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "+ [Pytorch -> Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "\n",
    "这里介绍了Pytorch里 transformer 层的实现模块.\n",
    "\n",
    "主要有如下几个类：\n",
    "+ `nn.Transformer`：一步到位构建整个Transformer模型，底层使用的是下面的API\n",
    "+ `nn.TransformerEncoder`：Encoder层，包含了多个 `TransoformerEncoderLayer`\n",
    "+ `nn.TransformerDecoder`：Decoder层，包含了多个 `TransformerDecoderLayer`\n",
    "+ `nn.TransformerEncoderLayer`：单层Encoder\n",
    "+ `nn.TransformerDecoderLayer`：单层Decoder\n",
    "\n",
    "但是需要注意的是，上述各种层的**底部都是调用的`activation.py`中的`MultiheadAttention`类**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3ba02-af2e-4f69-b9d6-9e0f2d6e0395",
   "metadata": {},
   "source": [
    "## Multi-Head Attention Layer\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention).\n",
    "\n",
    "它是`TransformerEncoderLayer`和`TransformerDecoderLayer`层中 **self-attention 层**的实现。  \n",
    "需要注意的是，pytorch源码中 `MultiheadAttention` 被放在了 Non-linear Activations 这一类中（并且是放在`activation.py`文件中的）。\n",
    "\n",
    "\n",
    "### 实例化参数\n",
    "`MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)`  \n",
    "+ `embed_dim`，整个MHA的输入/输出特征维度。  \n",
    "  **原始论文里，MHA的输入/输出特征维度可以是不一样的，但是在Pytorch的实现里，强制要求它们是一样的**（这个限制未来版本可能会取消）。\n",
    "+ `num_heads`，head数量，注意，分配到每个head的维度 = embed_dim/num_heads.\n",
    "+ `kdim`，自定义 key 中的特征维度，它是下面`forward()`方法里`key`对应的特征维度，默认下=`embed_dim`\n",
    "+ `vidm`，自定义 value 中的特征维度，`forward()`方法里`value`对应的特征维度，默认=`embed_dim`\n",
    "+ `bias`，\n",
    "+ `add_bias_kv`\n",
    "+ `add_zero_attn`\n",
    "+ `batch_first`，早期版本的pytorch中，会要求下面`forward()`方法里输入数据的各个维度顺序必须为：`(序列长度，batch_size, embedding_dimension)`，后来提供了这个`batch_first`参数，用于适应一下输入数据的维度变化.\n",
    "\n",
    "\n",
    "### 前向传播\n",
    "\n",
    "`MultiheadAttention.forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)`\n",
    "+ `query`, `key`, `value`，MHA的三个输入矩阵——这三个输入向量会被用于生成对应的Q,K,V\n",
    "  + `query.shape` = $(L, N, E_q)$，`key.shape` = $(S, N, E_k)$，`value.shape` = $(S, N, E_v)$.   \n",
    "    + $N$ 是 batch size\n",
    "    + $E$ 是 embedding 维度，$E_q$=`embed_dim`, $E_k$=`kdim`, $E_v$=`vdim`\n",
    "    + $L$ 是**target**序列的长度——因为它决定了MHA输出的序列长度\n",
    "    + $S$ 是**source**序列的长度\n",
    "  + Transformer中MHA会**在3个地方使用，这3个地方会影响 query, key, value 的内容**\n",
    "    + Encoder中 query, key, value 都是来自于样本的**输入序列**，此时 $L=S$，`kdim == vdim`\n",
    "    + Decoder的 **Masked MHA** 中 query, key, value 都来自于样本的**输出序列**，此时 $L=S$, `kdim == vdim`\n",
    "    + Decoder的 **decoder-encoder attention**中，query 来自于样本的**输出序列**，key, value 都是**Encoder最后一层的输出**，此时可能 $L\\neq S$, `kdim != vdim`\n",
    "  + **有关序列维度的说明**\n",
    "    + 输入的 `query`, `key`, `value` 不是self-attention的输入，它们需要经过各自的投影矩阵 $W_Q \\in (E_q, E_q)$, $W_K \\in (E_d, E_k)$, $W_V \\in (E_d, E_v)$ 的转换，上面这3个参数矩阵的维度，就是MHA里初始化的维度\n",
    "    + 线性投影过程如下（这里把batch的维度提前了）：   \n",
    "    $q = query * W_Q^T \\in (N, L, E_d)$,   \n",
    "    $k = key * W_K^T \\in (N, S, E_d)$,   \n",
    "    $v = value * W_V^T \\in (N, S, E_d)$  \n",
    "    + 可以看出，参数的 `kdim` 和 `vdim` 只是用于设置投影矩阵 $W_K, W_V$ 的维度，这两个矩阵的另一个维度被设置成了 $E_q$=`embed_dim`，经过上述三个投影矩阵的变换之后，`query`, `key`, `value` 的维度都变成了 $E_q$，也就是 `embed_dim`\n",
    "    + 后续进行的MHA操作为：\n",
    "    $${\\rm softmax}(\\frac{q*k^T}{\\sqrt{E_d}}) * v$$\n",
    "    $$((N, L, E_d) * (N, E_d, S)) * (N, S, E_d) \\rightarrow (N, L, E_d)$$\n",
    "    调用的是`torch.bmm()`算子，会在每个批次上做上述操作。\n",
    "    + 可以看出，最终决定输出序列长度的是 `query` 的序列长度 $L$，而 `key` 和 `value` 的序列长度必须一致，它们会在中间计算过程中相互抵消掉。\n",
    "\n",
    "\n",
    "+ `key_padding_mask`：填充位置的掩码，shape=$(N, S)$   \n",
    "  + 对于一个batch的训练数据来说，batch size 为 $N$ 表示有 $N$ 个序列，但是**每个序列的长度（也就是单词个数）是不一样的**，`key`,`value`中的 $S$ 都是指最长序列的长度，其他的序列就需要对缺少的单词位置做填充(padding)，padding位置的词通常使用全为0的embeding 来表示。   \n",
    "  + 为了不让MHA对这些位置的词产生注意力，需要记录这些padding位置，每个batch都使用这样一个 $(N, S)$ 的矩阵来标识padding的位置，该矩阵的每一行对应batch中的一个序列，值为`[False, False, True, True]` 或者 `[0, 0, 1, 1]` 的形式，其中结尾`True`或者1标识的就是padding的位置。   \n",
    "  + 需要注意的是，这里padding矩阵实际生效位置是在`query`和`key`做完矩阵乘法得到shape=$(N,L,S)$的中间矩阵 P 之后才使用的，由于只有`key`使用了padding掩码，只需要在 $N$ 个序列中对 $S$ 这一个维度标识padding位置即可，所以使用的padding矩阵shape=$(N,S)$. pytorch中的实现就是对中间矩阵 P 的 $S$ 维度，按照 `[False,False,True,True]` 中的标记，对应`True`的位置赋值为`-inf`，从而在后续的softmax中，该位置得到的概率为0.\n",
    "\n",
    "\n",
    "+ `attn_mask`，**只在Decoder中使用**，主要是解码时不让当前位置的词和后面要预测的词序列进行Attention.   \n",
    "  + shape=$(L,S)$ 或者 $(N * num\\_heads,L,S)$，如果是 2D 的矩阵，那么batch中的每个序列都会使用这个，如果是 3D 的矩阵，则会指定每个batch 的掩码\n",
    "  + 它的生效位置也是在`query`和`key`相乘得到shape=$(N,L,S)$的中间矩阵 P 之后使用，此时batch中**每个**序列的输入`query`部分（shape=$(L,E)$）和`key`部分（shape=$(S,E)$）相乘得到了一个 $(L,S)$ 的矩阵 P，矩阵中列表示`query`里的每个词，行表示`key`中的每个词，如果不想让`query`中第 $i$ 个词看到`key`中第 $j$ 个词之后的内容，只需要让 `P[i, j:]` 这部分为 `-inf` 即可，因此这个掩码的shape=$(N,L,S)$.\n",
    "  + 如果是在Decoder中的**Masked MHA**这里使用，此时输入的`query`,`key`,`value`都是来源于同一个地方，`query`和`key`维度相等，此时 P 是一个方阵，并且右上三角均为`-inf` —— 这是好理解的部分，也是正常它要解决的问题部分。\n",
    "  + 如果是在Decoder中的**encoder-decoder attention**层使用，此时输入的`query`是来自于之前的Masked MHA层，属于输出序列的内容，但是`key`,`value`是来自于Encoder，属于输入序列的内容，此时才会出现`query`和`key`长度不等的情况——这种情况下虽然也可以使用掩码，但是它的含义就不那么好解释了。\n",
    "\n",
    "\n",
    "+ `need_weights`，bool，表示是否需要返回`attn_output_weigths`，默认为`True`\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值\n",
    "  + `attn_output`，shape= $(L, N, E_q)$，$L$ 由 输入的`query`序列长度决定，$E$=`embed_dim`.\n",
    "  + `attn_output_weights`，shape=$(N, L, S)$，这个权重应该是每个输入序列中，每个word的attention权重.\n",
    "  \n",
    "\n",
    "### 一些说明\n",
    "\n",
    "Pytorch中的Multi-Head Attention的实现和论文《All you need is transformer》中有一些不一样的地方，总结如下：\n",
    "\n",
    "+ pytorch在实现MHA时，强制要求了输入的特征维度和输出的特征维度必须一致，也就是实例化时的`embed_dim`参数，但是在原论文中没有这个限制。   \n",
    "原论文中，输入的特征维度是`query`里的embed_dim，输出的特征维度由`value`的embed_dim决定。\n",
    "\n",
    "+ pytorch中MHA实例化参数 `kdim`,`vdim` 和含义和原论文里的 $d_k$, $d_v$ 不一样，如下所示.   \n",
    "原论文中的 $d_k$, $d_v$ 指的是 query, key 经过参数矩阵 $W$ 变换之后的 embed_dim，而pytoch中的 `kdim`,`vdim` 指定的是输入的 `key`,`value` 中，没有经过 $W$ 变换之前的特征维度，$W$ 这个参数矩阵设置的时候，其中一个维度就是 `kdim` （或者 `vdim`），另一个维度就是 `embed_dim`，因此 `key`, `value` 经过 $W$ 变换之后，维度就变成了 `embed_dim` ——这是 pytorch强制规定的。\n",
    "\n",
    "<img src=\"images/multi-head-attention-formula.png\" width=\"60%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e3a31-fc85-4651-81c1-25e238fc00b4",
   "metadata": {},
   "source": [
    "### 使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33fbc1e-678b-42a3-a226-835de4a60a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim, num_heads = 512, 8\n",
    "batch_size, target_len, source_len = 5, 10, 20\n",
    "\n",
    "query = torch.rand(target_len, batch_size, embed_dim)\n",
    "key = torch.rand(source_len, batch_size, embed_dim)\n",
    "value = torch.rand(source_len, batch_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d616872f-2a86-4e82-9175-7eac5ff7942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 512])\n",
      "torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "print(attn_output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e79f5f5-4ffa-4f35-989a-61dc70e70254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 512])\n",
      "torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "kdim, vdim = 256, 256\n",
    "key = torch.rand(source_len, batch_size, kdim)\n",
    "value = torch.rand(source_len, batch_size, vdim)\n",
    "\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, kdim=kdim, vdim=vdim)\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "print(attn_output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe17d8-066c-4704-8fd2-4c2befb06090",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoder Layer\n",
    "\n",
    "### 单层Encoder\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)，它内部包含了一个 self-attention 层 + 一个 feedforward 层.\n",
    "\n",
    "`nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`  \n",
    "\n",
    "+ 实例化参数\n",
    "  + `d_model`，输入序列中，每个word的特征个数——它同时也决定了输出序列里每个word的特征个数\n",
    "  + `nhead`，multiheadattention中的head个数\n",
    "  + `dim_feedforward`，这个维度设置的是前馈神经网络的节点个数，前馈神经网络的输出节点数还是 `d_model`\n",
    "  + `dropout`，dropout比例，默认 0.1  \n",
    "\n",
    "\n",
    "+ 前向传播`forward(src, src_mask=None, src_key_padding_mask=None)`\n",
    "  + `src`，输入的sequence, `shape` = $(S, N, E)$\n",
    "  + `src_mask`，输入sequence的mask\n",
    "  + `src_key_padding_mask`，每个batch中src的padding矩阵  \n",
    "\n",
    "\n",
    "+ `forward()`方法返回值  \n",
    "  它返回的是和 `src` shape 相同的 tensor.\n",
    "\n",
    "\n",
    "Encoder的内部只有**一层**`MultiheadAttention`：  \n",
    "`Encoder.forward()`实际执行时，调用方式为`MultiheadAttention.forward(src, src, src)`——`src`会同时作为 query, key, value 传入 self-attention。\n",
    "\n",
    "\n",
    "\n",
    "### 多层Encoder\n",
    "\n",
    "`TransformerEncoder(encoder_layer, num_layers, norm=None)`\n",
    "+ 参数：\n",
    "  + `encoder_layer`，是一个`TransformerEncoderLayer`类的实例对象\n",
    "  + `num_layers`，指定堆叠的 Encoder 层数\n",
    "+ `forward()`方法和`TransformerEncoderLayer`的一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ea4521-9567-4fc2-9f22-ee61078bcaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape:  torch.Size([32, 10, 512])\n",
      "out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model, num_heads, dim_ffn = 512, 8, 2048\n",
    "batch_size, source_len = 10, 32\n",
    "src = torch.rand(source_len, batch_size, d_model)\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=dim_ffn)\n",
    "out = encoder_layer(src)\n",
    "\n",
    "print(\"src.shape: \", src.shape)\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47797b0b-94f5-4bb8-a684-2e8d4acd2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape:  torch.Size([32, 10, 512])\n",
      "out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "multi_encoder_layer = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=3)\n",
    "out = multi_encoder_layer(src)\n",
    "\n",
    "print(\"src.shape: \", src.shape)\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f571605a-9d2c-4df4-a024-ae41bff6d17e",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "\n",
    "### 单层Decoder\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer)\n",
    "\n",
    "`TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`   \n",
    "\n",
    "+ 实例化参数：和 Encoder 一致\n",
    "  + `d_model`，输入序列中，每个word的特征个数——它同时也决定了输出序列里每个word的特征个数\n",
    "  + `nhead`，multiheadattention中的head个数\n",
    "  + `dim_feedforward`，这个维度设置的是前馈神经网络的节点个数，前馈神经网络的输出节点数还是 `d_model`\n",
    "  + `dropout`，dropout比例，默认 0.1\n",
    "\n",
    "\n",
    "+ 前向传播`forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)`  \n",
    "  这个方法和 Encoder 不一样，它多了一个 memory. \n",
    "  + `tgt`，输入的序列，`shape`= $(S, N, E)$，\n",
    "  + `memory`，来自 encoder 层（通常是最后一层）的输出, `shape`=$(S, N, E)$.\n",
    "  + `tgt_mask`，\n",
    "  + `memory_mask`\n",
    "  + `tgt_key_padding_mask`，\n",
    "  + `memory_key_padding_mask`\n",
    "\n",
    "> `Decoder`内部会调用两层`MultiheadAttention`\n",
    "> 1. 第一层调用时为`MultiheadAttention.forward(tgt, tgt, tgt)`\n",
    "> 2. 第二层调用时为`MultiheadAttention.forward(tgt, memory, memory)`\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值  \n",
    "它返回的是和 `tgt` shape 相同的 tensor，\n",
    "\n",
    "\n",
    "### 多层Decoder\n",
    "\n",
    "`TransformerDecoder(decoder_layer, num_layers, norm=None)`   \n",
    "参数同 `TransformerEncoder` 类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2de2580-cf68-4ec3-92b9-2ba4621f06d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape:  torch.Size([32, 10, 512])\n",
      "memory.shape:  torch.Size([32, 10, 512])\n",
      "decoder_out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model, nhead, dim_ffn = 512, 8, 2048\n",
    "batch_size, source_len = 10, 32\n",
    "tgt = torch.rand(source_len, batch_size, d_model)\n",
    "memory = torch.rand(source_len, batch_size, d_model)\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ffn)\n",
    "decoder_out = decoder_layer(tgt, memory)\n",
    "\n",
    "print(\"tgt.shape: \", tgt.shape)\n",
    "print(\"memory.shape: \", memory.shape)\n",
    "print(\"decoder_out.shape: \", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5bb3fed-26f4-4058-a047-fa68c00dabf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape:  torch.Size([32, 10, 512])\n",
      "memory.shape:  torch.Size([32, 10, 512])\n",
      "decoder_out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "multi_decoder_layer = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=3)\n",
    "decoder_out = multi_decoder_layer(tgt, memory)\n",
    "\n",
    "print(\"tgt.shape: \", tgt.shape)\n",
    "print(\"memory.shape: \", memory.shape)\n",
    "print(\"decoder_out.shape: \", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bac1587-6cfa-467a-acca-22dcec5d3747",
   "metadata": {},
   "source": [
    "## Transformer类\n",
    "\n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer)\n",
    "\n",
    "`nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=<function relu>, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None)`\n",
    "\n",
    "+ 实例化参数\n",
    "  + `d_model`，输入/输出的embedding dim，默认512\n",
    "  + `nhead`\n",
    "  + `num_encoder_layers`\n",
    "  + `num_decoder_layers`\n",
    "  + `dim_feedforward`\n",
    "  + `dropout`\n",
    "  + `activation`\n",
    "  \n",
    "+ 正向传播`forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)`\n",
    "  + `src`，shape=$(S,N,E)$\n",
    "  + `tgt`，shape=$(T,N,E)$\n",
    "  + `src_mask`，shape=$(S,S)$\n",
    "  + `tgt_mask`，shape=$(T,T)$\n",
    "  + `memory_mask`，shape=$(T,S)$\n",
    "  + `src_key_padding_mask`，shape=$(N,S)$\n",
    "  + `tgt_key_padding_mask`，shape=$(N,T)$\n",
    "  + `memory_key_padding_mask`，shape=$(N,S)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep-Learning-Env",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
