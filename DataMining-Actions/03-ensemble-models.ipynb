{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.23.3\n",
      "pandas:      1.4.4\n",
      "matplotlib:  3.5.3\n",
      "sklearn:     1.1.2\n",
      "seaborn:     0.12.1\n",
      "plotly:      5.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "import plotly\n",
    "import sklearn\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"pandas:     \", pd.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"seaborn:    \", sns.__version__)\n",
    "print(\"plotly:     \", plotly.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 介绍\n",
    "\n",
    "整理 `sklearn.ensemble` 模块里提供的集成学习模型，该模块里主要分为如下几类集成模型：\n",
    "+ Bagging\n",
    "+ Boosting\n",
    "+ 结合策略\n",
    "+ 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "使用的数据集如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回归问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(housing_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names:  ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "target_names:  ['MedHouseVal']\n"
     ]
    }
   ],
   "source": [
    "print('feature_names: ', housing_data.feature_names)\n",
    "print('target_names: ', housing_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "print(housing_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data.frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block group\n",
      "        - HouseAge      median house age in block group\n",
      "        - AveRooms      average number of rooms per household\n",
      "        - AveBedrms     average number of bedrooms per household\n",
      "        - Population    block group population\n",
      "        - AveOccup      average number of household members\n",
      "        - Latitude      block group latitude\n",
      "        - Longitude     block group longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
      "\n",
      "The target variable is the median house value for California districts,\n",
      "expressed in hundreds of thousands of dollars ($100,000).\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "An household is a group of people residing within a home. Since the average\n",
      "number of rooms and bedrooms in this dataset are provided per household, these\n",
      "columns may take surpinsingly large values for block groups with few households\n",
      "and many empty houses, such as vacation resorts.\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(housing_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 划分训练&验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_X_train, housing_X_test, housing_y_train, housing_y_test = train_test_split(housing_data.data, housing_data.target, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(cancer_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "target_names:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "print('feature_names: ', cancer_data.feature_names)\n",
    "print('target_names: ',cancer_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "print(cancer_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data.frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 划分训练&验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_X_train, cancer_X_test, cancer_y_train, cancer_y_test = train_test_split(cancer_data.data, cancer_data.target, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# Bagging\n",
    "\n",
    "提供了并行集成相关的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础bagging模型\n",
    "\n",
    "```python\n",
    "# 回归问题\n",
    "BaggingRegressor(\n",
    "    estimator=None, n_estimators=10, *, \n",
    "    max_samples=1.0, max_features=1.0, \n",
    "    bootstrap=True, bootstrap_features=False, oob_score=False, \n",
    "    warm_start=False, n_jobs=None, random_state=None, verbose=0\n",
    ")\n",
    "\n",
    "# 分类问题\n",
    "BaggingClassifier(\n",
    "    estimator=None, n_estimators=10, *, \n",
    "    max_samples=1.0, max_features=1.0, \n",
    "    bootstrap=True, bootstrap_features=False, oob_score=False, \n",
    "    warm_start=False, n_jobs=None, random_state=None, verbose=0\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "sklearn提供的API如下：\n",
    "```python\n",
    "# 回归问题\n",
    "RandomForestRegressor(\n",
    "    n_estimators=100, *, \n",
    "    criterion='squared_error', \n",
    "    max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "    max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "    bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n",
    "    verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, monotonic_cst=None\n",
    ")\n",
    "\n",
    "# 分类问题\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100, *, \n",
    "    criterion='gini', \n",
    "    max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "    bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n",
    "    verbose=0, warm_start=False, \n",
    "    class_weight=None,   # 独有参数\n",
    "    ccp_alpha=0.0, max_samples=None, monotonic_cst=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，sklearn还提供了一个基于随机森林的无监督密度估计方法。\n",
    "```python\n",
    "RandomTreesEmbedding(\n",
    "    n_estimators=100, *, \n",
    "    max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "    max_leaf_nodes=None, min_impurity_decrease=0.0, sparse_output=True, \n",
    "    n_jobs=None, random_state=None, verbose=0, warm_start=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "# Boosting\n",
    "\n",
    "提供了串行集成相关的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## GBDT - KEY\n",
    "\n",
    "sklearn提供了两个版本的GBDT实现，普通版本和快速版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 普通版本\n",
    "```python\n",
    "# 回归树 API\n",
    "GradientBoostingRegressor(*, \n",
    "    loss='squared_error',\n",
    "    learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "    init=None, random_state=None, max_features=None, \n",
    "    alpha=0.9,   # 回归树独有\n",
    "    verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1,\n",
    "    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n",
    ")\n",
    "\n",
    "# 分类树 API\n",
    "GradientBoostingClassifier(*,\n",
    "    loss='log_loss',\n",
    "    learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "    init=None, random_state=None, max_features=None, \n",
    "    verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0\n",
    ")\n",
    "```\n",
    "\n",
    "对比可以看出，两者的参数里，除了 `loss` 不一样，几乎都一致，除了回归树多了一个`alpha`参数，因此下面就一起介绍了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数\n",
    "\n",
    "GBDT普通版本的参数可以分为如下 4 类：\n",
    "\n",
    "+ 训练过程\n",
    "\n",
    "| 参数                                   | 含义                     | 影响力 | 说明                                    |\n",
    "| -------------------------------------- | ------------------------ | ------ | --------------------------------------- |\n",
    "| loss                                   | 损失函数                 | ⭐⭐⭐    | 可选值见下面说明                        |\n",
    "| <font color=\"red\">n_estimators</font>  | 基函数个数               | ⭐⭐⭐⭐⭐  | int，默认100                            |\n",
    "| <font color=\"red\">learning_rate</font> | 学习率                   | ⭐⭐⭐⭐⭐  | float，默认0.1                          |\n",
    "| <font color=\"red\">max_features</font>  | 基函数训练时使用的特征数 | ⭐⭐⭐⭐⭐  | 默认None表示使用全部。有多种方式配置。  |\n",
    "| subsample                              | 基函数训练时的采样比例   | ⭐⭐⭐    | float，必须在 $(0.0,1.0])$之间，默认1.0 |\n",
    "| init                                   | 初始值设置               | ⭐⭐      | 默认None                                |\n",
    "| random_state                           | 随机数状态               | ⭐      | 默认None                                |\n",
    "| alpha                                  |                          | ⭐      |                                         |\n",
    "\n",
    "\n",
    "+ 基函数参数\n",
    "\n",
    "| 参数                     | 含义                           | 影响力 | 说明                                            |\n",
    "| ------------------------ | ------------------------------ | ------ | ----------------------------------------------- |\n",
    "| criterion                | CART分裂时的准则               | ⭐⭐     | 可选值为`friedman_mse`（默认），`squared_error` |\n",
    "| min_samples_split        | 节点分裂时需要包含的最小样本数 | ⭐⭐     | int表示个数（默认2），float表示比例             |\n",
    "| min_samples_leaf         | 叶子节点的最小样本数           | ⭐⭐     | int表示个数，float表示比例                      |\n",
    "| max_depth                | 树的最大深度                   | ⭐⭐     | int，默认3                                      |\n",
    "| min_impurity_decrease    | 节点分裂时的最小不纯度增益     | ⭐⭐     | float，默认0.                                   |\n",
    "| max_leaf_nodes           | 叶子结点的最大个数             | ⭐⭐     | int，默认None，表示无限制                       |\n",
    "| min_weight_fraction_leaf |                                |        |                                                 |\n",
    "\n",
    "\n",
    "+ 提前停止\n",
    "\n",
    "| 参数                | 含义                                                   | 说明                              |\n",
    "| ------------------- | ------------------------------------------------------ | --------------------------------- |\n",
    "| validation_fraction | 用于计算提前停止的验证集比例                           | float，默认0.1                    |\n",
    "| n_iter_no_change    | 提前停止的轮数，表示多少轮验证集无变化之后，就提前停止 | int，默认None，表示不启用提前停止 |\n",
    "| tol                 | 提前停止判断的loss变化值                               | float，默认1e-4                   |\n",
    "\n",
    "+ 其他\n",
    "\n",
    "| 参数       | 含义 | 说明 |\n",
    "| ---------- | ---- | ---- |\n",
    "| verbose    |      |      |\n",
    "| ccp_alpha  |      |      |\n",
    "| warm_start |      |      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 属性和方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 快速版本\n",
    "```python\n",
    "# 回归树 API\n",
    "HistGradientBoostingRegressor(\n",
    "    loss='squared_error', \n",
    "    *, quantile=None, \n",
    "    learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, \n",
    "    min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255,\n",
    "    categorical_features='warn', monotonic_cst=None, interaction_cst=None, warm_start=False, \n",
    "    early_stopping='auto', scoring='loss', validation_fraction=0.1, \n",
    "    n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None\n",
    ")\n",
    "\n",
    "# 分类树 API\n",
    "HistGradientBoostingClassifier(\n",
    "    loss='log_loss', \n",
    "    *, \n",
    "    learning_rate=0.1, max_iter=100, max_leaf_nodes=31, max_depth=None, \n",
    "    min_samples_leaf=20, l2_regularization=0.0, max_features=1.0, max_bins=255, \n",
    "    categorical_features='warn', monotonic_cst=None, interaction_cst=None, warm_start=False, \n",
    "    early_stopping='auto', scoring='loss', validation_fraction=0.1, \n",
    "    n_iter_no_change=10, tol=1e-07, verbose=0, random_state=None, \n",
    "    class_weight=None  # 分类树独有\n",
    ")\n",
    "```\n",
    "\n",
    "对比可以看出，快速版本的回归和分类API参数差异也不大。\n",
    "\n",
    "但是快速版本相比普通版本，提供了一些其他的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数\n",
    "\n",
    "快速版本特有的参数如下：\n",
    "\n",
    "| 参数                 | 含义                                   | 影响力 | 说明         |\n",
    "| -------------------- | -------------------------------------- | ------ | ------------ |\n",
    "| max_iter             | 最大迭代次数，相当于`n_estimators`参数 |        | int，默认100 |\n",
    "| l2_regularization    | L2正则项                               |        |              |\n",
    "| max_bins             | 最大分箱数                             |        |              |\n",
    "| categorical_features | 处理分类特征                           |        |              |\n",
    "| monotonic_cst        |                                        |        |              |\n",
    "| interaction_cst      |                                        |        |              |\n",
    "| early_stopping       | 提前停止策略                           |        |              |\n",
    "| scoring              | 提前停止的评价指标                     |        |              |\n",
    "| quantile             | `loss='quantile'`时使用                |        |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 属性和方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "\n",
    "sklearn里GBDT支持的损失函数如下."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归问题\n",
    "`GradientBoostingRegressor`\n",
    "+ squared_error，默认值\n",
    "+ absolute_error\n",
    "+ quantile\n",
    "+ huber\n",
    "\n",
    "`HistGradientBoostingRegressor`\n",
    "+ squared_error\n",
    "+ absolute_error\n",
    "+ quantile\n",
    "+ gamma\n",
    "+ poisson\n",
    "\n",
    "各损失函数详情如下：\n",
    "\n",
    "+ **squared_error——平方误差**\n",
    "\n",
    "$$\n",
    "L = \\sum{(y_i - H(x_i))^2}\n",
    "$$\n",
    "\n",
    "+ **absolute_error——绝对误差**\n",
    "\n",
    "$$\n",
    "L = \\sum{|y_i - H(x_i)|}\n",
    "$$\n",
    "\n",
    "\n",
    "+ **quantile**\n",
    "\n",
    "$$\n",
    "L = \\sum{l(y_i,H(x_i))}\n",
    "$$\n",
    "\n",
    "其中\n",
    "$$\n",
    "l = \\begin{split} \n",
    "\\begin{cases}\n",
    "    \\alpha (y_i - H(x_i)), & y_i - H(x_i) > 0 \\\\\n",
    "    0,    & y_i - H(x_i) = 0 \\\\\n",
    "    (1-\\alpha) (y_i - H(x_i)), & y_i - H(x_i) < 0\n",
    "\\end{cases}\\end{split}, \\space \\space \\alpha \\in (0, 1)\n",
    "$$\n",
    "\n",
    "\n",
    "+ **huber——合页误差**\n",
    "\n",
    "$$\n",
    "L = \\sum{l(y_i,H(x_i))}\n",
    "$$\n",
    "\n",
    "其中\n",
    "$$\n",
    "l = \\begin{split} \n",
    "\\begin{cases}\\frac{1}{2}(y_i - H(x_i))^2, & |y_i - H(x_i)|\\leq\\alpha \\\\\n",
    "\\alpha(|y_i - H(x_i)|-\\frac{\\alpha}{2}),& |y_i - H(x_i)|>\\alpha \\end{cases}\\end{split}, \\space \\space \\alpha \\in (0, 1)\n",
    "$$\n",
    "\n",
    "其中$\\alpha$是需要我们自己设置的超参数，由参数`alpha`控制。在huber损失中，alpha是阈值，在quantile损失中，alpha用于辅助计算损失函数的输出结果，默认为0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类问题\n",
    "`GradientBoostingClassifier`\n",
    "+ log_loss\n",
    "+ exponential\n",
    "\n",
    "`HistGradientBoostingClassifier`\n",
    "+ log_loss, 只支持这一种\n",
    "\n",
    "\n",
    "对任意样本$i$而言，$y_i$为真实标签，$\\hat{y_i}$为预测标签，$H(x_i)$为集成算法输出结果，$p(x_i)$为基于$H(x_i)$和sigmoid/softmax函数计算的概率值。\n",
    "那么各损失函数详情如下：\n",
    "\n",
    "+ **log_loss——对数几率**，也叫交叉熵损失\n",
    "\n",
    "**二分类交叉熵损失**\n",
    "$$\n",
    "L = -\\left( y\\log p(x) + (1 - y)\\log(1 - p(x)) \\right)\n",
    "$$\n",
    "注意，log当中输入的一定是概率值。对于逻辑回归来说，概率就是算法的输出，因此我们可以认为逻辑回归中$p = H(x)$，但对于GBDT来说，$p(x_i) = Sigmoid(H(x_i))$，这一点一定要注意。\n",
    "\n",
    "\n",
    "**多分类交叉熵损失**，总共有K个类别\n",
    "\n",
    "$$\n",
    "L = -\\sum_{k=1}^Ky^*_k\\log(P^k(x))\n",
    "$$\n",
    "\n",
    "其中，$P^k(x)$是概率值，对于多分类GBDT来说，$p^k(x) = Softmax(H^k(x))$，$y^*$是由真实标签转化后的向量。\n",
    "\n",
    "+ **exponential——指数损失**\n",
    "\n",
    "**二分类指数损失**\n",
    "$$\n",
    "L = e^{-yH(x)}\n",
    "$$\n",
    "\n",
    "**多分类指数损失**，总共有K个类别\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L &=exp \\left( -\\frac{1}{K}\\boldsymbol{y^* · H^*(x)} \\right) = exp \\left( -\\frac{1}{K}(y^1H^1(x)+y^2H^2(x) \\ + \\  ... + y^kH^k(x)) \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "需要注意，指数损失中的$y^*$与交叉熵损失中的$y^*$不是同样的向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提前停止"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 袋外估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## AdaBoost\n",
    "\n",
    "sklearn提供的API如下：\n",
    "```python\n",
    "# 回归问题\n",
    "AdaBoostRegressor(estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
    "\n",
    "# 分类问题\n",
    "AdaBoostClassifier(estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 结合策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stacking - KEY\n",
    "\n",
    "有关Stacking的介绍，可以参考如下的几篇博客：\n",
    "1. [模型融合方法概述](https://zhuanlan.zhihu.com/p/25836678)\n",
    "2. [Kaggle机器学习之模型融合（stacking）心得](https://zhuanlan.zhihu.com/p/26890738)\n",
    "3. [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook#Second-Level-Predictions-from-the-First-level-Output)\n",
    "\n",
    "实现类详见`Stacking.py`文件。\n",
    "\n",
    "sklearn提供的API如下：\n",
    "```python\n",
    "# 回归问题\n",
    "StackingRegressor(\n",
    "    estimators, \n",
    "    final_estimator=None, *, \n",
    "    cv=None, n_jobs=None, passthrough=False, verbose=0\n",
    ")\n",
    "\n",
    "# 分类问题\n",
    "StackingClassifier(\n",
    "    estimators, \n",
    "    final_estimator=None, *, \n",
    "    cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Voting\n",
    "\n",
    "sklearn提供的API如下：\n",
    "\n",
    "```python\n",
    "# 回归问题\n",
    "VotingRegressor(\n",
    "    estimators, *, \n",
    "    weights=None, n_jobs=None, verbose=False\n",
    ")\n",
    "\n",
    "# 分类问题\n",
    "VotingClassifier(\n",
    "    estimators, *, \n",
    "    voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IsolationForest\n",
    "孤立森林异常检测算法。\n",
    "\n",
    "```python\n",
    "IsolationForest(\n",
    "    *, n_estimators=100, \n",
    "    max_samples='auto', contamination='auto', \n",
    "    max_features=1.0, bootstrap=False, \n",
    "    n_jobs=None, random_state=None, verbose=0, warm_start=False\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data-Analysis",
   "language": "python",
   "name": "data-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
