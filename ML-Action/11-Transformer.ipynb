{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convertible-coordinator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.19.2\n",
      "pandas:      1.0.1\n",
      "matplotlib:  3.3.1\n",
      "sklearn:     0.23.2\n",
      "seaborn:     0.11.1\n",
      "plotly:      4.14.1\n",
      "PyTorch:      1.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "import plotly\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from IPython.display import display\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"pandas:     \", pd.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"seaborn:    \", sns.__version__)\n",
    "print(\"plotly:     \", plotly.__version__)\n",
    "print(\"PyTorch:     \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-reach",
   "metadata": {},
   "source": [
    "# Pytorch-Transformer\n",
    "\n",
    "这里介绍了Pytorch里 transformer 层的实现模块."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "variable-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-chess",
   "metadata": {},
   "source": [
    "## MultiheadAttention Layer\n",
    "\n",
    "需要注意的是，pytorch源码中 `MultiheadAttention` 被放在了 Non-linear Activations 这一类中（并且是放在`activation.py`文件中的）。\n",
    "\n",
    "`MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)`\n",
    "+ 参数\n",
    "  + `embed_dim`，这个是所有head的总维度，分配到每个head的维度还需要除以下面的 num_heads.\n",
    "  + `num_heads`，\n",
    "  + `bias`，\n",
    "  + `add_bias_kv`\n",
    "  + `add_zero_attn`\n",
    "  + `kdim`，自定义 key 中的特征维度\n",
    "  + `vidm`，自定义 value 中的特征维度\n",
    "\n",
    "\n",
    "+ `forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)`\n",
    "  + `query`, `key`, `value`，self-attention的三个输入向量——这三个输入向量会被用于生成对应的Q,K,V\n",
    "    + `query.shape` = $(L, N, E)$，\n",
    "    + `key.shape` = $(S, N, E)$\n",
    "    + `value.shape` = $(S, N, E)$\n",
    "  \n",
    "  其中 $N$ 是batch size，$E$ 是 embedding 维度——**它必须等于参数中的`embed_dim`**，这个embedding 维度，既是输入的，也是输入的——**self-attention层输入和输入的特征维度必须一样**  ；  \n",
    "  $S$ 是**输入**序列的长度，$L$ 是**输出**序列的长度——**输入和输出序列的长度可以不一样**.\n",
    "  \n",
    "  + `key_padding_mask`，\n",
    "  + `need_weights`，bool，表示是否需要输出attention的weights\n",
    "  + `attn_mask`，2维或者3维的mask\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值\n",
    "  + `attn_output`，shape= $(L, N, E)$，$L$ 由 输入的`query`序列长度决定，$E$ 由输入序列中每个word的特征维度确定\n",
    "  + `attn_output_weights`，shape=$(N, L, S)$，这个权重应该是每个输入序列中，每个word的attention权重."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colored-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个 embed_dim 指定的既是输入序列中，每个word的特征维度，也指定了输入序列中，每个word的特征维度\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loose-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "E, S, L, N = embed_dim, 20, 10, 5\n",
    "query = torch.rand(L, N, E)\n",
    "key = torch.rand(S, N, E)\n",
    "value = torch.rand(S, N, E)\n",
    "\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "practical-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 512])\n",
      "torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-spotlight",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "\n",
    "单层Encoder：`nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`\n",
    "+ 参数\n",
    "  + `d_model`，输入序列中，每个word的特征个数——它同时也决定了输出序列里每个word的特征个数\n",
    "  + `nhead`，multiheadattention中的head个数\n",
    "  + `dim_feedforward`，这个维度设置的是前馈神经网络的节点个数，前馈神经网络的输出节点数还是 `d_model`\n",
    "  \n",
    "  \n",
    "+ `forward(src, src_mask=None, src_key_padding_mask=None)`方法\n",
    "  + `src`，输入的sequence, `shape` = $(S, N, E)$\n",
    "  + `src_mask`，输入sequence的mask\n",
    "  + `src_key_padding_mask`，每个batch中src的padding\n",
    "  \n",
    "  \n",
    "+ `forward()`方法返回值  \n",
    "  它返回的是和 `src` shape 相同的 tensor.\n",
    "  \n",
    "+ Encoder的内部只有**一层**`MultiheadAttention`：`Encoder.forward()`实际执行时，调用方式为`MultiheadAttention.forward(src, src, src)`——`src`会同时作为 query, key, value 传入 self-attention。\n",
    "\n",
    "多层Encoder：`TransformerEncoder(encoder_layer, num_layers, norm=None)`\n",
    "+ 参数：\n",
    "  + `encoder_layer`，是一个`TransformerEncoderLayer`类的示例\n",
    "  + `num_layers`，指定堆叠的 Encoder 层数\n",
    "+ `forward()`方法就和`TransformerEncoderLayer`的一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coupled-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape:  torch.Size([32, 10, 512])\n",
      "out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_ffn = 2048\n",
    "batch_size = 10\n",
    "source_seq_len = 32\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=dim_ffn)\n",
    "\n",
    "src = torch.rand(source_seq_len, batch_size , d_model)\n",
    "out = encoder_layer(src)\n",
    "\n",
    "print(\"src.shape: \", src.shape)\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-distribution",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "\n",
    "单层Decoder：`TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`\n",
    "\n",
    "+ 参数：\n",
    "  和 Encoder 一致\n",
    "\n",
    "+ `forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)`  \n",
    "  这个方法和 Encoder 不一致，它多了一个 memory.\n",
    "  + `tgt`，输入的序列，`shape`= $(S, N, E)$，\n",
    "  + `memory`，来自 encoder 层（通常是最后一层）的输出, `shape`=$(S, N, E)$.\n",
    "  + `tgt_mask`\n",
    "  + `memory_mask`\n",
    "  + `tgt_key_padding_mask`\n",
    "  + `memory_key_padding_mask`\n",
    "  \n",
    "\n",
    "+ `forward()`方法返回值  \n",
    "  它返回的是和 `tgt` shape 相同的 tensor.\n",
    "\n",
    "+ `Decoder`内部有两层`MultiheadAttention`：\n",
    "  + 第一层调用时为`MultiheadAttention.forward(tgt, tgt, tgt)`\n",
    "  + 第二层调用时为`MultiheadAttention.forward(tgt, memory, memory)`。\n",
    "  \n",
    "多层Decoder：`TransformerDecoder(decoder_layer, num_layers, norm=None)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "responsible-petite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape:  torch.Size([32, 10, 512])\n",
      "memory.shape:  torch.Size([32, 10, 512])\n",
      "decoder_out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_ffn = 2048\n",
    "batch_size = 10\n",
    "source_seq_len = 32\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ffn)\n",
    "\n",
    "tgt = torch.rand(source_seq_len, batch_size, d_model)\n",
    "memory = torch.rand(source_seq_len, batch_size, d_model)\n",
    "\n",
    "decoder_out = decoder_layer(tgt, memory)\n",
    "\n",
    "print(\"tgt.shape: \", tgt.shape)\n",
    "print(\"memory.shape: \", memory.shape)\n",
    "print(\"decoder_out.shape: \", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-sarah",
   "metadata": {},
   "source": [
    "# Huggingface-Transformer-PreTraindedModels\n",
    "\n",
    "这里介绍huggingface编写的transformer包，它里面基于transformer结构，实现了各种模型，包括BERT, DistilBERT, GPT2 等等.\n",
    "\n",
    "transformer 包里最重要的三个子模块是：\n",
    "1. [Configuration](https://huggingface.co/transformers/main_classes/configuration.html)  \n",
    "基础的类是 `PretrainedConfig`（好像也只有这一个类），用于 **导出/导入** 模型的配置。\n",
    "2. [Models](https://huggingface.co/transformers/main_classes/model.html)  \n",
    "基础的类有如下三个，用于构建模型：\n",
    "  + `PreTrainedModel`，pytorch的模型都继承了这个类，继承了 `torch.nn.Module` 类.\n",
    "  + `TFPreTrainedModel`，Tensorflow2.0里的模型都继承了这个类，继承了 `tf.keras.Model` 类.\n",
    "  + `FlaxPreTrainedModel`\n",
    "3. [Tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html)  \n",
    "包含两个基础的类，用于处理文本特征：\n",
    "  + `PreTrainedTokenizer`，这个比较慢\n",
    "  + `PreTrainedTokenizerFast`，这个比较快\n",
    "  \n",
    "上面这三个基础模块，都有如下两个方法用于**导入/导出**对应的配置：\n",
    "+ `.from_pretrained()`，导入预训练模型的 config/tokenizer/model\n",
    "  + `pretrained_model_name_or_path`，指定预训练模型的名称或者本地路径\n",
    "  + `cache_dir`，指定模型的缓存位置\n",
    "  + `force_download`，bool，指定是否强制下载模型\n",
    "  + `local_files_only`，bool，指定是否只使用本地模型\n",
    "  + `mirror`，指定下载的镜像地址\n",
    "+ `.save_pretrained()`，导出训练好模型的 config/tokenizer/model\n",
    "\n",
    "**第一次使用这些语句的时候，如果本地没有对应的模型，会下载这些模型**。\n",
    "  \n",
    "在上面的3个基础子模块之上，还提供了如下几个子模块工具：\n",
    "\n",
    "4. [Pipepline](https://huggingface.co/transformers/main_classes/pipelines.html)  \n",
    "用于快速使用模型，是对上面三个类的封装\n",
    "5. [Trainer](https://huggingface.co/transformers/main_classes/trainer.html)  \n",
    "用于快速训练或者 fine-tune 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-graduate",
   "metadata": {},
   "source": [
    "transformer源码中，所有的模型结构都存放在 `src/transformers/models`文件夹下，每个模型对应于一个文件夹（比如BERT对应的就是`bert`文件夹），每个模型的文件夹内，主要有如下几个`.py`文件，其中的 `xxx` 是对应模型的名称：\n",
    "+ **`configuration_xxx.py`**，编写了该模型对应的`PretrainedConfig`子类，比如BERT就是`BertConfig`。\n",
    "+ **`modeling_xxx.py`**，存放了pytorch编写的模型结构，要用到的模型类都放在这里面\n",
    "+ `modeling_tf_xxx.py`，存放tensorflow编写的模型结构\n",
    "+ `modeling_flax_xxx.py`\n",
    "+ **`tokenization_xxx.py`**，用于 分词的 实现类，比如BERT就是`BertTokenizer`类\n",
    "+ `tokenization_xxx_fast.py`，快速分词的实现类\n",
    "+ `convert_*.py`，用于将其它配置文件转换成对应的模型。\n",
    "\n",
    "\n",
    "可用的模型列表见官网 [Pretrained models](https://huggingface.co/transformers/pretrained_models.html).\n",
    "\n",
    "我一般常用的两个模型是 **BERT** 和 **Distil-BERT**，所以下面会着重关注这两个模型里的实现。\n",
    "\n",
    "+ [BERT Models](https://huggingface.co/transformers/model_doc/bert.html) 包含如下类（以pytorch为例）\n",
    "  + `BertConfig`，配置类.  \n",
    "  位于`configuration_bert.py`文件中，该文件中也只有这一个配置类\n",
    "  + `BertTokenizer`，实现 `WordPiece` 分词的类.  \n",
    "  位于`tokenization_bert.py`文件中，该文件中还有 `BasicTokenizer` 和 `WordpieceTokenizer` 两个类，不过`BertTokenizer`会调用它们.\n",
    "  + `BertModel`，最基本的 BERT 模型类，它返回的是 BERT 模型的原始结果，包括隐藏层状态.\n",
    "  + `BertForPreTraining`，在 `BertModel`的输出上加了一层处理，以下的几个模型都是如此.\n",
    "  + `BertForMaskedLM`\n",
    "  + `BertForNextSentencePrediction`\n",
    "  + `BertForSequenceClassification`\n",
    "  + 还有其他的一些基础工具类和其他任务对于的BERT模型类，这里就不介绍了。\n",
    "  \n",
    "\n",
    "+ [DistilBERT Models](https://huggingface.co/transformers/model_doc/distilbert.html) 包含如下类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-player",
   "metadata": {},
   "source": [
    "## BERT模型\n",
    "\n",
    "+ 可用的BERT模型有：\n",
    "  + bert-base-uncased\n",
    "  + bert-base-cased\n",
    "  + bert-large-uncased\n",
    "  + bert-large-cased\n",
    "  + bert-base-chinese\n",
    "\n",
    "\n",
    "+ 可用的Distil-Bert模型有：\n",
    "  + distilbert-base-uncased\n",
    "  + distilbert-base-cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "skilled-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nominated-elephant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "compound-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "copyrighted-exchange",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./BERT/bert-pre-trained-models/distilbert-base-uncased/ were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./BERT/bert-pre-trained-models/distilbert-base-uncased/ and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.__class__:     <class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "tokenizer.__class__:  <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "model.__class__:      <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"./BERT/bert-pre-trained-models/bert-base-uncased\"\n",
    "model_name = \"./BERT/bert-pre-trained-models/distilbert-base-uncased/\"\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, local_files_only=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, local_files_only=True)\n",
    "model = BertModel.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "print(\"config.__class__:    \", config.__class__)\n",
    "print(\"tokenizer.__class__: \", tokenizer.__class__)\n",
    "print(\"model.__class__:     \", model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-adolescent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-1.15(Python3.6.12)",
   "language": "python",
   "name": "tensorflow1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
