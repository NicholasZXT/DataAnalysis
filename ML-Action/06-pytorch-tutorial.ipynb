{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.19.2\n",
      "pandas:      1.2.2\n",
      "matplotlib:  3.3.4\n",
      "sklearn:     0.23.2\n",
      "seaborn:     0.11.1\n",
      "plotly:      4.14.3\n",
      "PyTorch:      1.2.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "import plotly\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from IPython.display import display\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"pandas:     \", pd.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"seaborn:    \", sns.__version__)\n",
    "print(\"plotly:     \", plotly.__version__)\n",
    "print(\"PyTorch:     \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor基础\n",
    "\n",
    "PyTorch的设计遵循着 **Tensor(高维数组) ——> Varible(自动求导AutoGrad) ——> nn.Module(神经网络层/模块)** 三个由低到高的抽象层次."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Tensor\n",
    "\n",
    "有两种方式可以创建Tensor：\n",
    "1. 使用`torch.Tensor()`构造方法，注意，这是`torch.Tensor`类的构造器方法\n",
    "2. 使用`torch.xxx()`函数来创建Tensor，比如：\n",
    "  + `torch.tensor(data, dtype=None, device=None, requires_grad=False,)`——这是静态函数，用于从已有数据中创建tensor，注意这里的`requires_grad`参数。\n",
    "  + `torch.ones()`\n",
    "  + `torch.zeros()`\n",
    "  \n",
    "上述两种方法都会返回`torch.Tensor`的对象。\n",
    "\n",
    "有关`torch.Tensor()`方法和`torch.tensor()`方法的讨论见问答 [What is the difference between torch.tensor and torch.Tensor?](https://stackoverflow.com/questions/51911749/what-is-the-difference-between-torch-tensor-and-torch-tensor).\n",
    "\n",
    "+ 基本创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.9339e+32, 7.5551e+31, 1.5089e-19],\n",
       "        [4.4656e+30, 3.3917e-15, 0.0000e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用构造方法，指定tensor的shape\n",
    "t1 = torch.Tensor(2,3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用构造方法，从 list of list 中创建\n",
    "t2 = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用静态方法\n",
    "t3 = torch.tensor([[1,2]])\n",
    "t3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tensor的一些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 常用的创建方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全 1 Tensor\n",
    "torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全 0 \n",
    "torch.zeros(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 步长\n",
    "torch.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4395, 0.3174, 0.5914],\n",
       "        [0.2081, 0.3722, 0.5950],\n",
       "        [0.5110, 0.2151, 0.0040],\n",
       "        [0.8960, 0.6777, 0.7021],\n",
       "        [0.1643, 0.4797, 0.0276]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 均匀分布随机数\n",
    "torch.rand(5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8362, -0.4455, -0.4478,  1.5865],\n",
       "        [ 1.1634,  1.4463,  0.3926,  0.1785],\n",
       "        [-0.8441, -1.5769, -0.8146,  0.8134]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正态随机数\n",
    "torch.randn(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对角线为1\n",
    "torch.eye(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor类型\n",
    "\n",
    "Tensor有各种类型，并且每种类型有CPU和GPU版本。\n",
    "\n",
    "类型之间的转换有三种方式，设已有tensor `a` 和 `b`\n",
    "1. `a.type(torch.FloatTensor)`，\n",
    "2. `a.float()`，便捷的方式\n",
    "3. `a.as_type(b)`，使用`b`的类型来设置`a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2])\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type(torch.FloatTensor)\n",
    "c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.float()\n",
    "c.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的输入中，特别是RNN模型，常常需要对输入的数据的不同维度进行交换，主要是Batch size 这个维度。  \n",
    "有两种方式可以处理：\n",
    "1. `torch.transpose()`\n",
    "2. `numpy.transpose()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "`torch.autograd` 模块是一套专门用于自动求导的引擎，能够根据输入和前向传播过程自动**动态地**构建计算图，并执行反向传播。\n",
    "\n",
    "Autograd 模块中的核心数据结构是`Variable`，它是对Tensor的封装，同时记录了对Tensor的操作，用于构建计算图。  \n",
    "`Variable`包含三个属性：\n",
    "+ `data`：保存`Variable`包含的Tensor\n",
    "+ `grad`：保存`data`对应的梯度，它本身也是一个`Variable`，与`data`形状一致\n",
    "+ `grad_fn`：指向一个Function，记录`Variable`的操作历史，用于构建计算图。对于叶子节点，它指向的None。\n",
    "\n",
    "注意:\n",
    "> 从 **0.4 版本**开始，`torch.Tensor`和`torch.autograd.Variable`被合并到了一起。  \n",
    "更确切地说，`torch.Tensor`能够像旧版`Variable`一样追踪历史，`Variable`封装还像过去那样工作，但返回一个`torch.Tensor`类型的对象。  \n",
    "这意味着不再需要在代码中到处使用`Variable`封装器。\n",
    "\n",
    "创建Tensor的时候，有两种方式让这个Tensor作为变量进行求导操作：\n",
    "1. 创建时指定参数`required_grad=True`\n",
    "2. 创建后通过`torch.Tensor`对象的`.required_grad`属性进行设置\n",
    "\n",
    "注意，**只有浮点数类型的Tensor才能设置求导**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [2., 2., 2.]], requires_grad=True)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面可以看出，Variable已经和Tensor合并了\n",
    "from torch.autograd import Variable\n",
    "a = Variable(torch.ones(2,3)*2, requires_grad=True)\n",
    "print(a.__class__)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## 多元函数的反向求导\n",
    "\n",
    "多元函数是 $R^n \\Rightarrow R$ 的一个映射：自变量 $x \\in R^n$，因变量 $y=f(x) \\in R$. 这种情况下，偏导数 $\\frac{\\partial f}{\\partial x} \\in R^n$ 也是一个 $n$ 维向量.  \n",
    "\n",
    "这里构造一个 $c = 3 \\times a^2 + b^2$ 的例子，这个计算中，可以进一步拆成如下的简单四则运算的节点：\n",
    "1. 节点 $a$ 和 $b$ 都是输入节点——也被称为计算图的**叶子节点**\n",
    "2. 中间节点 $t_1 = a^2$，$t_2 = b^2$ 和 $t_3 = 3 \\times t_1$ —— 对应于计算图的**非叶子节点**\n",
    "3. 最终的节点 $c = t_3 + t_2$。\n",
    "\n",
    "下面的代码中，为了简便起见，将 $t_1$ 和 $t_3$ 合并为一个 $t_1 = 3 \\times a^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad:  True\n",
      "a.grad:  None\n",
      "a.grad_fn:  None\n",
      "b.requires_grad:  True\n",
      "b.grad:  None\n",
      "b.grad_fn:  None\n"
     ]
    }
   ],
   "source": [
    "# 注意，这里只能使用 tensor 静态方法，不能使用 torch.Tensor() 构造方法\n",
    "# 这里必须是 浮点数 1.0，不能是 整数 1\n",
    "a = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# 第二种方式指定求导\n",
    "b = torch.tensor([1.0])\n",
    "b.requires_grad=True\n",
    "\n",
    "print(\"a.requires_grad: \",a.requires_grad)\n",
    "print(\"a.grad: \", a.grad)\n",
    "print(\"a.grad_fn: \", a.grad_fn)  # 由于 `a` 是手动创建的，所以它的 `grad_fn` 为 None\n",
    "print(\"b.requires_grad: \",b.requires_grad)\n",
    "print(\"b.grad: \", b.grad)\n",
    "print(\"b.grad_fn: \", b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------检查各节点的梯度-------------------\n",
      "t1.requires_grad:  True\n",
      "t1.grad:  None\n",
      "t1.grad_fn:  <MulBackward0 object at 0x0000022724238BE0>\n",
      "t2.requires_grad:  True\n",
      "t2.grad:  None\n",
      "t2.grad_fn:  <PowBackward0 object at 0x000002272D296CC0>\n",
      "c.requires_grad:  True\n",
      "c.grad:  None\n",
      "c.grad_fn:  <AddBackward0 object at 0x000002272D2960B8>\n",
      "-----------检查是否为叶子节点----------------\n",
      "a.is_leaf: True\n",
      "b.is_leaf: True\n",
      "t1.is_leaf: False\n",
      "t2.is_leaf: False\n",
      "c.is_leaf: False\n"
     ]
    }
   ],
   "source": [
    "# 下面的 t1，t2 是显式表示出来的非叶子节点\n",
    "t1 = 3*a**2\n",
    "t2 = b**2\n",
    "c = t1 + t2\n",
    "\n",
    "print(\"-----------检查各节点的梯度-------------------\")\n",
    "print(\"t1.requires_grad: \",t1.requires_grad)\n",
    "print(\"t1.grad: \", t1.grad)\n",
    "print(\"t1.grad_fn: \",t1.grad_fn)  # t1 是经过计算的（乘法），所以有梯度函数\n",
    "\n",
    "print(\"t2.requires_grad: \",t2.requires_grad)\n",
    "print(\"t2.grad: \", t2.grad)\n",
    "print(\"t2.grad_fn: \",t2.grad_fn)  # t2 是经过计算的（求幂），所以有梯度函数\n",
    "\n",
    "print(\"c.requires_grad: \",c.requires_grad)\n",
    "print(\"c.grad: \", c.grad)\n",
    "print(\"c.grad_fn: \",c.grad_fn)  # c 是经过计算的（加法），所以有梯度函数\n",
    "\n",
    "# 检查节点是否为叶子节点\n",
    "print(\"-----------检查是否为叶子节点----------------\")\n",
    "print(\"a.is_leaf:\", a.is_leaf)\n",
    "print(\"b.is_leaf:\", b.is_leaf)\n",
    "print(\"t1.is_leaf:\", t1.is_leaf)\n",
    "print(\"t2.is_leaf:\", t2.is_leaf)\n",
    "print(\"c.is_leaf:\", c.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面所有tensor的梯度`.grad`都是None，因为还没有执行过反向求导计算。    \n",
    "其中 `a` 和 `b` 是**叶子节点**，叶子节点是计算图的起点，所以对应的 `.grad_fn` 总是 None；   \n",
    "`t1` 代表的 $3\\times a^2$ 和 `t2` 代表的 $b^2$ 是**非叶子节点，非叶子节点总是由叶子节点经过层层运算得到**，所以它们的 `.grad_fn` 是对应的上一层的转化操作。\n",
    "\n",
    "下面从 `c` 所在的节点 关于两个叶子节点 `a` 和 `b` 执行反向求导计算 。  \n",
    "$\\frac{\\partial c}{\\partial a} = 6a$,   $\\frac{\\partial c}{\\partial b} = 2b$.  \n",
    "由于tensor `a`=1， `b`=1，代入上述的梯度，就得到 `a.grad`=6, `b.grad`=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad:  tensor([6.], grad_fn=<CloneBackward>)\n",
      "b.grad:  tensor([2.], grad_fn=<CloneBackward>)\n",
      "t1.grad:  None\n",
      "t2.grad:  None\n",
      "c.grad:  None\n"
     ]
    }
   ],
   "source": [
    "# 在 c 节点执行反向求导计算\n",
    "# 如果想再次执行下面的 .backward()，在没有设置参数 retain_graph=True ，只能重新构建一次计算图\n",
    "# 重新构建计算图有两种方式：\n",
    "# 第一种：重新赋值，\n",
    "# c = t1 + t2\n",
    "# c.backward()\n",
    "# 另一种是 设置 create_grap=True\n",
    "c.backward(create_graph=True)\n",
    "\n",
    "# 设置参数 retain_graph=True，可以多次执行梯度计算，不过这种情况下，叶子节点的梯度就是逐次累加的\n",
    "# c.backward(retain_graph=True)\n",
    "\n",
    "print(\"a.grad: \", a.grad)\n",
    "print(\"b.grad: \", b.grad)\n",
    "print(\"t1.grad: \", t1.grad)\n",
    "print(\"t2.grad: \", t2.grad)\n",
    "print(\"c.grad: \", c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认下，执行`.backward()`方法会构建从叶子节点到 `c` 节点的计算图，然后反向每个计算节点的梯度。  \n",
    "默认情况下，为了减少内存占用，**只有叶子节点的梯度会被保留，非叶子节点的梯度在计算完叶子节点的梯度后，会被删除** 。  \n",
    "**如果想保留计算图中非叶子节点的梯度，需要设置`.backward()`中的参数`retain_graph=True`** 。  \n",
    "参数`retain_graph=True`时，多次执行`.backward()`函数，**叶子节点的梯度是会累加的**。\n",
    "\n",
    "上面的`c.backward()`只能执行一次，如果再次执行，就会报错，就是因为计算图中非叶子节点的梯度已经被删除了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量函数的反向求导\n",
    "\n",
    "另一种情况是向量函数的偏导数.  \n",
    "所谓**向量函数**，是 $ R^n \\Rightarrow R^m$ 的一个映射：自变量 $x \\in R^n $ 是一个 $n$ 维向量，同时因变量 $f(x) = (y_1, y_2,..., y_m)^T \\in R^m$ 也是一个 $m$ 维向量，其中的 $y_j = y_j(x) \\in R$ 是一个多元函数。 \n",
    "\n",
    "PyTorch的`autograd`模块有个简单粗暴的规定，**只能执行标量对张量的求导，不能执行张量对张量的求导** 。  \n",
    "为了对向量函数求偏导数，也就是实现张量对张量的求导，PyTorch的`.backward()`方法里，提供了一个 `grad_variables=` 参数，它是一个 $m$ 维的向量，维度和 $f(x)$ 一样，作用是和 $f(x)$ 做内积，得到一个实数，然后对这个实数变量求偏导数.  \n",
    "具体来说，就是设 `grad_variables`参数对应的向量是 $v \\in R^m$，那么就会得到一个内积值 $l = \\sum_{j=1}^{m}{v_j y_j(x)} \\in R$，那么就是：  \n",
    "$\\frac{\\partial l}{\\partial x} = \\frac{\\partial l}{\\partial y}\\frac{\\partial y}{\\partial x}$，其中 $\\frac{\\partial l}{\\partial y} = v \\in R^m$，而 $\\frac{\\partial y}{\\partial x} \\in R^{m\\times n}$ 是一个雅克比矩阵，两者相乘，最后得到的就是一个 $n$ 维的梯度向量.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的例子中, $x \\in R^2$, $y = (x+1)^2+3 \\in R^2$ 表示对应的分量执行相同的运算，这里相当于 $f(x) \\in R^2$，并且  \n",
    "$\n",
    "\\left\\{\n",
    "\\begin{array}\\\n",
    "y_1(x) = y_1(x_1) = (x_1+1)^2+3 \\\\\n",
    "y_2(x) = y_2(x_2) = (x_2+1)^2+3\n",
    "\\end{array}\n",
    "\\right.\n",
    "$,\n",
    "\n",
    "$\n",
    "\\frac{\\partial y}{\\partial x} =\n",
    "\\left\\{\n",
    "\\begin{matrix}\n",
    "   \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "   \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} \n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "=\n",
    "\\left\\{\n",
    "\\begin{matrix}\n",
    "   2(x_1+1) & 0 \\\\\n",
    "   0 & 2(x_2+1) \n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "$.\n",
    "\n",
    "当 $x=[1, 2]^T$ 时，\n",
    "$\n",
    "\\frac{\\partial y}{\\partial x} =\n",
    "\\left\\{\n",
    "\\begin{matrix}\n",
    "   4 & 0 \\\\\n",
    "   0 & 6 \n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "$.\n",
    "\n",
    "当 `gradient` $=[1, 1]$时，\n",
    "$\n",
    "[1, 1]^T \\times\n",
    "\\left\\{\n",
    "\\begin{matrix}\n",
    "   4 & 0 \\\\\n",
    "   0 & 6 \n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "= [4, 6]^T\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 2.], requires_grad=True)\n",
      "y: tensor([ 7., 12.], grad_fn=<AddBackward0>)\n",
      "x.grad:  tensor([4., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = (x+1)**2+3\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "# y 是二维的值，直接调用 .backward() 会报错\n",
    "y.backward(gradient=torch.tensor([1.0, 1.0]))\n",
    "print(\"x.grad: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 `gradient` $=[1, 2]$时，\n",
    "$\n",
    "[1, 2]^T \\times\n",
    "\\left\\{\n",
    "\\begin{matrix}\n",
    "   4 & 0 \\\\\n",
    "   0 & 6 \n",
    "\\end{matrix}\n",
    "\\right\\}\n",
    "= [4, 12]^T\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  tensor([ 4., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = (x+1)**2+3\n",
    "# gradient = [1, 2]\n",
    "y.backward(gradient=torch.tensor([1.0, 2.0]))\n",
    "print(\"x.grad: \", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  tensor([8., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = (x+1)**2+3\n",
    "# gradient = [2, 1]\n",
    "y.backward(gradient=torch.tensor([2.0, 1.0]))\n",
    "print(\"x.grad: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义函数的反向求导-TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# 神经网络模块`nn.Module`\n",
    "\n",
    "使用PyTorch的`nn.Module`模块构建神经网络的过程如下图所示.\n",
    "\n",
    "<img src=\"images/pytorch-nn.module.png\" width=\"75%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络\n",
    "\n",
    "实际使用中，构建神经网络的最常见做法是继承 `nn.Module` 类，在其中定义自己的网络结构。\n",
    "\n",
    "`torch.nn`中的核心结构是`Module`，它是一个抽象的概念，既可以表示神经网络中的某个层，还可以表示包含很多层的神经网络。\n",
    "\n",
    "构建一个神经网络，有如下几步：\n",
    "1. 继承 `nn.Module` 抽象基类，\n",
    "2. 在初始化方法`__init__()`中，存放网络中可学习的参数，并使用`nn.Parameters()`封装\n",
    "3. 实现`nn.Module`抽象类中的前向传播方法`forward()`，梯度的反向传播方法不需要自己写，PyTorch会自动生成\n",
    "4. 调用时可以是 `net.forward(X)`，也可以直接使用 `net(X)`\n",
    "5. 网络的可学习参数可以通过网络实例对象的`.parameters()` 或者 `.named_parameters()` 返回\n",
    "\n",
    "下面的例子是**构建一个线性（没有激活函数）的全连接层，也就是 $y=Wx+b$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个线性的网络\n",
    "from torch import nn\n",
    "\n",
    "class LinearNN(nn.Module):\n",
    "    # 构造函数\n",
    "    def __init__(self, dim_input, dim_output):\n",
    "        # 调用父类的构造函数\n",
    "        super().__init__()\n",
    "        # 再初始化自己的参数\n",
    "        # 随机初始化参数矩阵W\n",
    "        self.w = nn.Parameter(torch.randn(dim_input, dim_output))\n",
    "        # 随机初始化偏置项\n",
    "        self.b = nn.Parameter(torch.randn(dim_output))\n",
    "        \n",
    "    # 实现抽象类 nn.Module 的 forward 方法\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        \"\"\"\n",
    "        y = X.mm(self.w) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n",
      "Parameter containing:\n",
      "tensor([[ 0.6475, -0.4241],\n",
      "        [-0.1332, -1.3617],\n",
      "        [ 0.3725,  1.7960]], requires_grad=True)\n",
      "b\n",
      "Parameter containing:\n",
      "tensor([0.4546, 2.4694], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear = LinearNN(dim_input=3, dim_output=2)\n",
    "\n",
    "for param, value in linear.named_parameters():\n",
    "    print(param)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1510,  0.3097, -1.4704],\n",
       "        [-0.1441,  0.3727,  0.6084],\n",
       "        [-1.0195, -1.2713,  0.2241],\n",
       "        [ 2.6327,  0.1713,  1.1290]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(4,3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2322, -0.5291],\n",
       "        [ 0.5383,  3.1157],\n",
       "        [ 0.0472,  5.0352],\n",
       "        [ 2.5571,  3.1473]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显式调用 forward 方法\n",
    "# y_out = linear.forward(X)\n",
    "\n",
    "# 也可以直接调用\n",
    "y_out = linear(X)\n",
    "\n",
    "y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "## 常用的神经网络模块-KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n",
    "\n",
    "`nn.Linear(in_features: int, out_features: int, bias: bool = True)` .  \n",
    "计算的是 $y=XW^T+b$，其中 $X$的维度是$batch\\_size \\times in\\_features$, $W$的维度是 $out\\_features \\times in\\_features$.\n",
    "+ 初始化参数：\n",
    "  + `in_features`：**int**，输入数据的维度\n",
    "  + `out_features`: **int**，输出数据的维度\n",
    "  + `bias`：是否有偏置项\n",
    "  \n",
    "\n",
    "+ `forward()`方法参数：\n",
    "  + 输入参数`Input`：$(batch\\_size, * , in\\_features)$, `*` 是中间维度（没啥用）\n",
    "  + 返回参数`Output`：$(batch\\_size, *, out\\_features)$\n",
    "  \n",
    "\n",
    "+ 属性\n",
    "  + `.weight`：权重矩阵\n",
    "  + `.bias`：偏置项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:\n",
      "Parameter containing:\n",
      "tensor([[ 0.1275, -0.4422,  0.2409],\n",
      "        [ 0.1011,  0.2749,  0.1577]], requires_grad=True)\n",
      "bias:\n",
      "Parameter containing:\n",
      "tensor([-0.1225, -0.1347], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "print(\"weight:\")\n",
    "print(linear_model.weight)\n",
    "print(\"bias:\")\n",
    "print(linear_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3011,  0.3329,  1.7874],\n",
       "        [-0.3963,  0.6201, -0.1444],\n",
       "        [ 1.4532,  1.7288, -0.6337],\n",
       "        [ 1.1768,  0.4847,  2.3175]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(4,3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1325,  0.0059],\n",
       "        [-0.4821, -0.0270],\n",
       "        [-0.8543,  0.3876],\n",
       "        [ 0.3715,  0.4830]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_out = linear_model(X)\n",
    "y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax层\n",
    "\n",
    "+ `nn.Softmax(dim)`\n",
    "\n",
    "+ `nn.LogSofrmax(dim)`\n",
    "\n",
    "`dim`参数指定计算的轴维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1051, -0.6662, -1.5944,  1.7258, -1.1150],\n",
       "        [-1.9155, -0.5558,  1.1335, -0.5306, -1.0881],\n",
       "        [-0.2570, -0.6757, -0.7405, -0.3356, -0.2761],\n",
       "        [ 1.0059, -0.0551,  0.2353,  0.7235, -0.1148]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(4, 5)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1191, 0.0679, 0.0268, 0.7428, 0.0434],\n",
       "        [0.0310, 0.1207, 0.6536, 0.1238, 0.0709],\n",
       "        [0.2392, 0.1574, 0.1475, 0.2212, 0.2347],\n",
       "        [0.3462, 0.1198, 0.1602, 0.2610, 0.1129]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1282, -2.6893, -3.6175, -0.2973, -3.1382],\n",
       "        [-3.4741, -2.1144, -0.4252, -2.0892, -2.6468],\n",
       "        [-1.4303, -1.8490, -1.9138, -1.5089, -1.4494],\n",
       "        [-1.0608, -2.1219, -1.8315, -1.3433, -2.1815]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsoft = nn.LogSoftmax(dim=1)\n",
    "logsoft(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN相关\n",
    "\n",
    "+ 卷积层  \n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')` .  \n",
    "这里不能指定卷积核，只能指定卷积核的`kernel_size`\n",
    "\n",
    "\n",
    "+ 偏置项\n",
    "\n",
    "\n",
    "+ 池化层  \n",
    "`nn.MaxPool2d(kernel_size, stride = None, padding = 0, dilation = 1, return_indices = False, ceil_mode = False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]],\n",
       "\n",
       "        [[32., 33., 34., 35.],\n",
       "         [36., 37., 38., 39.],\n",
       "         [40., 41., 42., 43.],\n",
       "         [44., 45., 46., 47.]]],\n",
       "\n",
       "\n",
       "       [[[48., 49., 50., 51.],\n",
       "         [52., 53., 54., 55.],\n",
       "         [56., 57., 58., 59.],\n",
       "         [60., 61., 62., 63.]],\n",
       "\n",
       "        [[64., 65., 66., 67.],\n",
       "         [68., 69., 70., 71.],\n",
       "         [72., 73., 74., 75.],\n",
       "         [76., 77., 78., 79.]],\n",
       "\n",
       "        [[80., 81., 82., 83.],\n",
       "         [84., 85., 86., 87.],\n",
       "         [88., 89., 90., 91.],\n",
       "         [92., 93., 94., 95.]]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 输入的是 2张 4x4x3 的图片，这里为了容易辨别，将通道数3提前了一点\n",
    "imgs = np.arange(2*4*4*3,dtype=np.float32).reshape((2,3,4,4))\n",
    "imgs_tensor = torch.tensor(imgs)\n",
    "display(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN相关\n",
    "\n",
    "pytorch中RNN相关的支持分为三类：\n",
    "1. `nn.RNNBase`，RNN的基础抽象类\n",
    "2. RNN的单个Cell类，处理序列中的单独一步\n",
    "  + `nn.RNNCell`\n",
    "  + `nn.GRUCell`\n",
    "  + `nn.LSTMCell`\n",
    "3. RNN的单元封装类，处理整个序列，它是对RNN Cell类的封装，可以**构建多层RNN**\n",
    "  + `nn.RNN`\n",
    "  + `nn.GRU`\n",
    "  + `nn.LSTM`\n",
    "\n",
    "\n",
    "RNN处理的是序列数据，所以输入数据的维度中，多了一个 sequence 维度（或者叫 time_step ），所以输入的数据shape是 `(batch_size, seq_length, in_features)`。\n",
    "\n",
    "+ 以文本处理为例：\n",
    "  + 一般将一个句子看做一个sequence，而句子中的每个单词都用一个词向量来表示，那么一个句子就是一个矩阵 $v \\times s$，$v$ 是词向量的长度——所有单词都是一样的，$s$ 是这个句子里单词的数量——一个序列的长度。  \n",
    "  + 但是文本中，不是所有的句子长度 $s$ 都是一样的，需要做进一步的处理 —— 选择最大的句子长度（或者平均长度）为准，所有的句子都规范化成这个长度，超过的部分截断，不足的部分单词补0——这个补充的单词0也对应于一个词向量，这样就得到了形状规范的一个 sequence 样本。   \n",
    "  + 每个 sequence 对于RNN 来说是一个样本，每次训练使用多个，也就是 batch_size。\n",
    "  \n",
    "\n",
    "+ pytorch中构建**一层**RNN的时候，需要配置的参数有如下几个：\n",
    "  + `in_features`——对应于输入层的节点数\n",
    "  + `hidden_units`——隐藏层节点数，也就是状态向量的长度\n",
    "  + **序列长度这个参数不需要设置，它直接从样本中推断即可**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(5, 3, bias=False, batch_first=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建一个 单层、单向 RNN\n",
    "# 需要设置的两个参数：输入特征数为5，隐藏层状态向量长度=3\n",
    "# 由数据决定的两个参数：batch_size=2，序列长度=4\n",
    "in_features = 5\n",
    "hidden_size = 3\n",
    "num_layers = 1\n",
    "rnn_layer = nn.RNN(input_size=in_features, hidden_size=hidden_size, num_layers=num_layers, bidirectional=False, nonlinearity='relu', bias=False, batch_first=True)\n",
    "rnn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入的数据shape应当为 (batch_size, seq_length, in_features)\n",
    "batch_size = 2\n",
    "seq_length = 4\n",
    "rnn_data = torch.randn(batch_size, seq_length, in_features)\n",
    "rnn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入数据\n",
    "# 这里只输入了训练数据，没有输入 隐藏层状态的初始化参数 h_0（可以省略）\n",
    "rnn_result = rnn_layer(input=rnn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "torch.Size([2, 4, 3])\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 结果是一个 长度=2 的tuple\n",
    "print(rnn_result.__class__)\n",
    "print(len(rnn_result))\n",
    "\n",
    "# 第一个是 output，它的 shape = (batch_size, seq_length, hidden_size*num_directions)\n",
    "print(rnn_result[0].shape)\n",
    "\n",
    "# 第二个是 h_n，表示各隐藏层的状态，shape=(num_layers*num_directions, batch_size, hidden_size)\n",
    "print(rnn_result[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "## 激活函数\n",
    "\n",
    "PyTorch中，激活函数也被封装成了一个单独的层来使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "构建完神经网络的前向传播过程之后，需要计算神经网络输出的损失。\n",
    "\n",
    "PyTorch中，损失函数也位于 `nn.Module`这个模块中。\n",
    "\n",
    "损失函数有两种使用方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n",
    "`nn.MSELoss()`类，它实例化的对象接受的参数为`(Input, Target)`，`Input`和`Target`的shape必须是相同的，**可以是矩阵，此时计算的MSE就是矩阵对应元素之间的MSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true:\n",
      "tensor([1., 2., 3.])\n",
      "y_pred:\n",
      "tensor([3., 4., 5.], requires_grad=True)\n",
      "MSE-Loss:\n",
      "tensor(4., grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# shape=(3,)\n",
    "y_true = torch.tensor([1,2,3], dtype=torch.float)\n",
    "y_pred = torch.tensor([3,4,5], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "# shape=(3,1)\n",
    "# y_true = torch.tensor([[1], [2], [3]], dtype=torch.float)\n",
    "# y_pred = torch.tensor([[3], [4], [5]], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "loss = mse(y_pred, y_true)\n",
    "\n",
    "print(\"y_true:\")\n",
    "print(y_true)\n",
    "print(\"y_pred:\")\n",
    "print(y_pred)\n",
    "\n",
    "print(\"MSE-Loss:\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 下面这个矩阵计算的就是对应位置元素的MSE   \n",
    "$mse = (0+1+0+1+0+1)/6=\\frac{3}{6}=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true:\n",
      "tensor([[1., 2.],\n",
      "        [2., 3.],\n",
      "        [3., 4.]])\n",
      "y_pred:\n",
      "tensor([[1., 3.],\n",
      "        [2., 4.],\n",
      "        [3., 5.]], requires_grad=True)\n",
      "MSE-Loss:\n",
      "tensor(0.5000, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor([[1,2], [2,3], [3,4]], dtype=torch.float)\n",
    "y_pred = torch.tensor([[1,3], [2,4], [3,5]], dtype=torch.float, requires_grad=True)\n",
    "# y_pred = torch.tensor([[1,4], [2,5], [3,6]], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "loss = mse(y_pred, y_true)\n",
    "\n",
    "print(\"y_true:\")\n",
    "print(y_true)\n",
    "print(\"y_pred:\")\n",
    "print(y_pred)\n",
    "\n",
    "print(\"MSE-Loss:\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对数似然误差\n",
    "\n",
    "对数似然误差(Negative Log Likelihood Loss, 缩写为**NLLL**)对应的类是 `nn.NLLLoss()`，通常用于多分类问题 .    \n",
    "\n",
    "它实例化的对象接受的参数为`(Input, Target)`：\n",
    "+ `Input`的 `shape=(batch_size, C)`，$C$ 是类别的个数。其中每一行的值应当是经过 softmax 之后 取对数的结果——也就是所谓的 Log Likelihood.\n",
    "+ `Target`的 `shape=(batch_size)`\n",
    "\n",
    "这个类的构造函数里有一个 `reduction='mean'` 的参数，表示会将得到的结果进行reduce，方式是取均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true.shape:  torch.Size([3])\n",
      "y_true:\n",
      "tensor([1, 0, 0])\n",
      "\n",
      "y_pred.shape:  torch.Size([3, 2])\n",
      "y_pred:\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]], requires_grad=True)\n",
      "\n",
      "Negative-Likehood-Log-Loss (关闭 reduce):\n",
      "tensor([-0., -1., -0.], grad_fn=<NllLossBackward>)\n",
      "\n",
      "Negative-Likehood-Log-Loss (reduce='mean'):\n",
      "tensor(-0.3333, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor([1, 0, 0], dtype=torch.long)\n",
    "y_pred = torch.tensor([[1,0], [1,0], [0,1]], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "print(\"y_true.shape: \", y_true.shape)\n",
    "print(\"y_true:\")\n",
    "print(y_true)\n",
    "print(\"\\ny_pred.shape: \", y_pred.shape)\n",
    "print(\"y_pred:\")\n",
    "print(y_pred)\n",
    "\n",
    "nll = nn.NLLLoss(reduction='none') # 关闭 reduce\n",
    "loss = nll(y_pred, y_true)\n",
    "\n",
    "print(\"\\nNegative-Likehood-Log-Loss (关闭 reduce):\")\n",
    "print(loss)\n",
    "\n",
    "nll = nn.NLLLoss() # 打开 reduce，默认为mean\n",
    "loss = nll(y_pred, y_true)\n",
    "\n",
    "print(\"\\nNegative-Likehood-Log-Loss (reduce='mean'):\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的例子演示了该损失函数的工作方式： \n",
    "+ 对于一个二分类的问题，有 3 个样本，\n",
    "+ 输入的 `y_true.shape=[3]`，每个元素是实际的类别 0 或 1。如果是多分类，有 $C$ 个类别，那么取值就是 $[0, C-1]$.\n",
    "+ 输入的 `y_pred.shape=[3, 2]`, 每一行是长度=2的向量，向量的每个值对应于取该类别的概率——**实际输入时这个应该是经过 softmax + log 之后的 log likelyhood**。\n",
    "+ 在 `reduce='none'`时，NLLLoss 会将 `y_true` 看做 `y_pred` 中每一行的索引，取出其中的值，并加上 负号。  \n",
    "这是因为 `y_true`中的元素取值为 $[0, C-1]$，就是 `y_pred` 每一行的长度，用于从中获得该样本取每一类的对数概率。\n",
    "+ `reduce='mean'`时，就是将上述结果进行聚合。\n",
    "\n",
    "实际中使用时，NLLOSS 的输入通常是经过 `nn.LogSoftmax` 变换得到的，下面是一个完整的例子（源自官方文档）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor:\n",
      " tensor([[-0.7142, -1.4875, -0.9207, -0.1043, -0.0645],\n",
      "        [ 0.5339,  0.2950,  0.4972, -0.7490,  1.0334],\n",
      "        [-0.2580, -1.1683, -0.9682, -0.8448, -0.2316]], requires_grad=True)\n",
      "\n",
      " LogSoftmax out:\n",
      " tensor([[-1.7968, -2.5701, -2.0033, -1.1869, -1.1470],\n",
      "        [-1.5426, -1.7815, -1.5793, -2.8255, -1.0430],\n",
      "        [-1.2461, -2.1564, -1.9563, -1.8329, -1.2197]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "\n",
      " NLLoss out:\n",
      " tensor(1.7775, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# NLLoss损失函数官方示例\n",
    "logsoftmax = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "\n",
    "# 输入的 size N x C = 3 x 5， 表示 3 个样本，类别是 5 类\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(\"input_tensor:\\n\", input)\n",
    "\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "\n",
    "# 先对 input 做 softmax + log 变换，每一行表示取各个类别的对数概率\n",
    "logsoftmax_out = logsoftmax(input)\n",
    "print(\"\\n LogSoftmax out:\\n\", logsoftmax_out)\n",
    "\n",
    "# 将 LogSoftmax 的结果作为 输入\n",
    "output = loss(logsoftmax_out, target)\n",
    "print(\"\\n NLLoss out:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵误差\n",
    "\n",
    "`nn.CrossEntropyLoss()`可以用于多分类问题，它就是对 `nn.LogSoftmax` + `nn.NLLLoss` 的封装 .    \n",
    "它实例化的对象接受的参数为`(Input, Target)`：\n",
    "+ `Input`的 `shape=(batch_size, C)`，$C$ 是类别的个数\n",
    "+ `Target`的 `shape=(batch_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossEnt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true.shape:  torch.Size([3])\n",
      "y_true:\n",
      "tensor([1, 0, 0])\n",
      "\n",
      "y_pred.shape:  torch.Size([3, 2])\n",
      "y_pred:\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]], requires_grad=True)\n",
      "\n",
      "Negative-Likehood-Log-Loss:\n",
      "tensor(0.9799, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor([1, 0, 0], dtype=torch.long)\n",
    "y_pred = torch.tensor([[1,0], [1,0], [0,1]], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "print(\"y_true.shape: \", y_true.shape)\n",
    "print(\"y_true:\")\n",
    "print(y_true)\n",
    "print(\"\\ny_pred.shape: \", y_pred.shape)\n",
    "print(\"y_pred:\")\n",
    "print(y_pred)\n",
    "\n",
    "loss = crossEnt(y_pred, y_true)\n",
    "\n",
    "print(\"\\nNegative-Likehood-Log-Loss:\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "\n",
    "# 优化器\n",
    "\n",
    "计算完神经网络前向传播的损失函数之后，需要使用PyTorch里的优化器模块 `torch.optim` 对损失函数进行反向求导，更新梯度."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器使用\n",
    "\n",
    "+ 构造优化器  \n",
    "构造一个优化器，通常要传入模型的参数、学习率等参数\n",
    "```python\n",
    "model = nn.Linear(3,2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "\n",
    "+ 使用优化器来更新梯度  \n",
    "调用优化器对象的`.step()`方法来更新梯度参数，更新参数之前，必须要进行反向求导的操作，也就是调用`.backward()`方法。  \n",
    "一个常用的简化版本如下：\n",
    "```python\n",
    "# 构造模型\n",
    "model = nn.Linear(3,2)\n",
    "# 构造模型对应的优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "# 在数据集上分批次迭代\n",
    "for input, target in dataset:\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 获取输出\n",
    "    output = model(input)\n",
    "    # 计算损失\n",
    "    loss = loss_fn(output, target)\n",
    "    # 反向求导，计算每个参数的梯度\n",
    "    loss.backward()\n",
    "    # 调用优化器进行梯度更新\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "可以参考StackOverflow回答 [pytorch - connection between loss.backward() and optimizer.step()](https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step/66192315#66192315).\n",
    "\n",
    "注意：`loss.backward()` **这一步不是必须的，如果没有计算梯度，那么调用`optimizer.step()`时，不会对参数进行更新**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面以函数 $z = 3x^2 + y^3$ 为例，介绍PyTorch中反向更新的方式.  \n",
    "$z$ 关于 $x$ 和 $y$ 的梯度更新公式分别为：  \n",
    "$x := x - \\alpha \\frac{\\partial z}{\\partial x} = x - \\alpha 6x$,   \n",
    "$y := y - \\alpha \\frac{\\partial z}{\\partial y} = y - \\alpha 3y^2$.  \n",
    "初始值分别为：$x=1$ 和 $y=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  None\n",
      "y.grad:  None\n",
      "z.grad:  None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "z = 3*x**2+y**3\n",
    "\n",
    "# 初始时梯度均为 None\n",
    "print(\"x.grad: \", x.grad)\n",
    "print(\"y.grad: \", y.grad)\n",
    "print(\"z.grad: \", z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算 $x=1$ 和 $y=2$ 时的梯度，分别为 $\\frac{\\partial z}{\\partial x} = 6x = 6, \\frac{\\partial z}{\\partial y} = 3y^2 = 12$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad:  tensor([6.])\n",
      "y.grad:  tensor([12.])\n",
      "z.grad:  None\n"
     ]
    }
   ],
   "source": [
    "# 计算梯度\n",
    "z.backward()\n",
    "print(\"x.grad: \", x.grad)\n",
    "print(\"y.grad: \", y.grad)\n",
    "print(\"z.grad: \", z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行一次梯度更新操作，也就是基于梯度的反方向，更新 $x$ 和 $y$，这里学习率 $\\alpha=0.1$.  \n",
    "$x := x - \\alpha \\frac{\\partial z}{\\partial x} = x - 0.1 \\times 6x = 1 - 0.1 \\times 6 = 0.4$,   \n",
    "$y := y - \\alpha \\frac{\\partial z}{\\partial y} = y - 0.1 \\times 3y^2 = 2 - 0.1 \\times 12 = 0.8$. \n",
    "\n",
    "**如果没有执行上面的`.backward()`这一步计算梯度，那么下面就不会更新参数值**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1.], requires_grad=True)\n",
      "y: tensor([2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 将要更新的参数x,y传入优化器，学习率设置为0.1\n",
    "optimizer = optim.SGD([x, y], lr=0.1)\n",
    "# 根据已有的梯度，更新x,y\n",
    "optimizer.step()\n",
    "\n",
    "# 更新之后的x和y\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动态学习率\n",
    "\n",
    "`torch.optim.lr_scheduler`子模块提供了随着训练epoch改变的动态学习率设置。\n",
    "\n",
    "+ `optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1, verbose=False)`  \n",
    "  **用于自定义学习率随着训练epoch变化的情况**。\n",
    "  + `optimizer`，传入的`optimizer`对象\n",
    "  + `lr_lambda`，用于设置随着epoch的学习速率，有两种方式：\n",
    "    1. fuction：直接设置的函数\n",
    "    2. list of function，每个函数对应于一组参数\n",
    "  + `last_epoch`\n",
    "  \n",
    "\n",
    "+ `optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1, verbose=False)`  \n",
    "  **等间隔调整学习率**，每隔 step_size 个 epoch，将学习率更新为原来的 gamma 倍.\n",
    "\n",
    "+ `optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1, verbose=False)`  \n",
    "按指数衰减调整学习率，调整公式: $lr = lr \\times gamma^{-epoch}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1 lr: 0.1\n",
      "Epoch-2 lr: 0.1\n",
      "Epoch-3 lr: 0.1\n",
      "Epoch-4 lr: 0.1\n",
      "Epoch-5 lr: 0.1\n",
      "\n",
      "Epoch-6 lr: 0.2\n",
      "Epoch-7 lr: 0.2\n",
      "Epoch-8 lr: 0.2\n",
      "Epoch-9 lr: 0.2\n",
      "Epoch-10 lr: 0.2\n",
      "\n",
      "Epoch-11 lr: 0.30000000000000004\n",
      "Epoch-12 lr: 0.30000000000000004\n",
      "Epoch-13 lr: 0.30000000000000004\n",
      "Epoch-14 lr: 0.30000000000000004\n",
      "Epoch-15 lr: 0.30000000000000004\n",
      "\n",
      "Epoch-16 lr: 0.4\n",
      "Epoch-17 lr: 0.4\n",
      "Epoch-18 lr: 0.4\n",
      "Epoch-19 lr: 0.4\n",
      "Epoch-20 lr: 0.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "z = 3*x**2+y**3\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = optim.SGD([x, y], lr=0.1)\n",
    "\n",
    "# 自定义根据 epoch 来计算学习率的 lambda 函数\n",
    "lambda1 = lambda epoch: np.ceil(epoch/5)\n",
    "\n",
    "# 然后创建一个更新优化器中学习率的schduler对象\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "\n",
    "# 打印每轮中的学习率参数\n",
    "for epoch in range(1, 21):\n",
    "    \n",
    "    # 这里没有指定.backward()方法计算梯度，所以每一轮中不会对参数进行更新\n",
    "    \n",
    "    # 先调用优化器的step\n",
    "    optimizer.step()\n",
    "    # 再调用 scheduler 更新学习率\n",
    "    scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    if epoch % 5 == 0:print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "\n",
    "PyTorch中的正则化也是在`optim`模块中实现的，不过optimizer只能实现L2正则化，L1正则化只能手动实现.  \n",
    "optimizer优化器的参数`weight_decay (float, optional)`就是 L2 正则项，它的默认值为0.\n",
    "\n",
    "可以参考如下讨论：\n",
    "+ [Simple L2 regularization?](https://discuss.pytorch.org/t/simple-l2-regularization/139)\n",
    "+ [Adding L1/L2 regularization in PyTorch?](https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 数据加载和变换\n",
    "\n",
    "详细说明可以参考官方文档 [WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "\n",
    "一般训练的时候，需要对数据进行分batch进行，为了更方便的提供数据，Pytorch提供了 `torch.utils.data` 包用于辅助处理数据。  \n",
    "这里面主要有两个类：\n",
    "1. `data.DataSet()`，一个抽象类，提供对数据的封装。\n",
    "2. `data.DataLoader()`，在 `DataSet`上进行迭代，每次返回一个 batch 的数据，并且可以实现 shuffle 和 多进程读取。\n",
    "\n",
    "+ 自定义数据集需要做如下步骤：\n",
    "  1. 编写一个类，继承`data.DataSet`类\n",
    "  2. 实现`__len__()`方法和 `__getitem__()` 方法\n",
    "  \n",
    "  \n",
    "+ 自定义数据集类之后，只需要将它传递给`data.DataLoader()`类作为初始化参数即可返回一个可迭代的对象，在此对象上执行分批操作即可."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# GPU加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查当前是否有可用的 CUDA 环境\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回当前使用的 CUDA设备的 id\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查一共有多少块 CUDA设备 可用\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2060'"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取 CUDA设备ID 对应的设备名称\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 指定设备\n",
    "# 指定CPU设备\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "# 指定GPU设备\n",
    "device_gpu = torch.device(\"cuda:0\")\n",
    "print(device_cpu)\n",
    "print(device_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 常用的一个用法是\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 释放占用的显存\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "# PyTorch示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络示例\n",
    "\n",
    "作为示例，实现如下的一个简单神经网络，**它只有一个隐藏层，并且隐藏层没有使用激活函数。**\n",
    "\n",
    "<img src=\"images/f2.png\" width=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建网络的前向传播过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\"\n",
    "    两层的感知机模型\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input, dim_hidden):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Parameter(torch.randn(dim_input, dim_hidden))\n",
    "        # 这里输出层本来应当是一个单元，但是作为分类问题，需要输出二分类下，各个类的概率，所以输出层也为2\n",
    "        self.w2 = nn.Parameter(torch.randn(dim_hidden, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        神经网络的前向传播函数\n",
    "        x: 输入的Tensor\n",
    "        \"\"\"\n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        # print(\"w1.shape: \", self.w1.shape)\n",
    "        layer = x.mm(self.w1)\n",
    "        output = layer.mm(self.w2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (128, 2)\n",
      "X:\n",
      " [[0.86375999 0.28490597]\n",
      " [0.07325639 0.7632372 ]\n",
      " [0.45271906 0.54229687]\n",
      " [0.72663578 0.84890511]\n",
      " [0.76819998 0.73314372]]\n",
      "\n",
      "y_true.shape:  (128,)\n",
      "y_true:\n",
      " [0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 使用numpy随机模拟数据\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rnd = RandomState(29)\n",
    "datasize = 128\n",
    "# 随机生成 datesize x 2 的矩阵\n",
    "X = rnd.rand(datasize,2)\n",
    "# 对于X中的每一行，计算一个对于的 y_true 值，\n",
    "# y_true = np.array([ [int(x1+x2<1)] for (x1,x2) in X])\n",
    "y_true = np.array([ int(x1+x2<1) for (x1,x2) in X])\n",
    "\n",
    "print(\"X.shape: \", X.shape)\n",
    "print(\"X:\\n\",X[:5,:])\n",
    "print(\"\\ny_true.shape: \", y_true.shape)\n",
    "print(\"y_true:\\n\",y_true[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter-w1 :\n",
      "Parameter containing:\n",
      "tensor([[ 1.1891,  0.3408, -0.3820],\n",
      "        [ 1.2111, -1.1438, -2.0302]], requires_grad=True)\n",
      "Parameter-w2 :\n",
      "Parameter containing:\n",
      "tensor([[ 1.7271, -1.0313],\n",
      "        [ 1.0159,  1.2647],\n",
      "        [-0.1260,  1.5183]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "percep = Perceptron(2, 3)\n",
    "# x = torch.tensor([1.0, 2.0]).reshape(1,2)\n",
    "# y  = percep.forward(x)\n",
    "# y\n",
    "\n",
    "for name, param in percep.named_parameters():\n",
    "    print(\"Parameter-{} :\\n{}\".format(name, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape:  torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "y_pred = percep.forward(torch.tensor(X).type(torch.FloatTensor))\n",
    "print(\"y_pred.shape: \", y_pred.shape)\n",
    "# y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy-Loss:  tensor(1.8746, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "crossEnt = nn.CrossEntropyLoss()\n",
    "\n",
    "y_true_tensor = torch.tensor(y_true).type(torch.long)\n",
    "loss = criterion(y_pred, y_true_tensor)\n",
    "print(\"CrossEntropy-Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用优化器更新梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter-w1 :\n",
      "Parameter containing:\n",
      "tensor([[ 0.7964,  0.3763, -0.1479],\n",
      "        [ 0.7198, -1.0995, -1.7373]], requires_grad=True)\n",
      "Parameter-w2 :\n",
      "Parameter containing:\n",
      "tensor([[ 1.3422, -0.6463],\n",
      "        [ 1.1711,  1.1095],\n",
      "        [ 0.2900,  1.1023]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. 优化器，传入已有神经网络对象的参数\n",
    "sgd = optim.SGD(params=percep.parameters(), lr=1)\n",
    "\n",
    "# 2. 清空梯度\n",
    "sgd.zero_grad()\n",
    "\n",
    "# 3. 对损失函数进行反向求导\n",
    "loss.backward()\n",
    "\n",
    "# 4. 执行优化器，更新一次梯度\n",
    "sgd.step()\n",
    "\n",
    "# 5. 检查更新后的梯度参数\n",
    "for name, param in percep.named_parameters():\n",
    "    print(\"Parameter-{} :\\n{}\".format(name, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy-Loss:  tensor(0.9022, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 检查新的交叉熵损失\n",
    "y_pred = percep.forward(torch.tensor(X).type(torch.FloatTensor))\n",
    "y_true_tensor = torch.tensor(y_true).type(torch.long)\n",
    "loss = criterion(y_pred, y_true_tensor)\n",
    "print(\"CrossEntropy-Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生数据\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(29)\n",
    "\n",
    "def sim_data(batch_size=10):\n",
    "    x = np.arange(0, batch_size).reshape((batch_size, 1))\n",
    "    y = x*2 + 3 + rng.randn(batch_size, 1)*5\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22726fd3668>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUC0lEQVR4nO3dfWhUd77H8c9MpqnWNMk81ZA0skQti6CVbCRuWYltZh+wbg37h5CqxZUircLSKNJs76IX2UJ2a3ZEbsS9bHHhIpfK/hHpsnsLU9kEKqyzK9Js281qm+4DMZuHmcSYTepOzrl/pCYZTTvJZCZnfpP366/MjzmZL1/Ch5Pv+c05Ltu2bQEAjON2ugAAQHoIcAAwFAEOAIYiwAHAUAQ4ABiKAAcAQ3mW+gN7e3vTOi4QCGhwcDDD1ZiLfsygF8noR7J86Ed5efmc65yBA4ChCHAAMFTKEcrdu3d14sQJJRIJTU5OauvWrdq9e7fu3LmjcDisgYEBBYNBNTU1qaioaClqBgBoHgH+0EMP6cSJE1qxYoUSiYSOHz+uzZs36+rVq9q4caMaGhrU3t6u9vZ27d27dylqBgBoHiMUl8ulFStWSJImJyc1OTkpl8ulaDSquro6SVJdXZ2i0Wh2KwUAJJnXLhTLsvTqq6+qr69P3/72t7V+/XqNjIzI6/VKkrxer27fvj3nsZFIRJFIRJLU0tKiQCCQXqEeT9rH5iP6MYNeJKMfUxJ9vRr73/9WPD6oh7wBrWo8KE/Z3Ls5TDWvAHe73XrjjTc0NjamU6dO6W9/+9u8PyAUCikUCk2/Tnc7Tz5sBcok+jGDXiSjH5I10Cc7fFwa6Jtem/jofbmaTsodLHOwsvRkZBvhqlWrtGHDBl2/fl0lJSWKx+OSpHg8ruLi4sVXCQCZcOlCUnhLmnp96YIz9WRJygC/ffu2xsbGJE3tSOnq6lJFRYVqamrU0dEhSero6NCWLVuyWykAzJM9HFvQuqlSjlDi8bja2tpkWZZs29bXv/51fe1rX9MTTzyhcDisy5cvKxAI6MiRI0tRLwCk5Cr1aa4n1bhKfUteSza5lvqJPHyVPjPoxwx6kYx+zD0DV7As72bgS34vFADINnewTFbTSenSBXnGRpVY9ai0a4+R4f1lCHAAeckdLJNePCpfHv9Hwr1QAMBQBDgAGIoABwBDEeAAYCgCHAAMRYADgKEIcAAwFAEOAIYiwAHAUAQ4ABiKAAcAQxHgAGAoAhwADEWAA4ChCHAAMBQBDgCGIsABwFAEOAAYigAHAEMR4ABgKAIcAAxFgAOAoTyp3jA4OKi2tjYNDw/L5XIpFAppx44dunjxot59910VFxdLkhobG1VdXZ31ggEAU1IGeEFBgfbt26eqqiqNj4+rublZmzZtkiQ9++yzeu6557JeJADgQSkD3Ov1yuv1SpJWrlypiooKxWKxrBcGAPhyKQN8tv7+fvX09GjdunX685//rHfeeUednZ2qqqrSCy+8oKKiogeOiUQiikQikqSWlhYFAoH0CvV40j42H9GPGfQiGf1Ils/9cNm2bc/njRMTEzpx4oS+973vqba2VsPDw9Pz77feekvxeFyHDh1K+Xt6e3vTKjQQCGhwcDCtY/MR/ZhBL5LRj2T50I/y8vI51+e1CyWRSKi1tVXbtm1TbW2tJKm0tFRut1tut1v19fX6+OOPM1ctACCllAFu27bOnTuniooK7dy5c3o9Ho9P/3z16lVVVlZmp0IAwJxSzsC7u7vV2dmpNWvW6NixY5Kmtgy+9957+vTTT+VyuRQMBnXw4MGsFwsAmJEywL/61a/q4sWLD6yz5xsAnMU3MQHAUAQ4ABiKAAcAQxHgAGAoAhwADEWAA4ChCHAAMBQBDgCGIsABwFAEOAAYigAHAEMR4ABgKAIcAAxFgAOAoQhwADAUAQ4AhiLAAcBQBDgAGIoABwBDEeAAYCgCHAAMRYADgKEIcAAwFAEOAIbypHrD4OCg2traNDw8LJfLpVAopB07dujOnTsKh8MaGBhQMBhUU1OTioqKlqJmAIDmEeAFBQXat2+fqqqqND4+rubmZm3atEm/+93vtHHjRjU0NKi9vV3t7e3au3fvUtQMAMawBvqkSxdkD8fkKvVJu/bIHSzLyO9OOULxer2qqqqSJK1cuVIVFRWKxWKKRqOqq6uTJNXV1SkajWakIADIF9ZAn+zwcdm/75C6u2T/vkN2+PhUqGfAgmbg/f396unp0bp16zQyMiKv1ytpKuRv376dkYIAIG9cuiDdH9afn5FnQsoRyj0TExNqbW3V/v379cgjj8z7AyKRiCKRiCSppaVFgUBg4VVK8ng8aR+bj+jHDHqRjH4kc7IfsbFR/XuOdc/YqHwZqGleAZ5IJNTa2qpt27aptrZWklRSUqJ4PC6v16t4PK7i4uI5jw2FQgqFQtOvBwcH0yo0EAikfWw+oh8z6EUy+pHMyX5Yqx6dcz2x6tEF1VReXj7nesoRim3bOnfunCoqKrRz587p9ZqaGnV0dEiSOjo6tGXLlnkXAyB/WQN9sn7RqslT/yHrF60Zm/caadce6f4LlsGyqfUMSHkG3t3drc7OTq1Zs0bHjh2TJDU2NqqhoUHhcFiXL19WIBDQkSNHMlIQAHPdu2h3b+5rS9In3bKaTmZs54VJ3MEyWU0ns7YLxWXbtp2R3zRPvb29aR3Hv4XJ6McMepHM0ZHBL1qndlzcx1VbJ/eLRx2oKD/+PtIeoQDAfNnDsQWtY3EIcAAZ4yr1LWgdi0OAA8icLF+0Q7J57wMHgFSyfdEOyQhwABnlDpZJDl2wXG4YoQCAoQhwADAUAQ4AhiLAAcBQBDgAGIoABwBDEeAAYCj2gQN54t6zF2Njo1P3oeYLNHmPAAfywOzbuE4/AWYZ38Z1uWCEAuSDLD97EbmJAAfyALdxXZ4IcCAPcBvX5YkAB/IBt3FdlriICeSB2bdx9YyNKsEulGWBAIex2DaX7N5tXH158AxIzA8BDiOxbQ5gBg5TsW0OIMBhJrbNAQQ4DMW2OYAAh6nYNgekvoh59uxZXbt2TSUlJWptbZUkXbx4Ue+++66Ki4slSY2Njaqurs5upcAsbJsD5hHg27dv13e+8x21tbUlrT/77LN67rnnslYYkArb5rDcpRyhbNiwQUVFRUtRCwBgAdLeB/7OO++os7NTVVVVeuGFF74w5CORiCKRiCSppaVFgUAgvUI9nrSPzUf0Ywa9SEY/kuVzP1y2bdup3tTf36+f/OQn0zPw4eHh6fn3W2+9pXg8rkOHDs3rA3t7e9MqNMC/yUnoxwx6kYx+JMuHfpSXl8+5ntYulNLSUrndbrndbtXX1+vjjz9eVHEAgIVLK8Dj8fj0z1evXlVlZWXGCgIAzE/KGfjp06f14YcfanR0VC+99JJ2796tDz74QJ9++qlcLpeCwaAOHjy4FLUCAGZJGeCvvPLKA2vPPPNMNmoBACwA38QEAEMR4ABgKAIcAAxFgAOAoQhwADAUAQ4AhiLAAcBQBDgAGIoABwBDpX07WSxv1udPgLeHY1PPoeRpOMCSI8CxYNZAn+zwcWmgT5JkS9In3bKaThLiwBJihIKFu3RhOrynfX5GDmDpEOBYMHs4tqB1ANlBgGPBXKW+Ba0DyA4CHAu3a490/6w7WDa1DmDJcBETC+YOlslqOskuFMBhBDjS4g6WSS8edboMYFljhAIAhiLAAcBQBDgAGIoABwBDEeAAYCgCHAAMRYADgKFS7gM/e/asrl27ppKSErW2tkqS7ty5o3A4rIGBAQWDQTU1NamoqCjrxQIAZqQ8A9++fbtee+21pLX29nZt3LhRZ86c0caNG9Xe3p6t+gAAXyBlgG/YsOGBs+toNKq6ujpJUl1dnaLRaHaqAwB8obS+Sj8yMiKv1ytJ8nq9un379he+NxKJKBKJSJJaWloUCATS+Uh5PJ60j81H9GMGvUhGP5Llcz+yfi+UUCikUCg0/XpwcDCt3xMIBNI+Nh/Rjxn0Ihn9SJYP/SgvL59zPa1dKCUlJYrH45KkeDyu4uLi9CsDAKQlrQCvqalRR0eHJKmjo0NbtmzJaFEAgNRSjlBOnz6tDz/8UKOjo3rppZe0e/duNTQ0KBwO6/LlywoEAjpy5MhS1AoAmCVlgL/yyitzrh8/fjzTtQAAFoBvYgKAoQhwADAUAQ4AhiLAAcBQBDgAGIoABwBDEeAAYCgCHAAMRYADgKGyfjdCIN9ZA33SpQuyh2NylfqkXXvkDpY5XRaWAQIcWARroE92+Lg00CdJsiXpk25ZTScJcWQdIxRgMS5dmA7vaZ+fkQPZRoADi2APxxa0DmQSAQ4sgqvUt6B1IJMIcGAxdu2R7p91B8um1oEs4yImsAjuYJmsppPsQoEjCHBgkdzBMunFo06XgWWIEQoAGIoABwBDEeAAYCgCHAAMRYADgKEIcAAwFAEOAIZa1D7ww4cPa8WKFXK73SooKFBLS0um6gIApLDoL/KcOHFCxcXFmagFALAAjFAAwFAu27btdA8+fPiwioqKJEnf/OY3FQqFHnhPJBJRJBKRJLW0tOju3btpfZbH41EikUi31LxDP2bQi2T0I1k+9KOwsHDO9UUFeCwWk8/n08jIiH784x/r+9//vjZs2PClx/T29qb1WYFAQIODg2kdm4/oxwx6kYx+JMuHfpSXl8+5vqgRis83dc/jkpISbdmyRTdv3lzMrwMALEDaAT4xMaHx8fHpn99//32tWbMmY4UBAL5c2rtQRkZGdOrUKUnS5OSkvvGNb2jz5s2ZqgsAkELaAb569Wq98cYbmawFALAAbCMEAEMR4ABgKAIcAAxFgAOAoQhwADAUAQ4AhiLAAcBQBDgAGIoABwBDLfqBDtlmDfRJly4oNjYqa9Wj0q49cgfLnC4LAByX0wFuDfTJDh+XBvr073uLn3TLajpJiANY9nJ7hHLpgjTQl7z2+Rk5ACx3OX0Gbg/HFrSeTfdGOfZwTK5SH6McAI7L6QB3lfo01+OCXKW+Ja1j9ihH0lRNjHIAOCy3Ryi79kj3B2SwbGp9KTHKAZCDcvoM3B0sk9V0Urp0QZ6xUSUc2oWSS6McALgnpwNcmgpxvXhUPgcfTJoroxwAmC23Ryi5IldGOQAwS86fgeeC2aMcdqEAyBUE+DzdG+UAQK5ghAIAhiLAAcBQBDgAGIoABwBDLeoi5vXr13X+/HlZlqX6+no1NDRkqCwAQCppn4FblqU333xTr732msLhsN577z394x//yGRtAIAvkfYZ+M2bN1VWVqbVq1dLkp566ilFo1E9/vjjGSsOD+IBFwDuSTvAY7GY/H7/9Gu/368bN25kpCjMjQdcAJgt7QC37QfvDuJyuR5Yi0QiikQikqSWlhYFAoG0Ps/j8aR9bL4Y+Z//0sQcd0V8+P9+pZKm/3SkplzA30Yy+pEsn/uRdoD7/X4NDQ1Nvx4aGpLX633gfaFQSKFQaPp1ujekCjh4M6tcMfnPW3OuT/zzlv69jHvD30Yy+pEsH/pRXl4+53raFzHXrl2rW7duqb+/X4lEQleuXFFNTU3aBSK1L7r7IXdFBJantM/ACwoKdODAAb3++uuyLEtPP/20KisrM1kb7rdrj/RJd/LDJbgrIrBsLWofeHV1taqrqzNVC1LIlQdcAMgN3I3QMLnwgAsAuYGv0gOAoQhwADAUAQ4AhiLAAcBQBDgAGMplz/WdeABAzjPmDLy5udnpEnIK/ZhBL5LRj2T53A9jAhwAkIwABwBDGRPgs+9oCPoxG71IRj+S5XM/uIgJAIYy5gwcAJCMAAcAQxlxN8Lr16/r/PnzsixL9fX1amhocLokRwwODqqtrU3Dw8NyuVwKhULasWOH02U5zrIsNTc3y+fz5fWWsfkYGxvTuXPn9Pe//10ul0svv/yynnjiCafLcsSvf/1rXb58WS6XS5WVlTp06JAKCwudLiujcj7ALcvSm2++qR/96Efy+/364Q9/qJqaGj3++ONOl7bkCgoKtG/fPlVVVWl8fFzNzc3atGnTsuzFbL/5zW9UUVGh8fFxp0tx3Pnz57V582YdPXpUiURCn332mdMlOSIWi+m3v/2twuGwCgsL9bOf/UxXrlzR9u3bnS4to3J+hHLz5k2VlZVp9erV8ng8euqppxSNRp0uyxFer1dVVVWSpJUrV6qiokKxWMzhqpw1NDSka9euqb6+3ulSHPevf/1LH330kZ555hlJUw/zXbVqlcNVOceyLN29e1eTk5O6e/funM/sNV3On4HHYjH5/f7p136/Xzdu3HCwotzQ39+vnp4erVu3zulSHPXLX/5Se/fu5exbU38TxcXFOnv2rP7617+qqqpK+/fv14oVK5wubcn5fD5997vf1csvv6zCwkI9+eSTevLJJ50uK+Ny/gx8rl2OLpfLgUpyx8TEhFpbW7V//3498sgjTpfjmD/+8Y8qKSmZ/q9kuZucnFRPT4++9a1v6ac//akefvhhtbe3O12WI+7cuaNoNKq2tjb9/Oc/18TEhDo7O50uK+NyPsD9fr+GhoamXw8NDeXlv0LzlUgk1Nraqm3btqm2ttbpchzV3d2tP/zhDzp8+LBOnz6tP/3pTzpz5ozTZTnG7/fL7/dr/fr1kqStW7eqp6fH4aqc0dXVpccee0zFxcXyeDyqra3VX/7yF6fLyricH6GsXbtWt27dUn9/v3w+n65cuaIf/OAHTpflCNu2de7cOVVUVGjnzp1Ol+O4559/Xs8//7wk6YMPPtDbb7+9bP82JKm0tFR+v1+9vb0qLy9XV1fXsr3AHQgEdOPGDX322WcqLCxUV1eX1q5d63RZGZfzAV5QUKADBw7o9ddfl2VZevrpp1VZWel0WY7o7u5WZ2en1qxZo2PHjkmSGhsbVV1d7XBlyBUHDhzQmTNnlEgk9Nhjj+nQoUNOl+SI9evXa+vWrXr11VdVUFCgr3zlK3n5lXq+Sg8Ahsr5GTgAYG4EOAAYigAHAEMR4ABgKAIcAAxFgAOAoQhwADDU/wNGgTuVy7V4vAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = sim_data()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "ax.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# MNIST手写数据识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis\\\\ML-Action'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里必须要使用 transforms.ToTensor() 转换成 tensor，否则每个图片样本是一个 PIL.Image.Image 的对象\n",
    "# mnist_train = datasets.MNIST(root='./datasets/PyTorch/', train=True, download=True, transform=T.ToTensor())\n",
    "# mnist_test = datasets.MNIST(root='./datasets/PyTorch/', train=False, download=True, transform=T.ToTensor())\n",
    "\n",
    "# 用于将tensor压平的匿名函数\n",
    "flatten = lambda t: torch.flatten(t)\n",
    "# t = torch.tensor([[1,2],[3,4]])\n",
    "# print(t)\n",
    "# print(flatten(t))\n",
    "\n",
    "# 上面得到的每张图像是 28x28 的矩阵，下面将每张图片进行了拉直，也就是转换成一个 28x28=784 的向量\n",
    "mnist_train = datasets.MNIST(root='./datasets/PyTorch/', train=True, download=True, transform=T.Compose([T.ToTensor(),T.Lambda(flatten)]))\n",
    "mnist_test = datasets.MNIST(root='./datasets/PyTorch/', train=False, download=True, transform=T.Compose([T.ToTensor(),T.Lambda(flatten)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.MNIST'>\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mnist_train.__class__)\n",
    "print(mnist_train.classes)\n",
    "\n",
    "# 可以使用下面的语句查看每个样本的类型，这里得到的是一个元组，元组的第一个元素就是对应的第一个图片样本，第二个元素是对应的手写数字值\n",
    "# 如果没有指定 transform参数，图片样本就是PIL.Image.Image对象，进行了 T.ToTensor() 的转换后，就是一个 Tensor\n",
    "# mnist_train[0]\n",
    "# 获取第一个图片样本\n",
    "# mnist_train[0][0]\n",
    "\n",
    "# 检查每张图片样本的shape\n",
    "mnist_train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 这里的.data属性存储的是没有经过变换之前的数值，所以仍然是28x28\n",
    "print(mnist_train.data.__class__)\n",
    "print(mnist_train.data.shape)\n",
    "print(mnist_train.data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([60000])\n",
      "tensor(5)\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train.targets.__class__)\n",
    "print(mnist_train.targets.shape)\n",
    "print(mnist_train.targets[0])\n",
    "print(mnist_test.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN模型\n",
    "\n",
    "这里构建一个 **输入层 + 隐藏层1 + 隐藏层2 + softmax层**，一共4层的神经网络，使用的内容如下：  \n",
    "+ 每一层的激活函数使用ReLU函数\n",
    "+ 损失函数使用L2正则项\n",
    "+ 使用衰减的指数学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDNN(nn.Module):\n",
    "    def __init__(self, input_dim, layer_1_dim, layer_2_dim):\n",
    "        \"\"\"\n",
    "        input_dim指定输入层的节点数\n",
    "        layer_1_dim指定第一个隐藏层的节点数\n",
    "        layer_2_dim指定第二个隐藏层的节点数，这也是softmax层和输出层的节点数\n",
    "        output_dim指定输出层的节点数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, layer_1_dim, bias=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_1_dim, layer_2_dim, bias=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # softmax层不需要指定节点数，它的节点数和上一层一致，只需要指定在哪个维度上进行softmax就行了\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X, train=False):\n",
    "        \"\"\"\n",
    "        输入X的shape是：batch_size * (28*28)\n",
    "        \"\"\"\n",
    "        layer_1_out = self.relu1(self.layer1(X))\n",
    "        layer_2_out = self.relu2(self.layer2(layer_1_out))\n",
    "        # 训练的时候不使用softmax，因为损失函数里会有softmax这一步\n",
    "        if train:\n",
    "            y_pred = layer_2_out\n",
    "        else:\n",
    "            y_pred = self.softmax(layer_2_out)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入层节点数\n",
    "INPUT_NODES = 28*28\n",
    "# 隐藏层1的节点数\n",
    "LAYER_1_NODES = 500\n",
    "# 隐藏层2的节点数，也是输出层节点数\n",
    "LAYER_2_NODES = 10\n",
    "# 每批样本数\n",
    "BATCH_SIZE=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# dataiter = iter(mnist_train_loader)\n",
    "# batch_data, batch_label = next(dataiter)\n",
    "# print(batch_data.__class__)\n",
    "# print(batch_data.shape)\n",
    "# y_pred = mnist_dnn(batch_data)\n",
    "# y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MnistDNN(\n",
      "  (layer1): Linear(in_features=784, out_features=500, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (layer2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 这个模型是定义在CPU上的\n",
    "mnist_dnn = MnistDNN(INPUT_NODES, LAYER_1_NODES, LAYER_2_NODES)\n",
    "print(mnist_dnn)\n",
    "# for param, value in mnist_dnn.named_parameters():\n",
    "#     print(\"param:\", param)\n",
    "#     print(\"value:\", value)\n",
    "\n",
    "# 定义损失函数\n",
    "crossEnt = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=mnist_dnn.parameters(), lr=0.1)\n",
    "\n",
    "# 定义指数衰减的学习率\n",
    "scheluer_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查模型是否在CUDA上\n",
    "# 随便取模型中的一个参数，检查参数的 .is_cuda 属性即可\n",
    "next(mnist_dnn.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- trainning on epoch 1--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 2--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 3--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 4--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 5--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 6--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 7--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 8--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 9--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "----------- trainning on epoch 10--------------------\n",
      "batch num 40, Train Cross-Entropy-Loss is: 2.30243\n",
      "batch num 80, Train Cross-Entropy-Loss is: 2.30245\n",
      "batch num 120, Train Cross-Entropy-Loss is: 2.30299\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 这里是使用CPU训练\n",
    "# 进行训练\n",
    "for epoch in range(1,11):\n",
    "    print(\"----------- trainning on epoch {}--------------------\".format(epoch))\n",
    "    for batch_num, (batch_data, batch_label) in enumerate(mnist_train_loader):\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算预测值\n",
    "        y_pred = mnist_dnn(batch_data)\n",
    "        # 计算损失函数值\n",
    "        loss = crossEnt(y_pred, batch_label)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 使用优化器更新参数\n",
    "        optimizer.step()\n",
    "        # 更新学习率\n",
    "        scheluer_exp.step()\n",
    "\n",
    "        if (batch_num+1) % 40 == 0:\n",
    "            # 计算新的预测值和损失值\n",
    "            y_pred = mnist_dnn(batch_data)\n",
    "            loss = crossEnt(y_pred, batch_label)\n",
    "            print(\"batch num {}, Train Cross-Entropy-Loss is: {:.5f}\".format(batch_num+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取 GPU 设备\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "# 将模型转移到GPU\n",
    "# mnist_dnn = mnist_dnn.cuda()\n",
    "mnist_dnn.to(gpu_device)\n",
    "\n",
    "# 检查模型是否在CUDA上\n",
    "next(mnist_dnn.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "crossEnt = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=mnist_dnn.parameters(), lr=0.1)\n",
    "# 定义指数衰减的学习率\n",
    "scheluer_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- trainning on epoch 1--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30167\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30156\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 2--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 3--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 4--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 5--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 6--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 7--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 8--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 9--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "----------- trainning on epoch 10--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.30159\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.30154\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.30142\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 调用GPU计算\n",
    "for epoch in range(1,11):\n",
    "    print(\"----------- trainning on epoch {}--------------------\".format(epoch))\n",
    "    for batch_num, (batch_data, batch_label) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # 这里每次都将数据转移到GPU\n",
    "        batch_data = batch_data.to(gpu_device)\n",
    "        batch_label = batch_label.to(gpu_device)\n",
    "        \n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算预测值\n",
    "        y_pred = mnist_dnn(batch_data)\n",
    "        # 计算损失函数值\n",
    "        loss = crossEnt(y_pred, batch_label)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 使用优化器更新参数\n",
    "        optimizer.step()\n",
    "        # 更新学习率\n",
    "        scheluer_exp.step()\n",
    "\n",
    "        if (batch_num+1) % 2 == 0:\n",
    "            # 计算新的预测值和损失值\n",
    "            y_pred = mnist_dnn(batch_data)\n",
    "            loss = crossEnt(y_pred, batch_label)\n",
    "            print(\"batch num {}, Train Cross-Entropy-Loss is: {:.5f}\".format(batch_num+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN：LeNet-5模型\n",
    "\n",
    "原始的LeNet-5模型总共有7层（不包含输入层）\n",
    "1. 输入层：32x32x1\n",
    "2. 卷积层：6 个过滤器，步长为 1，不使用0填充，卷积核为 5x5x1x6，输出尺寸为 32-5+1=28，即 28x28x6\n",
    "3. 池化层：窗口大小为 2x2，步长为 2，输出尺寸为 14x14x6\n",
    "4. 卷积层：16 个过滤器，步长为 1，不使用0填充，卷积核为 5x5x6x16，输出尺寸为 14-5+1=10，即 10x10x16\n",
    "5. 池化层：窗口大小为 2x2，步长为 2，输出尺寸为 5x5x6\n",
    "6. 全连接层：输出节点个数为 120\n",
    "7. 全连接层：输出节点个数为 84\n",
    "8. 全连接层；输出节点个数为 10\n",
    "\n",
    "在MNIST数据集上的应用就是：\n",
    "1. 输入层：28x28x1\n",
    "2. 卷积层：32 个过滤器，步长为 1，**使用0填充**，卷积核为 5x5x1x32，输出尺寸为 28x28x32\n",
    "3. 池化层：窗口大小为 2x2，步长为 2，**使用0填充**，输出尺寸为 14x14x32\n",
    "4. 卷积层：64 个过滤器，步长为 1，不使用0填充，卷积核为 5x5x32x64，输出尺寸为 14-5+1=10，即 10x10x64\n",
    "5. 池化层：窗口大小为 2x2，步长为 2，输出尺寸为 5x5x64\n",
    "6. 全连接层：节点个数为 512\n",
    "7. 全连接层：节点个数为 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), stride=1, padding=2, padding_mode='zeros', bias=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(5, 5), stride=1, padding=0, bias=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(in_features=(5*5*64), out_features=512, bias=True)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        layer1 = self.pool1( self.conv1(X) )\n",
    "        layer2 = self.pool2( self.conv2(layer1) )\n",
    "        # 需要将layer2输出的 5x5x64 的矩阵拉直成一个向量\n",
    "        layer2 = self.flatten(layer2)\n",
    "        layer3 = self.relu1( self.linear1(layer2) )\n",
    "        layer4 = self.relu2( self.linear2(layer3) )\n",
    "        return layer4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面得到的每张图像是 28x28 的矩阵，下面将每张图片进行了拉直，也就是转换成一个 28x28=784 的向量\n",
    "mnist_train = datasets.MNIST(root='./datasets/PyTorch/', train=True, download=True, transform=T.ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./datasets/PyTorch/', train=False, download=True, transform=T.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten()\n",
      "  (linear1): Linear(in_features=1600, out_features=512, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 这个模型是定义在CPU上的\n",
    "mnist_cnn = LeNet5()\n",
    "print(mnist_cnn)\n",
    "# for param, value in mnist_cnn.named_parameters():\n",
    "#     print(\"param:\", param)\n",
    "#     print(\"value:\", value)\n",
    "\n",
    "# 定义损失函数\n",
    "crossEnt = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=mnist_cnn.parameters(), lr=0.1)\n",
    "\n",
    "# 定义指数衰减的学习率\n",
    "scheluer_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查模型是否在CUDA上\n",
    "# 随便取模型中的一个参数，检查参数的 .is_cuda 属性即可\n",
    "next(mnist_dnn.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- trainning on epoch 1--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 2--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 3--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 4--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 5--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 6--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 7--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 8--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 9--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "----------- trainning on epoch 10--------------------\n",
      "batch num 200, Train Cross-Entropy-Loss is: 2.28877\n",
      "batch num 400, Train Cross-Entropy-Loss is: 2.28252\n",
      "batch num 600, Train Cross-Entropy-Loss is: 2.28452\n",
      "Wall time: 7min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 这里是使用CPU训练\n",
    "for epoch in range(1,11):\n",
    "    print(\"----------- trainning on epoch {}--------------------\".format(epoch))\n",
    "    for batch_num, (batch_data, batch_label) in enumerate(mnist_train_loader):\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算预测值\n",
    "        y_pred = mnist_cnn(batch_data)\n",
    "        # 计算损失函数值\n",
    "        loss = crossEnt(y_pred, batch_label)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 使用优化器更新参数\n",
    "        optimizer.step()\n",
    "        # 更新学习率\n",
    "        scheluer_exp.step()\n",
    "\n",
    "        if (batch_num+1) % 200 == 0:\n",
    "            # 计算新的预测值和损失值\n",
    "            y_pred = mnist_cnn(batch_data)\n",
    "            loss = crossEnt(y_pred, batch_label)\n",
    "            print(\"batch num {}, Train Cross-Entropy-Loss is: {:.5f}\".format(batch_num+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet5(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten()\n",
      "  (linear1): Linear(in_features=1600, out_features=512, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (relu2): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 这个模型是定义在CPU上的\n",
    "mnist_cnn = LeNet5()\n",
    "print(mnist_cnn)\n",
    "# for param, value in mnist_cnn.named_parameters():\n",
    "#     print(\"param:\", param)\n",
    "#     print(\"value:\", value)\n",
    "\n",
    "# 定义损失函数\n",
    "crossEnt = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=mnist_cnn.parameters(), lr=0.1)\n",
    "\n",
    "# 定义指数衰减的学习率\n",
    "scheluer_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取 GPU 设备\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "# 将模型转移到GPU\n",
    "# mnist_dnn = mnist_dnn.cuda()\n",
    "mnist_cnn.to(gpu_device)\n",
    "\n",
    "# 检查模型是否在CUDA上\n",
    "next(mnist_cnn.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- trainning on epoch 1--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29435\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29463\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29405\n",
      "----------- trainning on epoch 2--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 3--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 4--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 5--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 6--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 7--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 8--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 9--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "----------- trainning on epoch 10--------------------\n",
      "batch num 2, Train Cross-Entropy-Loss is: 2.29418\n",
      "batch num 4, Train Cross-Entropy-Loss is: 2.29459\n",
      "batch num 6, Train Cross-Entropy-Loss is: 2.29404\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 调用GPU计算\n",
    "for epoch in range(1,11):\n",
    "    print(\"----------- trainning on epoch {}--------------------\".format(epoch))\n",
    "    for batch_num, (batch_data, batch_label) in enumerate(mnist_train_loader):\n",
    "        \n",
    "        # 这里每次都将数据转移到GPU\n",
    "        batch_data = batch_data.to(gpu_device)\n",
    "        batch_label = batch_label.to(gpu_device)\n",
    "        \n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算预测值\n",
    "        y_pred = mnist_cnn(batch_data)\n",
    "        # 计算损失函数值\n",
    "        loss = crossEnt(y_pred, batch_label)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 使用优化器更新参数\n",
    "        optimizer.step()\n",
    "        # 更新学习率\n",
    "        scheluer_exp.step()\n",
    "\n",
    "        if (batch_num+1) % 2 == 0:\n",
    "            # 计算新的预测值和损失值\n",
    "            y_pred = mnist_cnn(batch_data)\n",
    "            loss = crossEnt(y_pred, batch_label)\n",
    "            print(\"batch num {}, Train Cross-Entropy-Loss is: {:.5f}\".format(batch_num+1, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# RNN练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN拟合$sin$函数序列\n",
    "这里展示一个使用RNN拟合$sin$函数的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10, 1])\n",
      "torch.Size([10000, 1])\n",
      "torch.Size([1000, 10, 1])\n",
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "TRAINING_EXAMPLES = 10000\n",
    "TESTING_EXAMPLES = 1000\n",
    "SAMPLE_INTERVAL = 0.01*np.pi\n",
    "TIME_STEPS = 10  # 序列的长度\n",
    "\n",
    "# 构造数据集\n",
    "# 需要注意的是，RNN输入数据的shape=(batch_size, time_steps, input_size)，特别要注意 time_steps 这个维度.\n",
    "def generate_data(seq):\n",
    "    \"\"\"\n",
    "    从序列seq中进行采样，用 [第i项: TIME_STEPS-1项]作为输入值X， 第 i+TIMES_STEPS 项作为Y值\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    # 序列的第 i 项和后面的 TIME_STEPS-1 项合在一起作为输入；第 i + TIME_STEPS 项作为输出。\n",
    "    # 即用sin函数前面的 TIME_STEPS 个点的信息，预测第 i + TIME_STEPS 个点的函数值。\n",
    "    # 样本数为 序列长度 - TIME_STEPS\n",
    "    for i in range(len(seq)-TIME_STEPS):\n",
    "        # X.append([seq[i: (i+TIME_STEPS)]])\n",
    "        # Y.append([seq[i+TIME_STEPS]])\n",
    "        X.append(seq[i: (i+TIME_STEPS)])\n",
    "        Y.append(seq[i+TIME_STEPS])\n",
    "    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
    "\n",
    "# 确定测试数据起点，这之前都是训练数据集：训练数据集的取值区间是 [0, test_start]，这中间取 TRAINING_EXAMPLES + TIME_STEPS 个点，\n",
    "test_start = (TRAINING_EXAMPLES + TIME_STEPS) * SAMPLE_INTERVAL\n",
    "# 确定测试数据集的终点：测试数据集的取值区间是 [test_start, test_end]， 这中间取 TESTING_EXAMPLES + TIME_STEPS 个点\n",
    "test_end = test_start + (TESTING_EXAMPLES + TIME_STEPS) * SAMPLE_INTERVAL\n",
    "\n",
    "sin_x_train = np.linspace(0, test_start, TRAINING_EXAMPLES + TIME_STEPS, dtype=np.float32)\n",
    "sin_y_train = np.sin(sin_x_train)\n",
    "sin_x_test = np.linspace(test_start, test_end, TESTING_EXAMPLES + TIME_STEPS, dtype=np.float32)\n",
    "sin_y_test = np.sin(sin_x_test)\n",
    "\n",
    "# 注意，这里是要用sin值序列的 第i项: TIME_STEPS-1 项 预测 第 i+TIMES_STEPS 项，所以只需要 sin_y_train值，不需要横坐标的值\n",
    "train_X, train_y = generate_data(sin_y_train)\n",
    "test_X, test_y = generate_data(sin_y_test)\n",
    "\n",
    "# 修改一下数据的 shape\n",
    "train_X = train_X.reshape(TRAINING_EXAMPLES, TIME_STEPS, 1)\n",
    "train_y = train_y.reshape(TRAINING_EXAMPLES, 1)\n",
    "test_X = test_X.reshape(TESTING_EXAMPLES, TIME_STEPS, 1)\n",
    "test_y = test_y.reshape(TESTING_EXAMPLES, 1)\n",
    "\n",
    "train_X = torch.tensor(train_X)\n",
    "train_y = torch.tensor(train_y)\n",
    "test_X = torch.tensor(test_X)\n",
    "test_y = torch.tensor(test_y)\n",
    "\n",
    "# 一共10000个样本，每个训练样本是一个长度(time_step)=10的 sequence，sequence中每个位置的值是一个长度=1的向量（也就是标量）\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 10, 5])\n",
      "torch.Size([1, 10000, 5])\n"
     ]
    }
   ],
   "source": [
    "# 测试一下数据和RNN\n",
    "# input_size=1，隐藏层是 长度=5 的向量，单层、单向 RNN\n",
    "rnn = nn.RNN(input_size=1, hidden_size=5, num_layers=1, bidirectional=False, nonlinearity='relu', bias=False, batch_first=True)\n",
    "\n",
    "res = rnn(train_X)\n",
    "# 输出层\n",
    "print(res[0].shape)\n",
    "# 隐藏层（状态层）\n",
    "print(res[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnSin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 单层、单向RNN，隐藏层向量维度 = 5\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=5, num_layers=1, nonlinearity='relu', bias=False, batch_first=True, bidirectional=False)\n",
    "        # 将RNN的输出拉平\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 最后用一个全连接层，将结果转成一个实数\n",
    "        self.linear = nn.Linear(in_features=50, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        rnn_out = self.rnn(X)\n",
    "        rnn_out_flat = self.flatten(rnn_out[0])\n",
    "        out = self.linear(rnn_out_flat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 ---- training MSE-Loss  is：0.3923\n",
      "epoch  1 ---- testing  MSE-Loss  is：0.3922\n",
      "epoch  2 ---- training MSE-Loss  is：0.3347\n",
      "epoch  2 ---- testing  MSE-Loss  is：0.3347\n",
      "epoch  3 ---- training MSE-Loss  is：0.2766\n",
      "epoch  3 ---- testing  MSE-Loss  is：0.2765\n",
      "epoch  4 ---- training MSE-Loss  is：0.2257\n",
      "epoch  4 ---- testing  MSE-Loss  is：0.2256\n",
      "epoch  5 ---- training MSE-Loss  is：0.1871\n",
      "epoch  5 ---- testing  MSE-Loss  is：0.1870\n",
      "epoch  6 ---- training MSE-Loss  is：0.1597\n",
      "epoch  6 ---- testing  MSE-Loss  is：0.1597\n",
      "epoch  7 ---- training MSE-Loss  is：0.1391\n",
      "epoch  7 ---- testing  MSE-Loss  is：0.1390\n",
      "epoch  8 ---- training MSE-Loss  is：0.1224\n",
      "epoch  8 ---- testing  MSE-Loss  is：0.1224\n",
      "epoch  9 ---- training MSE-Loss  is：0.1089\n",
      "epoch  9 ---- testing  MSE-Loss  is：0.1089\n",
      "epoch 10 ---- training MSE-Loss  is：0.0977\n",
      "epoch 10 ---- testing  MSE-Loss  is：0.0977\n",
      "epoch 11 ---- training MSE-Loss  is：0.0885\n",
      "epoch 11 ---- testing  MSE-Loss  is：0.0885\n",
      "epoch 12 ---- training MSE-Loss  is：0.0808\n",
      "epoch 12 ---- testing  MSE-Loss  is：0.0809\n",
      "epoch 13 ---- training MSE-Loss  is：0.0745\n",
      "epoch 13 ---- testing  MSE-Loss  is：0.0745\n",
      "epoch 14 ---- training MSE-Loss  is：0.0692\n",
      "epoch 14 ---- testing  MSE-Loss  is：0.0692\n",
      "epoch 15 ---- training MSE-Loss  is：0.0647\n",
      "epoch 15 ---- testing  MSE-Loss  is：0.0647\n",
      "epoch 16 ---- training MSE-Loss  is：0.0609\n",
      "epoch 16 ---- testing  MSE-Loss  is：0.0609\n",
      "epoch 17 ---- training MSE-Loss  is：0.0576\n",
      "epoch 17 ---- testing  MSE-Loss  is：0.0577\n",
      "epoch 18 ---- training MSE-Loss  is：0.0548\n",
      "epoch 18 ---- testing  MSE-Loss  is：0.0548\n",
      "epoch 19 ---- training MSE-Loss  is：0.0523\n",
      "epoch 19 ---- testing  MSE-Loss  is：0.0524\n",
      "epoch 20 ---- training MSE-Loss  is：0.0501\n",
      "epoch 20 ---- testing  MSE-Loss  is：0.0502\n",
      "epoch 21 ---- training MSE-Loss  is：0.0481\n",
      "epoch 21 ---- testing  MSE-Loss  is：0.0482\n",
      "epoch 22 ---- training MSE-Loss  is：0.0463\n",
      "epoch 22 ---- testing  MSE-Loss  is：0.0464\n",
      "epoch 23 ---- training MSE-Loss  is：0.0448\n",
      "epoch 23 ---- testing  MSE-Loss  is：0.0448\n",
      "epoch 24 ---- training MSE-Loss  is：0.0433\n",
      "epoch 24 ---- testing  MSE-Loss  is：0.0434\n",
      "epoch 25 ---- training MSE-Loss  is：0.0420\n",
      "epoch 25 ---- testing  MSE-Loss  is：0.0421\n",
      "epoch 26 ---- training MSE-Loss  is：0.0408\n",
      "epoch 26 ---- testing  MSE-Loss  is：0.0409\n",
      "epoch 27 ---- training MSE-Loss  is：0.0397\n",
      "epoch 27 ---- testing  MSE-Loss  is：0.0397\n",
      "epoch 28 ---- training MSE-Loss  is：0.0386\n",
      "epoch 28 ---- testing  MSE-Loss  is：0.0387\n",
      "epoch 29 ---- training MSE-Loss  is：0.0376\n",
      "epoch 29 ---- testing  MSE-Loss  is：0.0376\n",
      "epoch 30 ---- training MSE-Loss  is：0.0366\n",
      "epoch 30 ---- testing  MSE-Loss  is：0.0366\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "sin_rnn = RnnSin()\n",
    "# 定义损失函数\n",
    "mse = nn.MSELoss()\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=sin_rnn.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    # 获取预测值\n",
    "    sin_pred = sin_rnn(train_X)\n",
    "    # sin_pred.shape\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 计算损失函数值\n",
    "    loss = mse(sin_pred, train_y)\n",
    "    # 计算梯度\n",
    "    loss.backward()\n",
    "    # 使用优化器更新参数\n",
    "    optimizer.step()\n",
    "    # 新的MSE损失\n",
    "    train_pred_y = sin_rnn(train_X)\n",
    "    train_loss = mse(train_pred_y, train_y)\n",
    "    test_pred_y = sin_rnn(test_X)\n",
    "    test_loss = mse(test_pred_y, test_y)\n",
    "    # print(loss)\n",
    "    print(\"epoch {:2} ---- training MSE-Loss  is：{:.4f}\".format(epoch, train_loss))\n",
    "    print(\"epoch {:2} ---- testing  MSE-Loss  is：{:.4f}\".format(epoch, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB影评数据集分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, filepath, part=None):\n",
    "        super(ImdbDataset, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.__nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.doc_vector, self.label = self.transform(filepath, part)\n",
    "\n",
    "    def transform(self, filepath, part):\n",
    "        text_data = load_files(filepath)\n",
    "        text_list, label = text_data.data, text_data.target\n",
    "        if part is not None:\n",
    "            text_list = text_list[:part]\n",
    "            label = label[:part]\n",
    "        text = [doc.decode(\"utf-8\").replace(\"<br />\", \" \") for doc in text_list]\n",
    "        sentence_vec = [self.sentence2vector(self.__nlp(doc)) for doc in text]\n",
    "        # 上面得到的 sentence_vec 中，每个句子的长度都不一样，需要选择最短的句子做截断处理-----这个处理的不好\n",
    "        # seq_length = min([sentence.shape[0] for sentence in sentence_vec])\n",
    "        # sentence_vec_trunc = [sentence[:seq_length, :] for sentence in sentence_vec]\n",
    "        # doc_len = [sentence.shape for sentence in sentence_vec_trunc]\n",
    "        # print(\"seq_length\", seq_length)\n",
    "        # print(\"doc_len\", doc_len)\n",
    "        # doc_vector = np.array(sentence_vec_truc)\n",
    "        \n",
    "        # 选择句子的平均长度，不足的进行padding\n",
    "        seq_length = int(np.mean(([sentence.shape[0] for sentence in sentence_vec])))\n",
    "        sentence_vec_pad = [self.sentence_vector_padding(sentence, seq_length) for sentence in sentence_vec]\n",
    "        doc_vector = np.array(sentence_vec_pad)\n",
    "        return doc_vector, label\n",
    "\n",
    "    def sentence2vector(self, doc):\n",
    "        \"\"\"\n",
    "        用于从 sentence 中过滤出有用的word\n",
    "        @param sentence: spacy的doc对象，封装了一个句子\n",
    "        @return: 句子中有效单词所组成的 词向量矩阵,\n",
    "        \"\"\"\n",
    "        doc_matrix = doc.tensor\n",
    "        # 接下来要从上面剔除掉 标点符号 等无效部分的词向量\n",
    "        index_list = []\n",
    "        for token in doc:\n",
    "            if token.is_stop or token.is_punct or token.is_currency:\n",
    "                continue\n",
    "            else:\n",
    "                index_list.append(token.i)\n",
    "        result = doc_matrix[index_list, :]\n",
    "        # print(result.shape)\n",
    "        return result\n",
    "\n",
    "    def sentence_vector_padding(self, sen_vec, seq_length):\n",
    "        \"\"\"\n",
    "        对每个句子对应的词向量矩阵进行填充，保证句子长度一样\n",
    "        \"\"\"\n",
    "        if sen_vec.shape[0] >= seq_length:\n",
    "            return sen_vec[:seq_length, :]\n",
    "        else:\n",
    "            pad_len = int(seq_length-sen_vec.shape[0])\n",
    "            sen_vec_pad = np.pad(array=sen_vec, pad_width=((0,pad_len),(0,0)), mode='constant', constant_values=0)\n",
    "            return sen_vec_pad\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.doc_vector.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.doc_vector[item, :, :], self.label[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "imdb_train = ImdbDataset(filepath='./datasets/aclImdb/train/', part=2000)\n",
    "imdb_test = ImdbDataset(filepath='./datasets/aclImdb/test/', part=200)\n",
    "\n",
    "doc_vector = imdb_train.doc_vector\n",
    "print(doc_vector.shape)\n",
    "doc_label = imdb_train.label\n",
    "print(doc_label.shape)\n",
    "\n",
    "# 构建训练数据集的加载器\n",
    "batch_size = 200\n",
    "imdb_train_loader = DataLoader(dataset=imdb_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([200, 107, 96])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "iter_batch = iter(imdb_train_loader)\n",
    "X, y = next(iter_batch)\n",
    "print(X.__class__)\n",
    "print(X.shape)\n",
    "print(y.__class__)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, seq_length, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm_layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(in_features=seq_length*hidden_size, out_features=2, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        lstm_out = self.lstm_layer(X)\n",
    "        lstm_flatten = self.flatten(lstm_out[0])\n",
    "        y_pred = self.linear(lstm_flatten)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "lstm_rnn = ImdbLSTM(input_size=96, hidden_size=48, seq_length=107, num_layers=1)\n",
    "# 定义损失函数\n",
    "crossEnt = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=lstm_rnn.parameters(), lr=0.1)\n",
    "\n",
    "# 测试\n",
    "# out = lstm_rnn(X)\n",
    "# loss = crossEnt(out, y.long())\n",
    "\n",
    "# 测试数据集，此时是放在内存中供CPU使用的\n",
    "test_data, test_label = imdb_test.doc_vector, imdb_test.label\n",
    "test_data = torch.tensor(test_data)\n",
    "test_label = torch.tensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, batch  2, train_loss: 0.6756\n",
      "epoch  1, batch  2, test_loss:  0.7166\n",
      "epoch  1, batch  4, train_loss: 0.6747\n",
      "epoch  1, batch  4, test_loss:  0.6938\n",
      "epoch  1, batch  6, train_loss: 0.6806\n",
      "epoch  1, batch  6, test_loss:  0.6999\n",
      "epoch  1, batch  8, train_loss: 0.6725\n",
      "epoch  1, batch  8, test_loss:  0.6916\n",
      "epoch  1, batch 10, train_loss: 0.6630\n",
      "epoch  1, batch 10, test_loss:  0.6985\n",
      "epoch  2, batch  2, train_loss: 0.6616\n",
      "epoch  2, batch  2, test_loss:  0.7105\n",
      "epoch  2, batch  4, train_loss: 0.6572\n",
      "epoch  2, batch  4, test_loss:  0.6913\n",
      "epoch  2, batch  6, train_loss: 0.6590\n",
      "epoch  2, batch  6, test_loss:  0.6925\n",
      "epoch  2, batch  8, train_loss: 0.6521\n",
      "epoch  2, batch  8, test_loss:  0.6889\n",
      "epoch  2, batch 10, train_loss: 0.6431\n",
      "epoch  2, batch 10, test_loss:  0.6967\n",
      "epoch  3, batch  2, train_loss: 0.6464\n",
      "epoch  3, batch  2, test_loss:  0.7009\n",
      "epoch  3, batch  4, train_loss: 0.6411\n",
      "epoch  3, batch  4, test_loss:  0.6896\n",
      "epoch  3, batch  6, train_loss: 0.6410\n",
      "epoch  3, batch  6, test_loss:  0.6889\n",
      "epoch  3, batch  8, train_loss: 0.6349\n",
      "epoch  3, batch  8, test_loss:  0.6874\n",
      "epoch  3, batch 10, train_loss: 0.6254\n",
      "epoch  3, batch 10, test_loss:  0.6944\n",
      "epoch  4, batch  2, train_loss: 0.6333\n",
      "epoch  4, batch  2, test_loss:  0.6954\n",
      "epoch  4, batch  4, train_loss: 0.6260\n",
      "epoch  4, batch  4, test_loss:  0.6882\n",
      "epoch  4, batch  6, train_loss: 0.6247\n",
      "epoch  4, batch  6, test_loss:  0.6868\n",
      "epoch  4, batch  8, train_loss: 0.6193\n",
      "epoch  4, batch  8, test_loss:  0.6864\n",
      "epoch  4, batch 10, train_loss: 0.6093\n",
      "epoch  4, batch 10, test_loss:  0.6925\n",
      "epoch  5, batch  2, train_loss: 0.6211\n",
      "epoch  5, batch  2, test_loss:  0.6920\n",
      "epoch  5, batch  4, train_loss: 0.6116\n",
      "epoch  5, batch  4, test_loss:  0.6870\n",
      "epoch  5, batch  6, train_loss: 0.6095\n",
      "epoch  5, batch  6, test_loss:  0.6855\n",
      "epoch  5, batch  8, train_loss: 0.6049\n",
      "epoch  5, batch  8, test_loss:  0.6857\n",
      "epoch  5, batch 10, train_loss: 0.5945\n",
      "epoch  5, batch 10, test_loss:  0.6910\n",
      "epoch  6, batch  2, train_loss: 0.6094\n",
      "epoch  6, batch  2, test_loss:  0.6897\n",
      "epoch  6, batch  4, train_loss: 0.5979\n",
      "epoch  6, batch  4, test_loss:  0.6862\n",
      "epoch  6, batch  6, train_loss: 0.5953\n",
      "epoch  6, batch  6, test_loss:  0.6846\n",
      "epoch  6, batch  8, train_loss: 0.5915\n",
      "epoch  6, batch  8, test_loss:  0.6853\n",
      "epoch  6, batch 10, train_loss: 0.5806\n",
      "epoch  6, batch 10, test_loss:  0.6900\n",
      "epoch  7, batch  2, train_loss: 0.5981\n",
      "epoch  7, batch  2, test_loss:  0.6882\n",
      "epoch  7, batch  4, train_loss: 0.5849\n",
      "epoch  7, batch  4, test_loss:  0.6857\n",
      "epoch  7, batch  6, train_loss: 0.5817\n",
      "epoch  7, batch  6, test_loss:  0.6842\n",
      "epoch  7, batch  8, train_loss: 0.5788\n",
      "epoch  7, batch  8, test_loss:  0.6851\n",
      "epoch  7, batch 10, train_loss: 0.5676\n",
      "epoch  7, batch 10, test_loss:  0.6893\n",
      "epoch  8, batch  2, train_loss: 0.5870\n",
      "epoch  8, batch  2, test_loss:  0.6872\n",
      "epoch  8, batch  4, train_loss: 0.5723\n",
      "epoch  8, batch  4, test_loss:  0.6855\n",
      "epoch  8, batch  6, train_loss: 0.5688\n",
      "epoch  8, batch  6, test_loss:  0.6840\n",
      "epoch  8, batch  8, train_loss: 0.5668\n",
      "epoch  8, batch  8, test_loss:  0.6852\n",
      "epoch  8, batch 10, train_loss: 0.5553\n",
      "epoch  8, batch 10, test_loss:  0.6890\n",
      "epoch  9, batch  2, train_loss: 0.5760\n",
      "epoch  9, batch  2, test_loss:  0.6866\n",
      "epoch  9, batch  4, train_loss: 0.5602\n",
      "epoch  9, batch  4, test_loss:  0.6855\n",
      "epoch  9, batch  6, train_loss: 0.5563\n",
      "epoch  9, batch  6, test_loss:  0.6841\n",
      "epoch  9, batch  8, train_loss: 0.5554\n",
      "epoch  9, batch  8, test_loss:  0.6856\n",
      "epoch  9, batch 10, train_loss: 0.5436\n",
      "epoch  9, batch 10, test_loss:  0.6890\n",
      "epoch 10, batch  2, train_loss: 0.5652\n",
      "epoch 10, batch  2, test_loss:  0.6863\n",
      "epoch 10, batch  4, train_loss: 0.5486\n",
      "epoch 10, batch  4, test_loss:  0.6856\n",
      "epoch 10, batch  6, train_loss: 0.5443\n",
      "epoch 10, batch  6, test_loss:  0.6845\n",
      "epoch 10, batch  8, train_loss: 0.5446\n",
      "epoch 10, batch  8, test_loss:  0.6861\n",
      "epoch 10, batch 10, train_loss: 0.5324\n",
      "epoch 10, batch 10, test_loss:  0.6892\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 开始迭代训练\n",
    "for epoch in range(1,11):\n",
    "    for batch_id, (X, y) in enumerate(imdb_train_loader):\n",
    "        y_pred = lstm_rnn(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = crossEnt(y_pred, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_new = lstm_rnn(X)\n",
    "        train_loss = crossEnt(y_pred_new, y.long())\n",
    "        y_test = lstm_rnn(test_data)\n",
    "        test_loss = crossEnt(y_test, test_label.long())\n",
    "        \n",
    "        if (batch_id+1) % 2 == 0:\n",
    "            print(\"epoch {:2}, batch {:2}, train_loss: {:.4f}\".format(epoch, batch_id+1, train_loss))\n",
    "            print(\"epoch {:2}, batch {:2}, test_loss:  {:.4f}\".format(epoch, batch_id+1, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化模型\n",
    "lstm_rnn = ImdbLSTM(input_size=96, hidden_size=48, seq_length=107, num_layers=1)\n",
    "# 定义损失函数\n",
    "crossEnt = nn.CrossEntropyLoss()\n",
    "# 定义优化器\n",
    "optimizer = optim.SGD(params=lstm_rnn.parameters(), lr=0.1)\n",
    "\n",
    "# 获取 GPU 设备\n",
    "gpu_device = torch.device(\"cuda:0\")\n",
    "# 将模型转移到GPU\n",
    "lstm_rnn.to(gpu_device)\n",
    "\n",
    "# 注意，测试数据也要放到GPU上\n",
    "test_data = test_data.to(gpu_device)\n",
    "test_label = test_label.to(gpu_device)\n",
    "\n",
    "# 检查模型是否在CUDA上\n",
    "next(lstm_rnn.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, batch  2, train_loss: 0.6741\n",
      "epoch  1, batch  2, test_loss:  0.7017\n",
      "epoch  1, batch  4, train_loss: 0.6768\n",
      "epoch  1, batch  4, test_loss:  0.6898\n",
      "epoch  1, batch  6, train_loss: 0.6704\n",
      "epoch  1, batch  6, test_loss:  0.6894\n",
      "epoch  1, batch  8, train_loss: 0.6667\n",
      "epoch  1, batch  8, test_loss:  0.6890\n",
      "epoch  1, batch 10, train_loss: 0.6708\n",
      "epoch  1, batch 10, test_loss:  0.6969\n",
      "epoch  2, batch  2, train_loss: 0.6579\n",
      "epoch  2, batch  2, test_loss:  0.6974\n",
      "epoch  2, batch  4, train_loss: 0.6600\n",
      "epoch  2, batch  4, test_loss:  0.6874\n",
      "epoch  2, batch  6, train_loss: 0.6513\n",
      "epoch  2, batch  6, test_loss:  0.6870\n",
      "epoch  2, batch  8, train_loss: 0.6474\n",
      "epoch  2, batch  8, test_loss:  0.6872\n",
      "epoch  2, batch 10, train_loss: 0.6515\n",
      "epoch  2, batch 10, test_loss:  0.6940\n",
      "epoch  3, batch  2, train_loss: 0.6431\n",
      "epoch  3, batch  2, test_loss:  0.6947\n",
      "epoch  3, batch  4, train_loss: 0.6446\n",
      "epoch  3, batch  4, test_loss:  0.6858\n",
      "epoch  3, batch  6, train_loss: 0.6340\n",
      "epoch  3, batch  6, test_loss:  0.6854\n",
      "epoch  3, batch  8, train_loss: 0.6301\n",
      "epoch  3, batch  8, test_loss:  0.6859\n",
      "epoch  3, batch 10, train_loss: 0.6342\n",
      "epoch  3, batch 10, test_loss:  0.6920\n",
      "epoch  4, batch  2, train_loss: 0.6295\n",
      "epoch  4, batch  2, test_loss:  0.6929\n",
      "epoch  4, batch  4, train_loss: 0.6301\n",
      "epoch  4, batch  4, test_loss:  0.6847\n",
      "epoch  4, batch  6, train_loss: 0.6180\n",
      "epoch  4, batch  6, test_loss:  0.6843\n",
      "epoch  4, batch  8, train_loss: 0.6142\n",
      "epoch  4, batch  8, test_loss:  0.6850\n",
      "epoch  4, batch 10, train_loss: 0.6184\n",
      "epoch  4, batch 10, test_loss:  0.6904\n",
      "epoch  5, batch  2, train_loss: 0.6165\n",
      "epoch  5, batch  2, test_loss:  0.6917\n",
      "epoch  5, batch  4, train_loss: 0.6164\n",
      "epoch  5, batch  4, test_loss:  0.6839\n",
      "epoch  5, batch  6, train_loss: 0.6030\n",
      "epoch  5, batch  6, test_loss:  0.6836\n",
      "epoch  5, batch  8, train_loss: 0.5994\n",
      "epoch  5, batch  8, test_loss:  0.6845\n",
      "epoch  5, batch 10, train_loss: 0.6037\n",
      "epoch  5, batch 10, test_loss:  0.6892\n",
      "epoch  6, batch  2, train_loss: 0.6041\n",
      "epoch  6, batch  2, test_loss:  0.6908\n",
      "epoch  6, batch  4, train_loss: 0.6034\n",
      "epoch  6, batch  4, test_loss:  0.6835\n",
      "epoch  6, batch  6, train_loss: 0.5890\n",
      "epoch  6, batch  6, test_loss:  0.6831\n",
      "epoch  6, batch  8, train_loss: 0.5856\n",
      "epoch  6, batch  8, test_loss:  0.6841\n",
      "epoch  6, batch 10, train_loss: 0.5899\n",
      "epoch  6, batch 10, test_loss:  0.6883\n",
      "epoch  7, batch  2, train_loss: 0.5921\n",
      "epoch  7, batch  2, test_loss:  0.6901\n",
      "epoch  7, batch  4, train_loss: 0.5908\n",
      "epoch  7, batch  4, test_loss:  0.6833\n",
      "epoch  7, batch  6, train_loss: 0.5757\n",
      "epoch  7, batch  6, test_loss:  0.6829\n",
      "epoch  7, batch  8, train_loss: 0.5726\n",
      "epoch  7, batch  8, test_loss:  0.6840\n",
      "epoch  7, batch 10, train_loss: 0.5769\n",
      "epoch  7, batch 10, test_loss:  0.6877\n",
      "epoch  8, batch  2, train_loss: 0.5804\n",
      "epoch  8, batch  2, test_loss:  0.6895\n",
      "epoch  8, batch  4, train_loss: 0.5787\n",
      "epoch  8, batch  4, test_loss:  0.6832\n",
      "epoch  8, batch  6, train_loss: 0.5632\n",
      "epoch  8, batch  6, test_loss:  0.6828\n",
      "epoch  8, batch  8, train_loss: 0.5602\n",
      "epoch  8, batch  8, test_loss:  0.6841\n",
      "epoch  8, batch 10, train_loss: 0.5646\n",
      "epoch  8, batch 10, test_loss:  0.6873\n",
      "epoch  9, batch  2, train_loss: 0.5690\n",
      "epoch  9, batch  2, test_loss:  0.6891\n",
      "epoch  9, batch  4, train_loss: 0.5671\n",
      "epoch  9, batch  4, test_loss:  0.6834\n",
      "epoch  9, batch  6, train_loss: 0.5513\n",
      "epoch  9, batch  6, test_loss:  0.6830\n",
      "epoch  9, batch  8, train_loss: 0.5484\n",
      "epoch  9, batch  8, test_loss:  0.6843\n",
      "epoch  9, batch 10, train_loss: 0.5528\n",
      "epoch  9, batch 10, test_loss:  0.6872\n",
      "epoch 10, batch  2, train_loss: 0.5578\n",
      "epoch 10, batch  2, test_loss:  0.6888\n",
      "epoch 10, batch  4, train_loss: 0.5558\n",
      "epoch 10, batch  4, test_loss:  0.6838\n",
      "epoch 10, batch  6, train_loss: 0.5400\n",
      "epoch 10, batch  6, test_loss:  0.6833\n",
      "epoch 10, batch  8, train_loss: 0.5372\n",
      "epoch 10, batch  8, test_loss:  0.6848\n",
      "epoch 10, batch 10, train_loss: 0.5415\n",
      "epoch 10, batch 10, test_loss:  0.6873\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 开始迭代训练\n",
    "for epoch in range(1,11):\n",
    "    for batch_id, (X, y) in enumerate(imdb_train_loader):\n",
    "        \n",
    "        # 将数据转移到GPU\n",
    "        X = X.to(gpu_device)\n",
    "        y = y.to(gpu_device)\n",
    "        \n",
    "        y_pred = lstm_rnn(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = crossEnt(y_pred, y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_new = lstm_rnn(X)\n",
    "        train_loss = crossEnt(y_pred_new, y.long())\n",
    "        y_test = lstm_rnn(test_data)\n",
    "        test_loss = crossEnt(y_test, test_label.long())\n",
    "        \n",
    "        if (batch_id+1) % 2 == 0:\n",
    "            print(\"epoch {:2}, batch {:2}, train_loss: {:.4f}\".format(epoch, batch_id+1, train_loss))\n",
    "            print(\"epoch {:2}, batch {:2}, test_loss:  {:.4f}\".format(epoch, batch_id+1, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
