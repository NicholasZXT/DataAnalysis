{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.19.2\n",
      "pandas:      1.2.2\n",
      "matplotlib:  3.3.4\n",
      "sklearn:     0.23.2\n",
      "seaborn:     0.11.1\n",
      "plotly:      4.14.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly import graph_objects as go\n",
    "\n",
    "import matplotlib\n",
    "import plotly\n",
    "import sklearn\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "from time import time\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"pandas:     \", pd.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"seaborn:    \", sns.__version__)\n",
    "print(\"plotly:     \", plotly.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "\n",
    "文本处理是属于NLP下的一个范畴，涉及面很广，这里只关注其中的两类任务：\n",
    "1. 文本聚类\n",
    "2. 文本分类\n",
    "\n",
    "上述两类任务都需要对文本数据进行处理以提取特征，以下介绍文本处理过程中的一些通用流程和概念。\n",
    "\n",
    "在文本分析的语境中，数据集通常被称为语料库（corpus），每个由单个文本表示的数据点被称为文档（document）。\n",
    "\n",
    "一般有两种表示法：\n",
    "+ 词袋表示法  \n",
    "舍弃输入文本中的大部分结构，如章节、段落、句子和格式，**只计算语料库中每个单词在每个文本中的出现频次** .  \n",
    "通常计算步骤分为如下 3 个：\n",
    "  1. 分词（tokenization）.将每个文档划分为出现在其中的单词（称为词例token），比如按空格和标点划分.\n",
    "  2. 构建词表（vocabulary building）。收集一个词表，里面包含出现在任意文档中的所有词，并对它们进行编号（比如按字母顺序排序）.\n",
    "  3. 编码（encoding）。对于每个文档，计算词表中每个单词在该文档中的出现频次.\n",
    "  \n",
    "  \n",
    "  \n",
    "+ Tf-idf表示法\n",
    "\n",
    "\n",
    "\n",
    "此外，对于单词处理，特别是分词（Tokenization）这一步，还可以做额外的处理.  \n",
    "词表中通常同时包含某些单词的单复数形式、各种时态形式，这个问题可以通过**用词干（word stem）表示每个单词来解决**。  \n",
    "通常有两种方式：\n",
    "1. 基于规则的启发法（比如删除常见的后缀），通常将其称为**词干提取（stemming）**\n",
    "2. 使用由已知单词形式组成的字典（明确的且经过人工验证的系统），通常称为**词形还原（lemmatization）**\n",
    "\n",
    "一般说来，**词形还原相比于词干提取更加好用**。\n",
    "  \n",
    "  \n",
    "完成上面这些步骤，通常有两个包可以使用：\n",
    "+ NLTK\n",
    "+ Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn中的文本特征提取\n",
    "\n",
    "sklearn中提供了一些用于处理文本特征的类，都在`sklearn.feature_extraction.text`这个package里，主要是如下三个类：\n",
    "\n",
    "`CountVectorizer()`，用于将文本转换成count. \n",
    "+ 参数\n",
    "  + `input='content'`，输入的类型，可以是文本，文件，字符串等\n",
    "  + `strip_accents=None`, 移除 accent 的方式\n",
    "  + `lowercase=True`, 是否转成小写\n",
    "  + `preprocessor=None`, 自定义预处理过程\n",
    "  + `tokenizer=None`,  自定义分词器\n",
    "  + `stop_words=None`, 停用词集合\n",
    "  + `token_pattern='(?u)\\b\\w\\w+\\b'`, 分词的正则表达式\n",
    "  + `ngram_range=(1, 1)`, tuple (min_n, max_n), n-gram的范围\n",
    "  + `analyzer='word'`, 取值 {‘word’, ‘char’, ‘char_wb’}, 分词的粒度\n",
    "  + `max_df=1.0`, 超过此阈值的词会被忽略，有两种设置方式\n",
    "    + 0~1 之间的float\n",
    "    + int，\n",
    "  + `min_df=1`, 单词最小出现次数阈值\n",
    "  + `max_features=None`, int，最大的特征值数量，按照频次排序，取top\n",
    "  + `vocabulary=None`, 自定义词典\n",
    "  + `binary=False` \n",
    "+ 属性\n",
    "  + `vocabulary_`：dict\n",
    "  + `stop_words_`：set\n",
    "  + `fixed_vocabulary_`: boolean\n",
    "+ 常用方法\n",
    "  + `fit()`,`transform()`\n",
    "  + `get_stop_words()`\n",
    "  + `get_feature_names()`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`TfidfVectorizer()`，用于将原始文本数据转换成tf-idf特征\n",
    "+ 参数，大部分参数和`CountVectorizer()`一致，\n",
    "+ 属性，和`CountVectorizer()`一致，还多了如下属性\n",
    "  + `idf_`，array of shape (n_features,)，\n",
    "+ 方法，和`CountVectorizer()`一致\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`TfidfTransformer()`，用于将`CountVectorizer()`的结果转换成Tf-idf特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer()\n",
    "count_vec.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取 单词字典表\n",
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取字典中的词，顺序就是上述字典表定义的顺序\n",
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取停用词\n",
    "count_vec.stop_words_\n",
    "# 或者\n",
    "count_vec.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接得到的 X 是个稀疏矩阵\n",
    "X = count_vec.transform(corpus)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_vec.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf_vec.transform(corpus)\n",
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_trans = TfidfTransformer()\n",
    "X_ = count_vec.transform(corpus)\n",
    "tfidf_trans.fit(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf_trans.transform(X_)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "NLTK的API结构参考这里 [NLTK Python Module Index](https://www.nltk.org/py-modindex.html).\n",
    "\n",
    "\n",
    "1. 分词.  \n",
    "NLTK提供了`nltk.tokenize`这个模块，主要是如下两个分词**函数**（不是类）\n",
    "  + `nltk.tokenize.sent_tokenize(text, language='english')`  \n",
    "用于分割句子\n",
    "  + `nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)`  \n",
    "用于分割单词\n",
    "\n",
    "\n",
    "2. 词干提取.  \n",
    "NLTK提供了`nltk.stem`这个模块.\n",
    "  + `nltk.stem.porter.PorterStemmer()`类，封装了Porter提取算法\n",
    "  + `nltk.stem.snowball`模块里提供了非英语类词干的提取方法\n",
    "  + `nltk.stem.wordnet.WordNetLemmatizer()`类，提供了词形还原的算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Adam, how are you?',\n",
       " 'I hope everything is going well.',\n",
       " 'Today is a good day, see you dude.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Adam',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'everything',\n",
       " 'is',\n",
       " 'going',\n",
       " 'well',\n",
       " '.',\n",
       " 'Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'day',\n",
       " ',',\n",
       " 'see',\n",
       " 'you',\n",
       " 'dude',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(porter_stemmer.stem('working'))\n",
    "print(porter_stemmer.stem('works'))\n",
    "print(porter_stemmer.stem('worked'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "increased\n",
      "wa\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('increased'))\n",
    "print(lemmatizer.lemmatize('was'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy的使用和NLTK完全不同，NLTK大体上还是面向过程的调用函数方式来完成任务，而spaCy是采用的pipeline方式完成.  \n",
    "\n",
    "spaCy中，训练好的模型被称为 **trained pipeline**，这些pipilines实际上也是单独的一个python package。\n",
    "\n",
    "spaCy的官网中，提供了一个有关文本处理术语的简单教程 [spacy-101](https://spacy.io/usage/spacy-101)，可以看一看。\n",
    "\n",
    "spaCy的使用步骤如下：\n",
    "1. 载入语言模型.  \n",
    "每个语言模型里，基于所选的语言，封装了一系列对文件进行处理的Pipeline.\n",
    "```python\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "```\n",
    "2. 将需要处理的文本传递给语言模型，生成一个`Doc`对象——它包含了一系列对文本进行处理的步骤，封装成一个Pipeline，其中第一步就是分词\n",
    "```python\n",
    "doc = nlp(\"Text to be process\")\n",
    "```\n",
    "3. 调用`Doc`对象的各种方法，获取不同的内容."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 加载语言模型  \n",
    "每个语言模型都是一个Language对象."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一种方式\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# 或者从自定义的位置加载\n",
    "# nlp = spacy.load(\"/path/to/en_core_web_sm\") \n",
    "nlp.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第二种方式，以单独的package加载\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lang': 'en',\n",
       " 'name': 'core_web_sm',\n",
       " 'version': '3.0.0',\n",
       " 'spacy_version': '>=3.0.0,<3.1.0',\n",
       " 'description': 'English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.',\n",
       " 'author': 'Explosion',\n",
       " 'email': 'contact@explosion.ai',\n",
       " 'url': 'https://explosion.ai',\n",
       " 'license': 'MIT',\n",
       " 'spacy_git_version': 'e7db07a0b',\n",
       " 'vectors': {'width': 0, 'vectors': 0, 'keys': 0, 'name': None},\n",
       " 'labels': {'tok2vec': [],\n",
       "  'tagger': ['$',\n",
       "   \"''\",\n",
       "   ',',\n",
       "   '-LRB-',\n",
       "   '-RRB-',\n",
       "   '.',\n",
       "   ':',\n",
       "   'ADD',\n",
       "   'AFX',\n",
       "   'CC',\n",
       "   'CD',\n",
       "   'DT',\n",
       "   'EX',\n",
       "   'FW',\n",
       "   'HYPH',\n",
       "   'IN',\n",
       "   'JJ',\n",
       "   'JJR',\n",
       "   'JJS',\n",
       "   'LS',\n",
       "   'MD',\n",
       "   'NFP',\n",
       "   'NN',\n",
       "   'NNP',\n",
       "   'NNPS',\n",
       "   'NNS',\n",
       "   'PDT',\n",
       "   'POS',\n",
       "   'PRP',\n",
       "   'PRP$',\n",
       "   'RB',\n",
       "   'RBR',\n",
       "   'RBS',\n",
       "   'RP',\n",
       "   'SYM',\n",
       "   'TO',\n",
       "   'UH',\n",
       "   'VB',\n",
       "   'VBD',\n",
       "   'VBG',\n",
       "   'VBN',\n",
       "   'VBP',\n",
       "   'VBZ',\n",
       "   'WDT',\n",
       "   'WP',\n",
       "   'WP$',\n",
       "   'WRB',\n",
       "   'XX',\n",
       "   '``'],\n",
       "  'parser': ['ROOT',\n",
       "   'acl',\n",
       "   'acomp',\n",
       "   'advcl',\n",
       "   'advmod',\n",
       "   'agent',\n",
       "   'amod',\n",
       "   'appos',\n",
       "   'attr',\n",
       "   'aux',\n",
       "   'auxpass',\n",
       "   'case',\n",
       "   'cc',\n",
       "   'ccomp',\n",
       "   'compound',\n",
       "   'conj',\n",
       "   'csubj',\n",
       "   'csubjpass',\n",
       "   'dative',\n",
       "   'dep',\n",
       "   'det',\n",
       "   'dobj',\n",
       "   'expl',\n",
       "   'intj',\n",
       "   'mark',\n",
       "   'meta',\n",
       "   'neg',\n",
       "   'nmod',\n",
       "   'npadvmod',\n",
       "   'nsubj',\n",
       "   'nsubjpass',\n",
       "   'nummod',\n",
       "   'oprd',\n",
       "   'parataxis',\n",
       "   'pcomp',\n",
       "   'pobj',\n",
       "   'poss',\n",
       "   'preconj',\n",
       "   'predet',\n",
       "   'prep',\n",
       "   'prt',\n",
       "   'punct',\n",
       "   'quantmod',\n",
       "   'relcl',\n",
       "   'xcomp'],\n",
       "  'senter': ['I', 'S'],\n",
       "  'ner': ['CARDINAL',\n",
       "   'DATE',\n",
       "   'EVENT',\n",
       "   'FAC',\n",
       "   'GPE',\n",
       "   'LANGUAGE',\n",
       "   'LAW',\n",
       "   'LOC',\n",
       "   'MONEY',\n",
       "   'NORP',\n",
       "   'ORDINAL',\n",
       "   'ORG',\n",
       "   'PERCENT',\n",
       "   'PERSON',\n",
       "   'PRODUCT',\n",
       "   'QUANTITY',\n",
       "   'TIME',\n",
       "   'WORK_OF_ART'],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': []},\n",
       " 'pipeline': ['tok2vec',\n",
       "  'tagger',\n",
       "  'parser',\n",
       "  'ner',\n",
       "  'attribute_ruler',\n",
       "  'lemmatizer'],\n",
       " 'components': ['tok2vec',\n",
       "  'tagger',\n",
       "  'parser',\n",
       "  'senter',\n",
       "  'ner',\n",
       "  'attribute_ruler',\n",
       "  'lemmatizer'],\n",
       " 'disabled': ['senter'],\n",
       " 'performance': {'token_acc': 0.9993053983000001,\n",
       "  'tag_acc': 0.9721120187000001,\n",
       "  'dep_uas': 0.9163332177,\n",
       "  'dep_las': 0.8977001329000001,\n",
       "  'ents_p': 0.8483091787,\n",
       "  'ents_r': 0.8354366987,\n",
       "  'ents_f': 0.8418237327,\n",
       "  'sents_p': 0.8979094784,\n",
       "  'sents_r': 0.8755194934,\n",
       "  'sents_f': 0.8865731463000001,\n",
       "  'speed': 10859.3473736922,\n",
       "  'dep_las_per_type': {'prep': {'p': 0.8527070662,\n",
       "    'r': 0.8617340163,\n",
       "    'f': 0.8571967767},\n",
       "   'det': {'p': 0.9763465375, 'r': 0.9778194569, 'f': 0.9770824421000001},\n",
       "   'pobj': {'p': 0.9603007636, 'r': 0.9679560181, 'f': 0.964113195},\n",
       "   'nsubj': {'p': 0.9573928142, 'r': 0.9421248631, 'f': 0.9496974782000001},\n",
       "   'aux': {'p': 0.9790575916, 'r': 0.9821953174, 'f': 0.9806239445},\n",
       "   'advmod': {'p': 0.8549869320000001,\n",
       "    'r': 0.8531886253000001,\n",
       "    'f': 0.854086832},\n",
       "   'relcl': {'p': 0.7642653352000001,\n",
       "    'r': 0.7775761974000001,\n",
       "    'f': 0.7708633094},\n",
       "   'root': {'p': 0.9100514766000001,\n",
       "    'r': 0.8863381490000001,\n",
       "    'f': 0.8980382983},\n",
       "   'xcomp': {'p': 0.8736915562, 'r': 0.8987796123, 'f': 0.8860580326},\n",
       "   'amod': {'p': 0.9145399052000001,\n",
       "    'r': 0.9124068675,\n",
       "    'f': 0.9134721411000001},\n",
       "   'compound': {'p': 0.916965122, 'r': 0.9268211183, 'f': 0.9218667775},\n",
       "   'poss': {'p': 0.9734833266, 'r': 0.9754428341, 'f': 0.9744620953},\n",
       "   'ccomp': {'p': 0.7768093975, 'r': 0.8350305499, 'f': 0.8048684727000001},\n",
       "   'attr': {'p': 0.8984280532000001, 'r': 0.9373423045, 'f': 0.917472731},\n",
       "   'case': {'p': 0.9772502473,\n",
       "    'r': 0.9889889890000001,\n",
       "    'f': 0.9830845771000001},\n",
       "   'mark': {'p': 0.9032, 'r': 0.8974562798, 'f': 0.9003189793},\n",
       "   'intj': {'p': 0.6546026751, 'r': 0.6095238095000001, 'f': 0.6312594841},\n",
       "   'advcl': {'p': 0.6674341264, 'r': 0.6570133468, 'f': 0.6621827411},\n",
       "   'cc': {'p': 0.8301659851000001, 'r': 0.8254993422, 'f': 0.827826087},\n",
       "   'neg': {'p': 0.9471057884,\n",
       "    'r': 0.9523331661000001,\n",
       "    'f': 0.9497122842000001},\n",
       "   'conj': {'p': 0.7557513461, 'r': 0.7774420947, 'f': 0.7664432862},\n",
       "   'nsubjpass': {'p': 0.9228764982000001,\n",
       "    'r': 0.9082051282,\n",
       "    'f': 0.9154820367000001},\n",
       "   'auxpass': {'p': 0.9466903598, 'r': 0.9708428246, 'f': 0.9586144849},\n",
       "   'dobj': {'p': 0.9196734059, 'r': 0.9424655351000001, 'f': 0.9309299854},\n",
       "   'nummod': {'p': 0.9364191251, 'r': 0.9297979798, 'f': 0.9330968069000001},\n",
       "   'npadvmod': {'p': 0.7637178052, 'r': 0.7268206039, 'f': 0.7448125228},\n",
       "   'prt': {'p': 0.8143564356, 'r': 0.8844086022000001, 'f': 0.8479381443},\n",
       "   'pcomp': {'p': 0.8705388383, 'r': 0.8711484594000001, 'f': 0.8708435422},\n",
       "   'expl': {'p': 0.9789029536, 'r': 0.9935760171, 'f': 0.9861849097000001},\n",
       "   'acl': {'p': 0.7324106113000001,\n",
       "    'r': 0.6928532460000001,\n",
       "    'f': 0.7120829829},\n",
       "   'agent': {'p': 0.8938356164,\n",
       "    'r': 0.9354838710000001,\n",
       "    'f': 0.9141856392000001},\n",
       "   'dative': {'p': 0.7704485488, 'r': 0.6697247706, 'f': 0.7165644172000001},\n",
       "   'acomp': {'p': 0.9058931019, 'r': 0.8993197279, 'f': 0.902594447},\n",
       "   'dep': {'p': 0.4608695652, 'r': 0.1720779221, 'f': 0.2505910165},\n",
       "   'csubj': {'p': 0.6709677419, 'r': 0.6153846154, 'f': 0.6419753086000001},\n",
       "   'quantmod': {'p': 0.8619768477, 'r': 0.7863525589, 'f': 0.8224299065},\n",
       "   'nmod': {'p': 0.7531948882, 'r': 0.5746496039, 'f': 0.6519184238},\n",
       "   'appos': {'p': 0.6889692586, 'r': 0.6611713666, 'f': 0.6747841488},\n",
       "   'predet': {'p': 0.8442622951000001, 'r': 0.8841201717, 'f': 0.8637316562},\n",
       "   'preconj': {'p': 0.580952381, 'r': 0.7093023256000001, 'f': 0.6387434555},\n",
       "   'oprd': {'p': 0.8193548387, 'r': 0.7582089552, 'f': 0.7875968992},\n",
       "   'parataxis': {'p': 0.5831134565, 'r': 0.4793926247, 'f': 0.5261904762},\n",
       "   'meta': {'p': 0.4705882353, 'r': 0.3076923077, 'f': 0.3720930233},\n",
       "   'csubjpass': {'p': 0.8333333333, 'r': 0.8333333333, 'f': 0.8333333333}},\n",
       "  'ents_per_type': {'DATE': {'p': 0.8645768025,\n",
       "    'r': 0.8755555556,\n",
       "    'f': 0.8700315457000001},\n",
       "   'GPE': {'p': 0.9231436001000001, 'r': 0.8912133891, 'f': 0.9068975305},\n",
       "   'ORDINAL': {'p': 0.7709497207, 'r': 0.8571428571, 'f': 0.8117647059},\n",
       "   'ORG': {'p': 0.8055629139, 'r': 0.8062036055, 'f': 0.8058831324},\n",
       "   'QUANTITY': {'p': 0.8235294118, 'r': 0.6153846154, 'f': 0.7044025157},\n",
       "   'CARDINAL': {'p': 0.8196721311, 'r': 0.8620689655, 'f': 0.8403361345},\n",
       "   'PERSON': {'p': 0.8394070414,\n",
       "    'r': 0.8870757180000001,\n",
       "    'f': 0.8625833069000001},\n",
       "   'NORP': {'p': 0.910801964, 'r': 0.8904000000000001, 'f': 0.9004854369},\n",
       "   'FAC': {'p': 0.375, 'r': 0.30000000000000004, 'f': 0.33333333330000003},\n",
       "   'TIME': {'p': 0.7392638037, 'r': 0.7046783626, 'f': 0.7215568862},\n",
       "   'LOC': {'p': 0.7425373134000001, 'r': 0.6337579618, 'f': 0.6838487973},\n",
       "   'WORK_OF_ART': {'p': 0.4608695652,\n",
       "    'r': 0.27319587630000003,\n",
       "    'f': 0.3430420712},\n",
       "   'LAW': {'p': 0.5641025641,\n",
       "    'r': 0.34375000000000006,\n",
       "    'f': 0.42718446600000004},\n",
       "   'EVENT': {'p': 0.5913978495000001,\n",
       "    'r': 0.316091954,\n",
       "    'f': 0.41198501870000004},\n",
       "   'MONEY': {'p': 0.9208633094,\n",
       "    'r': 0.9067296340000001,\n",
       "    'f': 0.9137418203000001},\n",
       "   'PERCENT': {'p': 0.9106858054, 'r': 0.8744257274, 'f': 0.8921874999999999},\n",
       "   'LANGUAGE': {'p': 0.7407407407000001, 'r': 0.625, 'f': 0.6779661017},\n",
       "   'PRODUCT': {'p': 0.6144578313, 'r': 0.2417061611, 'f': 0.3469387755}}},\n",
       " 'sources': [{'name': 'OntoNotes 5',\n",
       "   'url': 'https://catalog.ldc.upenn.edu/LDC2013T19',\n",
       "   'license': 'commercial (licensed by Explosion)'}],\n",
       " 'requirements': []}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 语言模型的信息\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/MyProgramFiles/Anaconda/lib/site-packages/en_core_web_sm/en_core_web_sm-3.0.0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查当前pipeline里的步骤名称\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.vocab.Vocab object at 0x00000185AF7103A8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "761"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nlp.vocab)\n",
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 文档的Doc对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实例化Doc对象\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "doc.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Doc`对象实现了`__getitem__()`方法和`__iter__()`方法，所以可以索引或者迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "Apple\n"
     ]
    }
   ],
   "source": [
    "# 索引token\n",
    "print(doc[0].__class__)\n",
    "print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token.text token.lemma_\n",
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n"
     ]
    }
   ],
   "source": [
    "# 打印分词后的 token 和词形还原后的样子\n",
    "print(\"token.text\", \"token.lemma_\")\n",
    "for token in doc:\n",
    "    print(token.text,  token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 单词的Token对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "Apple\n",
      "<class 'numpy.ndarray'>\n",
      "(96,)\n",
      "[-1.2839912  -0.43607238  0.99951804 -2.360404    3.0775332  -3.6693985\n",
      "  1.5802528   2.0529222  -1.294323    2.1639879   2.7700207   0.17506354\n",
      "  0.0953052   0.4334653  -0.07980722 -0.60584813  0.39560458  5.2149515\n",
      "  0.88727987 -2.617487    1.8526615   1.1368239   3.9496171  -2.648594\n",
      " -0.06955147  2.0044653  -3.7960272  -3.1989868   0.35334867 -0.9531797\n",
      " -0.78921217 -2.428277    0.08931901 -3.615139    0.07555252  1.1542726\n",
      "  2.0777988  -0.10464281  1.0964748  -1.7745137   3.4849372  -0.02747166\n",
      "  3.3159113  -1.8869483   0.139435   -2.0437162   1.5334139   4.711532\n",
      " -3.0375469  -0.8940801  -0.9046861   0.57582724 -2.6169019  -1.4401975\n",
      " -3.3344631   2.4536233   1.6070327   2.484366   -1.5178642  -0.07589482\n",
      " -0.60906625 -1.5116876   3.2915626  -3.8901231  -2.3225067   5.4058423\n",
      "  2.6897678   1.3720402   1.3001097   0.06986803 -3.4310882   0.3431276\n",
      " -2.000019   -1.5987017   3.4761806  -3.130803   -0.4823103   0.63526744\n",
      " -1.6966465  -1.6946815   1.1866399   2.1364102   0.24674171 -2.814292\n",
      "  0.12162346 -1.3381917   3.2049303   1.7935071   1.8020983  -2.7525592\n",
      "  3.2679296  -1.8049885  -0.5194188   0.09404367  0.3067871  -1.7171175 ]\n"
     ]
    }
   ],
   "source": [
    "t1 = doc[0]\n",
    "print(t1.__class__)\n",
    "print(t1)\n",
    "# token对应的词向量\n",
    "print(t1.vector.__class__)\n",
    "print(t1.vector.shape)\n",
    "print(t1.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Gensim\n",
    "\n",
    "gensim是专门用于训练生成词向量的package。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本数据集\n",
    "\n",
    "这里有 3 份数据集，分别是\n",
    "1. IMDB电影评分数据集\n",
    "2. sklearn自带的新闻数据集\n",
    "3. 《python数据挖掘入门》里文本处理使用的tweet数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB电影评分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis\\\\ML-Action'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = load_files('./datasets/aclImdb/train/')\n",
    "imdb_test = load_files('./datasets/aclImdb/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn自带的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里指定了数据集下载的文件夹，第一次使用时，会下载相关的数据集到这个文件夹里\n",
    "data = fetch_20newsgroups(data_home='datasets/sklearn-news-data/', subset='all')\n",
    "# print(data.__class__)\n",
    "\n",
    "# 显示新闻数据集里文章有哪些分类\n",
    "# display(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300 (2300,)\n",
      "1531 (1531,)\n"
     ]
    }
   ],
   "source": [
    "# 这里只需要部分分类的数据集\n",
    "categories = [\n",
    "    'sci.space',\n",
    "    'rec.sport.baseball',\n",
    "    'talk.politics.mideast',\n",
    "    'talk.politics.guns'\n",
    "]\n",
    "\n",
    "# 训练集\n",
    "train = fetch_20newsgroups(data_home='datasets/sklearn-news-data/', subset='train', categories=categories)\n",
    "# 测试集\n",
    "test = fetch_20newsgroups(data_home='datasets/sklearn-news-data/', subset='test', categories=categories)\n",
    "\n",
    "# 查看数据集大小\n",
    "print(len(train.data), train.target.shape)\n",
    "print(len(test.data), test.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[0].__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: tclock@orion.oac.uci.edu (Tim Clock)\\nSubject: Re: Final Solution for Gaza ?\\nNntp-Posting-Host: orion.oac.uci.edu\\nOrganization: University of California, Irvine\\nLines: 66\\n\\nIn article <1483500354@igc.apc.org> Center for Policy Research <cpr@igc.apc.org> writes:\\n>\\n>From: Center for Policy Research <cpr>\\n>Subject: Final Solution for Gaza ?\\n>\\n>While Israeli Jews fete the uprising of the Warsaw ghetto,\\n\\n\"fete\"??? Since this word both formally and commonly refers to\\npositive/joyous events, your misuse of it here is rather unsettling.\\n \\n>they repress by violent means the uprising of the Gaza ghetto \\n>and attempt to starve the Gazans.\\n\\nI certainly abhor those Israeli policies and attitudes that are\\nabusive towards the Palestinians/Gazans. Given that, however, there \\n*is no comparison* between the reality of the Warsaw Ghetto and in \\nGaza.  \\n>\\n>The right of the Gazan population to resist occupation is\\n>recognized in international law and by any person with a sense of\\n>justice. \\n\\nJust as international law recognizes the right of the occupying \\nentity to maintain order, especially in the face of elements\\nthat are consciously attempting to disrupt the civil structure. \\nIronically, international law recognizes each of these focusses\\n(that of the occupied and the occupier) even though they are \\ninherently in conflict.\\n>\\n>As Israel denies Gazans the only two options which are compatible\\n>with basic human rights and international law, that of becoming\\n>Israeli citizens with full rights or respecting their right for\\n>self-determination, it must be concluded that the Israeli Jewish\\n>society does not consider Gazans full human beings.\\n\\nIsrael certainly cannot, and should not, continue its present\\npolicies towards Gazan residents. There is, however, a third \\nalternative- the creation and implementation of a jewish \"dhimmi\"\\nsystem with Gazans/Palestinians as benignly \"protected\" citizens.\\nWould you find THAT as acceptable in that form as you do with\\nregard to Islam\\'s policies towards its minorities?\\n \\n>Whether they have some Final Solution up their sleeve ?\\n\\nIt is a race, then? Between Israel\\'s anti-Palestinian/Gazan\\n\"Final Solution\" and the Arab World\\'s anti-Israel/jewish\\n\"Final Solution\". Do you favor one? neither? \\n>\\n>I urge all those who have slight human compassion to do whatever\\n>they can to help the Gazans regain their full human, civil and\\n>political rights, to which they are entitled as human beings.\\n\\nSince there is justifiable worry by various parties that Israel\\nand Arab/Palestinian \"final solution\" intentions exist, isn\\'t it\\nimportant that BOTH Israeli *and* Palestinian/Gazan \"rights\"\\nbe secured?\\n>\\n>Elias Davidsson Iceland\\n>\\n\\n\\n--\\nTim Clock                                   Ph.D./Graduate student\\nUCI  tel#: 714,8565361                      Department of Politics and Society\\n     fax#: 714,8568441                      University of California - Irvine\\nHome tel#: 714,8563446                      Irvine, CA 92717\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随便看一下里面的一篇文章\n",
    "train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    597\n",
       "1    593\n",
       "3    564\n",
       "2    546\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看看类别是否均衡\n",
    "pd.Series(train.target).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯文本分类示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "import spacy\n",
    "# 首先必须载入一个语言模型\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先取一部分子集看下效果\n",
    "part = train.data[:10]\n",
    "\n",
    "cnt_vec = CountVectorizer()\n",
    "cnt_vec.fit(part)\n",
    "\n",
    "part_ = pd.DataFrame(cnt_vec.transform(part).toarray(), columns=cnt_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用spaCy对文件进行处理\n",
    "# 这里使用spaCy的词形还原，但是分词器使用的是scikit-learn里提供的正则表达式\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))\n",
    "\n",
    "# 自定义的分词函数\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyProgramFiles\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n",
      "C:\\MyProgramFiles\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: [W002] Tokenizer.from_list is now deprecated. Create a new Doc object instead and pass in the strings as the `words` keyword argument, for example:\n",
      "from spacy.tokens import Doc\n",
      "doc = Doc(nlp.vocab, words=[...])\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.95, max_features=None, min_df=5,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function custom_tokenizer at 0x000001F16CC768B8>,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5, max_df=0.95)\n",
    "# lemma_vect.fit(part)\n",
    "# part_ = pd.DataFrame(cnt_vec.transform(part).toarray(), columns=cnt_vec.get_feature_names())\n",
    "lemma_vect.fit(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyProgramFiles\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: [W002] Tokenizer.from_list is now deprecated. Create a new Doc object instead and pass in the strings as the `words` keyword argument, for example:\n",
      "from spacy.tokens import Doc\n",
      "doc = Doc(nlp.vocab, words=[...])\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_token = lemma_vect.transform(train.data)\n",
    "test_token = lemma_vect.transform(test.data)\n",
    "train_y = train.target\n",
    "test_y = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2300, 8448)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531, 8448)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB = BernoulliNB()\n",
    "multiNB = MultinomialNB()\n",
    "\n",
    "berNB.fit(train_token, train_y)\n",
    "multiNB.fit(train_token, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "berNB_score = berNB.score(test_token, test_y)\n",
    "multiNB_score = multiNB.score(test_token, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8883082952318746"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9738732854343566"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiNB_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "berNB_y_pred = berNB.predict(test_token)\n",
    "multiNB_y_pred = multiNB.predict(test_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "berNB_pred_score = accuracy_score(y_true=test_y, y_pred=berNB_y_pred)\n",
    "multiNB_pred_score = accuracy_score(y_true=test_y, y_pred=multiNB_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8883082952318746"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berNB_pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9738732854343566"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiNB_pred_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用spaCy对文件进行处理\n",
    "import spacy\n",
    "# 首先必须载入一个语言模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# 这里使用spaCy的词形还原，但是分词器使用的是scikit-learn里提供的正则表达式\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))\n",
    "\n",
    "# 自定义的分词函数\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\MyProgramFiles\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: [W002] Tokenizer.from_list is now deprecated. Create a new Doc object instead and pass in the strings as the `words` keyword argument, for example:\n",
      "from spacy.tokens import Doc\n",
      "doc = Doc(nlp.vocab, words=[...])\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# t = train.data[1]\n",
    "# t_res = custom_tokenizer(t)\n",
    "\n",
    "train_proc = [custom_tokenizer(doc) for doc in train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing time is :11.42\n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "# 使用gensim训练模型\n",
    "t1 = time()\n",
    "model = word2vec.Word2Vec(train_proc,iter=50, size=100, window=3, sg=0, workers=4)\n",
    "t2 = time()\n",
    "\n",
    "print(\"runing time is :{:.2f}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing time is :10.94\n"
     ]
    }
   ],
   "source": [
    "# %time\n",
    "# 使用gensim训练模型\n",
    "t1 = time()\n",
    "model = word2vec.Word2Vec(train_proc,iter=50, size=100, window=3, sg=0, workers=8)\n",
    "t2 = time()\n",
    "\n",
    "print(\"runing time is :{:.2f}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1e6237ec688>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1e628df6d08>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'policy' in model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.46654677e+00, -1.99778438e+00,  1.26974404e+00,  5.50841391e-01,\n",
       "       -6.28954530e-01,  1.28450203e+00, -7.91259587e-01, -4.42545146e-01,\n",
       "       -2.63815850e-01,  2.10831928e+00, -1.98553884e+00, -1.15344435e-01,\n",
       "       -1.62778163e+00,  1.45624906e-01, -3.31313372e-01,  2.18228966e-01,\n",
       "        1.20822871e+00, -1.04821241e+00, -2.85565615e+00, -2.82280177e-01,\n",
       "       -1.79006910e+00, -2.54633689e+00,  1.30262002e-01,  1.86022115e+00,\n",
       "        3.36442888e-01, -6.46334291e-01,  2.42829466e+00, -2.52690744e+00,\n",
       "        2.36002254e+00,  6.97122872e-01, -4.39212620e-01,  9.54245448e-01,\n",
       "        2.59631556e-02, -1.55718148e+00, -1.44868529e+00, -1.58921421e+00,\n",
       "        1.29590106e+00, -4.33103770e-01,  1.45411468e+00,  2.47827798e-01,\n",
       "       -1.26484489e+00,  1.31447625e+00,  1.01077892e-01, -4.03619200e-01,\n",
       "        1.58041000e+00,  1.13859832e+00,  9.37864125e-01,  4.93863732e-01,\n",
       "       -8.31795692e-01,  8.17711473e-01, -9.16759610e-01,  5.99537566e-02,\n",
       "       -3.73655498e-01, -4.98788387e-01, -7.69510686e-01,  2.97787488e-01,\n",
       "       -1.48637354e+00,  6.26486242e-01, -2.47621059e+00,  1.02689469e+00,\n",
       "       -4.75363344e-01, -1.83365297e+00, -1.79995263e+00,  4.07261044e-01,\n",
       "       -6.61909059e-02,  8.08036923e-02, -1.06671107e+00,  1.69375286e-01,\n",
       "       -4.73596096e-01, -8.46775949e-01, -8.46597599e-04,  6.89261675e-01,\n",
       "       -8.77974212e-01,  4.54288065e-01, -4.03399646e-01, -3.95492166e-01,\n",
       "       -1.45100272e+00,  6.51426613e-01,  5.41570365e-01,  5.38356304e-01,\n",
       "       -1.30447578e+00, -5.38048036e-02, -5.21744132e-01,  5.18297791e-01,\n",
       "       -1.21181823e-01,  9.50638771e-01, -4.40441191e-01,  5.76104164e-01,\n",
       "       -7.66472995e-01, -1.91807103e+00,  1.96406150e+00,  1.12731969e+00,\n",
       "        1.57234281e-01,  1.90286183e+00,  1.32262766e-01, -1.37146723e+00,\n",
       "       -2.17445865e-01,  2.16577077e+00,  1.17955470e+00, -5.81313074e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['policy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('entity', 0.505190372467041),\n",
       " ('terrorism', 0.48177653551101685),\n",
       " ('regime', 0.45322495698928833),\n",
       " ('action', 0.4518165588378906),\n",
       " ('goverment', 0.4459748864173889),\n",
       " ('gazan', 0.4422627091407776),\n",
       " ('government', 0.4344480633735657),\n",
       " ('existance', 0.429462730884552),\n",
       " ('philosophy', 0.42827075719833374),\n",
       " ('Nazis', 0.4210732877254486)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "以下是别人的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1893\n",
      "1892\n"
     ]
    }
   ],
   "source": [
    "#读取停用词\n",
    "stop_words = []\n",
    "with open(\"data\\stop_words.txt\", \"r\", encoding=\"utf-8\") as f_reader:\n",
    "    for line in f_reader:\n",
    "        line = line.replace(\"\\r\",\"\").replace(\"\\n\",\"\").strip()\n",
    "        stop_words.append(line)\n",
    "print(len(stop_words))\n",
    "stop_words = set(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\bruce\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.707 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#文本预处理\n",
    "sentecnces = []\n",
    "rules = u\"[\\u4e00-\\u9fa5]+\"\n",
    "pattern = re.compile(rules)\n",
    "f_writer = open(\"data\\分词后的天龙八部.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "with open(\"data\\天龙八部.txt\", \"r\" , encoding=\"utf-8\") as f_reader:\n",
    "    for line in f_reader:\n",
    "        line = line.replace(\"\\r\",\"\").replace(\"\\n\",\"\").strip()\n",
    "        if line == \"\" or line is None:\n",
    "            continue\n",
    "        line = \" \".join(jieba.cut(line))\n",
    "        seg_list = pattern.findall(line)\n",
    "        word_list = []\n",
    "        for word in seg_list:\n",
    "            if word not in stop_words:\n",
    "                word_list.append(word)\n",
    "        if len(word_list) > 0:\n",
    "            sentecnces.append(word_list)\n",
    "            line = \" \".join(word_list)\n",
    "            f_writer.write(line + \"\\n\")\n",
    "            f_writer.flush()\n",
    "f_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['书名', '天龙八部'], ['作者', '金庸'], ['本文', '早安', '电子书', '网友', '分享', '版权', '原作者'], ['用于', '商业行为', '后果自负'], ['早安', '电子书'], ['金庸', '作品集', '三联', '版', '序'], ['小学', '时', '爱读', '课外书', '低年级', '时看', '儿童', '画报', '小朋友', '小学生', '内容', '小朋友', '文库', '似懂非懂', '阅读', '各种各样', '章回小说', '五六年', '级', '时', '看新', '文艺作品', '喜爱', '古典文学', '作品', '多于', '近代', '当代', '新文学', '个性', '使然', '朋友', '喜欢', '新文学', '不爱', '古典文学'], ['知识', '当代', '书报', '中', '寻求', '小学', '时代', '得益', '记忆', '最深', '爸爸', '哥哥', '购置', '邹韬奋', '所撰', '萍踪', '寄语', '萍踪', '忆语', '世界各地', '旅行', '记', '主编', '生活', '周报', '新', '旧', '童年时代', '深受', '邹先生', '生活', '书店', '之惠', '生活', '书店', '三联书店', '组成部分', '十多年', '前', '香港三联书店', '签', '合同', '中国', '大陆', '地区', '出版', '小说', '因事', '未果', '重', '行', '筹划', '三联书店', '独家', '出版', '中国', '大陆', '地区', '简体字', '感到', '欣慰', '回忆', '昔日', '心中', '充满', '温馨', '之意'], ['撰写', '这套', '总数', '三十六', '册', '作品集', '是从', '一九五五年', '七二年', '约', '十三', '四年', '包括', '十二部', '长篇小说', '两篇', '中篇小说', '一篇', '短篇小说', '一篇', '历史', '人物', '评传', '若干篇', '历史', '考据', '文字', '出版', '过程', '奇怪', '香港', '台湾', '海外', '地区', '中国', '大陆', '先出', '各种各样', '翻版', '盗印', '出版', '校订', '授权', '正', '版本', '中国', '大陆', '三联', '版', '出版', '天津', '百花文艺出版社', '一家', '授权', '出版', '书剑', '恩仇录', '校印', '依足', '合同', '支付', '版税', '依足', '法例', '缴付', '所得税', '余数', '捐给', '几家', '文化', '机构', '支助', '围棋', '活动', '这是', '愉快', '经验', '未经', '授权'], ['不付', '版税', '版本', '粗制滥造', '错讹', '百出', '借用', '金庸', '之名', '撰写', '出版', '武侠小说', '写', '不敢掠美', '充满', '无聊', '打斗', '色情', '描写', '之作', '令人', '不快', '出版社', '翻印', '香港', '台湾', '作家', '作品', '而用', '笔名', '出版发行', '收到', '无数', '读者', '来信', '揭露', '大表', '愤慨', '三联', '版', '发行', '制止', '种种', '讲', '道义', '侠义', '小说', '主旨', '讲', '是非', '讲', '道义', '太', '过份']]\n"
     ]
    }
   ],
   "source": [
    "print(sentecnces[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#模型训练\n",
    "model = word2vec.Word2Vec(sentecnces,iter=50, size=100, window=3, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "白世镜 0.46209681034088135\n",
      "宋长老 0.4431273341178894\n",
      "徐长老 0.40213707089424133\n",
      "玄慈 0.3699125647544861\n",
      "乔大爷 0.3584541380405426\n",
      "鲍千灵 0.3493095338344574\n",
      "谭公 0.3465454578399658\n",
      "努儿海 0.3400297462940216\n",
      "马夫人 0.33992263674736023\n",
      "陈长老 0.3371778130531311\n"
     ]
    }
   ],
   "source": [
    "#选出10个与乔峰最相近的10个词\n",
    "for e in model.most_similar(positive=[\"乔峰\"],topn=10):\n",
    "    print(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载预料\n",
    "sentences2 = word2vec.Text8Corpus(\"data\\分词后的天龙八部.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.word2vec.Text8Corpus object at 0x000002203F199DA0>\n"
     ]
    }
   ],
   "source": [
    "print(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#训练模型\n",
    "model = word2vec.Word2Vec(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿紫道 0.996263861656189\n",
      "姊夫 0.9961894750595093\n",
      "冷冷的 0.9957616329193115\n",
      "找 0.995625376701355\n",
      "两位 0.9950875639915466\n",
      "赵钱孙 0.9950235486030579\n",
      "全冠清 0.9950124025344849\n",
      "乔帮主 0.9949069023132324\n",
      "姊姊 0.9948595762252808\n",
      "叹 0.9947936534881592\n"
     ]
    }
   ],
   "source": [
    "#选出10个与乔峰最相近的10个词\n",
    "for e in model.most_similar(positive=[\"乔峰\"],topn=10):\n",
    "    print(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#保存模型\n",
    "model.save(\"data/天龙八部.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "model2 = word2vec.Word2Vec.load(\"data/天龙八部.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿紫道 0.996263861656189\n",
      "姊夫 0.9961894750595093\n",
      "冷冷的 0.9957616329193115\n",
      "找 0.995625376701355\n",
      "两位 0.9950875639915466\n",
      "赵钱孙 0.9950235486030579\n",
      "全冠清 0.9950124025344849\n",
      "乔帮主 0.9949069023132324\n",
      "姊姊 0.9948595762252808\n",
      "叹 0.9947936534881592\n"
     ]
    }
   ],
   "source": [
    "#选出10个与乔峰最相近的10个词\n",
    "for e in model2.most_similar(positive=[\"乔峰\"],topn=10):\n",
    "    print(e[0],e[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983976449658\n"
     ]
    }
   ],
   "source": [
    "#计算两个词语的相似度\n",
    "sim_value = model.similarity('乔峰','萧峰')\n",
    "print(sim_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988961361533\n"
     ]
    }
   ],
   "source": [
    "#计算两个集合的相似度\n",
    "list1 = ['乔峰','萧远山']\n",
    "list2= ['慕容复','慕容博']\n",
    "sim_value = model.n_similarity(list1,list2)\n",
    "print(sim_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "丁春秋\n"
     ]
    }
   ],
   "source": [
    "#选出集合中不同类型的词语\n",
    "list3 = ['段誉','阿紫','王语嫣','丁春秋']\n",
    "print(model.doesnt_match(list3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#查看词向量值\n",
    "print(type(model['乔峰']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(model['乔峰']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0956201   0.83770102 -0.50264329  0.17705031 -0.03397365  0.53726691\n",
      " -0.94604319 -0.62564951 -0.16479155 -0.56092721 -0.05749961  0.19491424\n",
      "  0.23317778  0.69680882 -0.01325463  0.49103716 -0.2274354  -0.27408433\n",
      "  0.30029318  0.30257019  0.38569745 -0.24391353 -0.20851924 -1.03797174\n",
      "  0.1151455  -0.12836897 -0.25457156 -0.21057677 -0.59543329 -0.41599837\n",
      " -0.13542509  0.23070075 -0.0045849  -0.12233413  0.35966769  0.30875343\n",
      " -0.55712724 -0.05722067 -0.20944791  0.12286058 -0.49387416  0.26767832\n",
      " -0.03382351 -0.09347658  0.6634863   0.25116423 -0.17000762 -0.41160905\n",
      "  0.26862589 -0.48492855 -0.2278533   0.45218593 -0.15033394  0.17578831\n",
      " -0.58941668 -0.0460459   0.04037135  0.27327055 -0.22823675  0.2203647\n",
      "  0.05365429 -0.04600301 -0.7688188   0.13671628 -0.53027683 -0.1469509\n",
      "  1.03760362  0.12982909 -0.22781004  0.54549789  0.05604199 -0.42564315\n",
      " -0.06961453 -0.16930862 -0.73733568 -0.40141526  0.83430684 -0.35497716\n",
      "  0.45108098  0.59830898  0.56303334  0.40351576  0.08104084 -0.05698548\n",
      " -0.54859221  0.25344476 -0.3362723  -0.22261383 -0.2233018   0.040442\n",
      "  0.42754599 -1.2365644   0.56711614  0.03170419 -0.08998632  0.26160204\n",
      "  0.18751431  0.04812396 -0.26835883  0.34943965]\n"
     ]
    }
   ],
   "source": [
    "print(model['乔峰'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
