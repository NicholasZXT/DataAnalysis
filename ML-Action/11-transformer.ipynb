{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "terminal-riding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package版本信息：\n",
      "numpy:       1.21.2\n",
      "pandas:      1.3.4\n",
      "matplotlib:  3.5.1\n",
      "sklearn:     1.0.2\n",
      "PyTorch:      1.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "from IPython.display import display\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"package版本信息：\")\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"pandas:     \", pd.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(\"PyTorch:     \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-affiliation",
   "metadata": {},
   "source": [
    "# Pytorch-Transformer\n",
    "\n",
    "参考文档:\n",
    "+ [Pytorch tutorial -> Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "+ [Pytorch -> Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "\n",
    "这里介绍了Pytorch里 transformer 层的实现模块.\n",
    "\n",
    "主要有如下几个类：\n",
    "+ `nn.Transformer`：一步到位构建整个Transformer模型，底层使用的是下面的API\n",
    "+ `nn.TransformerEncoder`：Encoder层，包含了多个 `TransoformerEncoderLayer`\n",
    "+ `nn.TransformerDecoder`：Decoder层，包含了多个 `TransformerDecoderLayer`\n",
    "+ `nn.TransformerEncoderLayer`：单层Encoder\n",
    "+ `nn.TransformerDecoderLayer`：单层Decoder\n",
    "\n",
    "但是需要注意的是，上述各种层的**底部都是调用的`activation.py`中的`MultiheadAttention`类**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-agenda",
   "metadata": {},
   "source": [
    "## MultiheadAttention Layer\n",
    "\n",
    "它是`TransformerEncoderLayer`和`TransformerDecoderLayer`层中 **self-attention 层**的实现。  \n",
    "需要注意的是，pytorch源码中 `MultiheadAttention` 被放在了 Non-linear Activations 这一类中（并且是放在`activation.py`文件中的）。\n",
    "\n",
    "`MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)`   \n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention).\n",
    "+ 参数\n",
    "  + `embed_dim`，self-attention层的输入/输出特征维度——**输入和输出的维度都是一样的**\n",
    "  + `num_heads`，head数量，注意，分配到每个head的维度 = embed_dim/num_heads.\n",
    "  + `kdim`，自定义 key 中的特征维度\n",
    "  + `vidm`，自定义 value 中的特征维度\n",
    "  + `bias`，\n",
    "  + `add_bias_kv`\n",
    "  + `add_zero_attn`\n",
    "\n",
    "\n",
    "+ `forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)`\n",
    "  + `query`, `key`, `value`，self-attention的三个输入向量——这三个输入向量会被用于生成对应的Q,K,V\n",
    "    + `query.shape` = $(L, N, E)$，\n",
    "    + `key.shape` = $(S, N, E)$\n",
    "    + `value.shape` = $(S, N, E)$\n",
    "  \n",
    "  其中 $N$ 是batch size，$E$ 是 embedding 维度——**它必须等于参数中的`embed_dim`**，这个embedding 维度，既是输入的，也是输出的——**self-attention层输入和输入的特征维度必须一样**  ；  \n",
    "  $S$ 是**输入**序列的长度，$L$ 是**输出**序列的长度——**输入和输出序列的长度可以不一样**.\n",
    "  \n",
    "  + `key_padding_mask`，\n",
    "  + `need_weights`，bool，表示是否需要输出attention的weights\n",
    "  + `attn_mask`，2维或者3维的mask\n",
    "\n",
    "注意，上述的输入中，各个维度顺序必须为(序列长度，batch_size, embedding_dimension)，在之后的版本中，提供了一个\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值\n",
    "  + `attn_output`，shape= $(L, N, E)$，$L$ 由 输入的`query`序列长度决定，$E$ 由输入序列中每个word的特征维度确定\n",
    "  + `attn_output_weights`，shape=$(N, L, S)$，这个权重应该是每个输入序列中，每个word的attention权重."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "permanent-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个 embed_dim 指定的既是输入序列中，每个word的特征维度，也指定了输出序列中，每个word的特征维度\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sunrise-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "E, S, L, N = embed_dim, 20, 10, 5\n",
    "query = torch.rand(L, N, E)\n",
    "key = torch.rand(S, N, E)\n",
    "value = torch.rand(S, N, E)\n",
    "\n",
    "attn_output, attn_output_weights = multihead_attn(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "civilian-dubai",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 512])\n",
      "torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "print(attn_output.shape)\n",
    "print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c975d9-a8ce-4564-be5a-d8f263baa926",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Encoder Layer\n",
    "\n",
    "### 单层Encoder\n",
    "\n",
    "`nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`  \n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer)，它内部包含了一个 self-attention 层 + 一个 feedforward 层.\n",
    "\n",
    "+ 参数\n",
    "  + `d_model`，输入序列中，每个word的特征个数——它同时也决定了输出序列里每个word的特征个数\n",
    "  + `nhead`，multiheadattention中的head个数\n",
    "  + `dim_feedforward`，这个维度设置的是前馈神经网络的节点个数，前馈神经网络的输出节点数还是 `d_model`\n",
    "  + `dropout`，dropout比例，默认 0.1\n",
    "  + \n",
    "  \n",
    "  \n",
    "+ `forward(src, src_mask=None, src_key_padding_mask=None)`方法\n",
    "  + `src`，输入的sequence, `shape` = $(S, N, E)$\n",
    "  + `src_mask`，输入sequence的mask\n",
    "  + `src_key_padding_mask`，每个batch中src的padding\n",
    "  \n",
    "  \n",
    "+ `forward()`方法返回值  \n",
    "  它返回的是和 `src` shape 相同的 tensor.\n",
    "\n",
    "\n",
    "Encoder的内部只有**一层**`MultiheadAttention`：`Encoder.forward()`实际执行时，调用方式为`MultiheadAttention.forward(src, src, src)`——`src`会同时作为 query, key, value 传入 self-attention。\n",
    "\n",
    "\n",
    "\n",
    "### 多层Encoder\n",
    "\n",
    "`TransformerEncoder(encoder_layer, num_layers, norm=None)`\n",
    "+ 参数：\n",
    "  + `encoder_layer`，是一个`TransformerEncoderLayer`类的实例对象\n",
    "  + `num_layers`，指定堆叠的 Encoder 层数\n",
    "+ `forward()`方法和`TransformerEncoderLayer`的一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "variable-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.shape:  torch.Size([32, 10, 512])\n",
      "out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_ffn = 2048\n",
    "batch_size = 10\n",
    "source_seq_len = 32\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=dim_ffn)\n",
    "\n",
    "src = torch.rand(source_seq_len, batch_size , d_model)\n",
    "out = encoder_layer(src)\n",
    "\n",
    "print(\"src.shape: \", src.shape)\n",
    "print(\"out.shape: \", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be2b79-8ce5-439d-88ba-f2837babc56f",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "\n",
    "### 单层Decoder\n",
    "\n",
    "`TransformerDecoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu')`   \n",
    "[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer)\n",
    "\n",
    "+ 参数：\n",
    "  和 Encoder 一致\n",
    "\n",
    "+ `forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)`  \n",
    "  这个方法和 Encoder 不一致，它多了一个 memory.\n",
    "  + `tgt`，输入的序列，`shape`= $(S, N, E)$，\n",
    "  + `memory`，来自 encoder 层（通常是最后一层）的输出, `shape`=$(S, N, E)$.\n",
    "  + `tgt_mask`\n",
    "  + `memory_mask`\n",
    "  + `tgt_key_padding_mask`\n",
    "  + `memory_key_padding_mask`\n",
    "\n",
    "\n",
    "+ `forward()`方法返回值  \n",
    "  它返回的是和 `tgt` shape 相同的 tensor.\n",
    "\n",
    "\n",
    "`Decoder`内部有两层`MultiheadAttention`：\n",
    "  + 第一层调用时为`MultiheadAttention.forward(tgt, tgt, tgt)`\n",
    "  + 第二层调用时为`MultiheadAttention.forward(tgt, memory, memory)`\n",
    "\n",
    "\n",
    "\n",
    "### 多层Decoder\n",
    "\n",
    "`TransformerDecoder(decoder_layer, num_layers, norm=None)`   \n",
    "参数同 `TransformerEncoder` 类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coated-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape:  torch.Size([32, 10, 512])\n",
      "memory.shape:  torch.Size([32, 10, 512])\n",
      "decoder_out.shape:  torch.Size([32, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "nhead = 8\n",
    "dim_ffn = 2048\n",
    "batch_size = 10\n",
    "source_seq_len = 32\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_ffn)\n",
    "\n",
    "tgt = torch.rand(source_seq_len, batch_size, d_model)\n",
    "memory = torch.rand(source_seq_len, batch_size, d_model)\n",
    "\n",
    "decoder_out = decoder_layer(tgt, memory)\n",
    "\n",
    "print(\"tgt.shape: \", tgt.shape)\n",
    "print(\"memory.shape: \", memory.shape)\n",
    "print(\"decoder_out.shape: \", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-logistics",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "# Huggingface-Transformer\n",
    "\n",
    "这里介绍huggingface编写的transformer包，它里面基于transformer结构，实现了各种模型，包括BERT, DistilBERT, GPT2 等等.\n",
    "\n",
    "transformer 包里最重要的三个子模块是：\n",
    "1. [Configuration](https://huggingface.co/transformers/main_classes/configuration.html)  \n",
    "基础的类是 `PretrainedConfig`（好像也只有这一个类），用于 **导出/导入** 模型的配置。\n",
    "\n",
    "2. [Tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html)  \n",
    "包含两个基础的类，用于将文本特征转换成transformer能处理的序列特征：\n",
    "   + `PreTrainedTokenizer`，这个比较慢\n",
    "   + `PreTrainedTokenizerFast`，这个比较快\n",
    "\n",
    "\n",
    "3. [Models](https://huggingface.co/transformers/main_classes/model.html)  \n",
    "基础的类有如下三个，用于构建模型：\n",
    "   + `PreTrainedModel`，pytorch的模型都继承了这个类，继承了 `torch.nn.Module` 类.\n",
    "   + `TFPreTrainedModel`，Tensorflow2.0里的模型都继承了这个类，继承了 `tf.keras.Model` 类.\n",
    "   + `FlaxPreTrainedModel`\n",
    "\n",
    "  \n",
    "上面这三个基础模块，都有如下两个方法用于**导入/导出**对应的配置：\n",
    "+ `.from_pretrained()`，导入预训练模型的 config/tokenizer/model\n",
    "  + `pretrained_model_name_or_path`，指定预训练模型的名称或者本地路径\n",
    "  + `cache_dir`，指定模型的缓存位置\n",
    "  + `force_download`，bool，指定是否强制下载模型\n",
    "  + `local_files_only`，bool，指定是否只使用本地模型\n",
    "  + `mirror`，指定下载的镜像地址\n",
    "+ `.save_pretrained()`，导出训练好模型的 config/tokenizer/model\n",
    "\n",
    "**第一次使用这些语句的时候，如果本地没有对应的模型，会下载这些模型**。\n",
    "  \n",
    "在上面的3个基础子模块之上，还提供了如下几个子模块工具：\n",
    "\n",
    "4. [Pipepline](https://huggingface.co/transformers/main_classes/pipelines.html)  \n",
    "用于快速使用模型，是对上面三个类的封装\n",
    "5. [Trainer](https://huggingface.co/transformers/main_classes/trainer.html)  \n",
    "用于快速训练或者 fine-tune 模型\n",
    "\n",
    "\n",
    "transformer源码中，所有的模型结构都存放在 `src/transformers/models`文件夹下，每个模型对应于一个文件夹（比如BERT对应的就是`bert`文件夹），每个模型的文件夹内，主要有如下几个`.py`文件，其中的 `xxx` 是对应模型的名称：\n",
    "+ **`configuration_xxx.py`——KEY**，编写了该模型对应的`PretrainedConfig`子类，比如BERT就是`BertConfig`。\n",
    "+ **`modeling_xxx.py`——KEY**，存放了pytorch编写的模型结构，要用到的模型类都放在这里面\n",
    "+ `modeling_tf_xxx.py`，存放tensorflow编写的模型结构\n",
    "+ `modeling_flax_xxx.py`\n",
    "+ **`tokenization_xxx.py`——KEY**，用于 分词的 实现类，比如BERT就是`BertTokenizer`类\n",
    "+ `tokenization_xxx_fast.py`，快速分词的实现类\n",
    "+ `convert_*.py`，用于将其它配置文件转换成对应的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-distance",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "## Tokenizer\n",
    "\n",
    "`Tokenizer`类是处理transformer结构输入的主要类，它用于将文本类的数据转换成transformer结构能够接受的序列数据.  \n",
    "\n",
    "+ 所有的模型里，Tokenizer 都有两种实现：\n",
    "  1. 基于python代码的实现，对应于`PreTrainedTokenizer`类\n",
    "  2. 基于Rust Library的**快速**实现，对应于`PreTrainedTokenizerFast`类.\n",
    "\n",
    "\n",
    "+ 上述两个类又都是基于`PreTrainedTokenizerBase`类，它们实现了如下的几个方法：\n",
    "  + 对文本进行分词，并将分词后的word映射到对应词典的id\n",
    "  + 生成特殊的Tokens，比如mask等。\n",
    "\n",
    "\n",
    "+ `Tokenizer`类的`__call__`方法接受的参数如下：\n",
    "  + `text`，类型为`str`, `List[str]`， `List[List[str]]`\n",
    "  + `text_pair`\n",
    "  + `padding`，指定是否补全\n",
    "  + `truncation`，指定是否截断\n",
    "  + `max_length`\n",
    "  + `stride`\n",
    "\n",
    "\n",
    "+ `Tokenizer.__call__()`方法返回的是一个` transformers.tokenization_utils_base.BatchEncoding`类对象，封装了返回的结果，它类似于一个dict，有如下重要的keys:\n",
    "  + `input_ids`，分词后每个 token 对应 vocabulary 的 index —— 这个**作为输入模型的数据**.\n",
    "  + `attention_mask`，表明填充的 token，填充位的 token 对应于 0.\n",
    "  + `token_type_ids`，\n",
    "  \n",
    "  \n",
    "+ `Tokenizer.encode()`方法：将文本转换成 sequences of ids.\n",
    "  + 它接受的参数和 `__call__()` 方法一样\n",
    "  + 返回的是 `List[int]`，表示的是文本分词后tokens 对应的 tokenized Ids.\n",
    "\n",
    "  \n",
    "+ `Tokenizer.tokenize()`方法：将文本转换成分词后的 tokens，\n",
    "  + 返回的是 `List[str]`，表示的是文本对应 **分词后** 的 tokens .\n",
    "\n",
    "\n",
    "+ 还有其他方法：\n",
    "  + `convert_ids_to_tokens()`\n",
    "  + `conver_tokens_to_ids()`\n",
    "  + `conver_tokens_to_string()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "insured-place",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\DataAnalysis\n"
     ]
    }
   ],
   "source": [
    "cd D:\\Projects\\DataAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "packed-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.vocab_size:  30522\n"
     ]
    }
   ],
   "source": [
    "# 导入 BERT tokenizer 的设置\n",
    "from transformers import BertTokenizer\n",
    "model_path = r\"BERT\\bert-pre-trained-models\\bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_path, local_files_only=True)\n",
    "\n",
    "# 查看分词表大小\n",
    "print(\"tokenizer.vocab_size: \", tokenizer.vocab_size)\n",
    "# 查看分词器所使用的的词表\n",
    "vocab = tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nonprofit-sleeping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_words:  ['a', 'titan', 'rt', '##x', 'has', '24', '##gb', 'of', 'vr', '##am']\n",
      "tokenized_ids:  [101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102]\n",
      "decoded_tokens:  [CLS] a titan rtx has 24gb of vram [SEP]\n",
      "\n",
      "batch_encoding: \n",
      " {'input_ids': [101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"A Titan RTX has 24GB of VRAM\"\n",
    "\n",
    "# 查看分词后的 tokens\n",
    "tokenized_words = tokenizer.tokenize(sentence)\n",
    "\n",
    "# 查看分词后的 tokens id\n",
    "tokenized_ids = tokenizer.encode(sentence)\n",
    "\n",
    "# 从 token id 映射回 token\n",
    "decoded_tokens = tokenizer.decode(tokenized_ids)\n",
    "\n",
    "# 直接调用 __call__() 方法，可以得到分词后的所有信息，一步到位\n",
    "batch_encoding = tokenizer(sentence)\n",
    "\n",
    "print(\"tokenized_words: \", tokenized_words)\n",
    "print(\"tokenized_ids: \", tokenized_ids)\n",
    "print(\"decoded_tokens: \", decoded_tokens)\n",
    "print(\"\\nbatch_encoding: \\n\", batch_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conscious-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1.length:  18 ; s2.length:  18\n",
      "s1['input_ids']:\n",
      " [101, 2023, 2003, 1037, 2460, 6251, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s1['attention_mask']:\n",
      " [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "s2['input_ids']:\n",
      " [101, 2023, 2003, 1037, 2738, 2936, 5537, 1012, 2009, 2003, 2012, 2560, 2936, 2084, 1996, 5537, 1015, 102]\n",
      "s2['attention_mask']:\n",
      " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 多个长度不一样的句子要使用 padding，并且由 attention_mask 区分\n",
    "s1 = \"this is a short sentence\"\n",
    "s2 = \"This is a rather longer sequence. It is at least longer than the sequence 1\"\n",
    "\n",
    "batch_encoding = tokenizer([s1, s2], padding=True)\n",
    "\n",
    "print(\"s1.length: \", len(batch_encoding['input_ids'][0]), \"; s2.length: \", len(batch_encoding['input_ids'][1]))\n",
    "print(\"s1['input_ids']:\\n\", batch_encoding['input_ids'][0])\n",
    "print(\"s1['attention_mask']:\\n\", batch_encoding['attention_mask'][0])\n",
    "print(\"s2['input_ids']:\\n\", batch_encoding['input_ids'][1])\n",
    "print(\"s2['attention_mask']:\\n\", batch_encoding['attention_mask'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-beijing",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "## Model\n",
    "\n",
    "transformer的Model基类`PreTrainedModel`有如下的一些属性或方法：\n",
    "+ `.base_model`，返回当前使用的模型，是`torch.nn.Module`类对象\n",
    "+ `get_input_embeddings()`，返回Embedding层，也是一个`nn.Module`类对象\n",
    "+ `get_output_embeddings()`，返回输出层的 embeddings。\n",
    "\n",
    "\n",
    "+ 可用的模型列表见官网 [Pretrained models](https://huggingface.co/transformers/pretrained_models.html).\n",
    "  + BERT模型（名称）有：\n",
    "    + bert-base-uncased\n",
    "    + bert-base-cased\n",
    "    + bert-large-uncased\n",
    "    + bert-large-cased\n",
    "    + bert-base-chinese\n",
    "  + Distil-Bert模型（名称）有：\n",
    "    + distilbert-base-uncased\n",
    "    + distilbert-base-cased\n",
    "\n",
    "我一般常用的两个模型是 **BERT** 和 **Distil-BERT**，所以下面会着重关注这两个模型里的实现。\n",
    "\n",
    "+ [BERT Models](https://huggingface.co/transformers/model_doc/bert.html) 包含如下类（以pytorch为例）\n",
    "  + `BertConfig`，配置类.  \n",
    "  位于`configuration_bert.py`文件中，该文件中也只有这一个配置类\n",
    "  + `BertTokenizer`，实现 `WordPiece` 分词的类.  \n",
    "  位于`tokenization_bert.py`文件中，该文件中还有 `BasicTokenizer` 和 `WordpieceTokenizer` 两个类，不过`BertTokenizer`会调用它们.\n",
    "  + `BertModel`，最基本的 BERT 模型类，它返回的是 BERT 模型的原始结果，包括隐藏层状态.\n",
    "  + `BertForPreTraining`，在 `BertModel`的输出上加了一层处理，以下的几个模型都是如此.\n",
    "  + `BertForMaskedLM`\n",
    "  + `BertForNextSentencePrediction`\n",
    "  + `BertForSequenceClassification`\n",
    "  + 还有其他的一些基础工具类和其他任务对于的BERT模型类，这里就不介绍了。\n",
    "  \n",
    "\n",
    "+ [DistilBERT Models](https://huggingface.co/transformers/model_doc/distilbert.html) 包含如下类:   \n",
    "  ....略\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-senator",
   "metadata": {},
   "source": [
    "### BertModel\n",
    "\n",
    "最基本的BERT架构，输出的是最原始的BERT encoder 状态，没有添加任何多余的层.\n",
    "\n",
    "+ `forward(input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)`方法\n",
    "\n",
    "+ `forward()`方法返回的是`BaseModelOutputWithPoolingAndCrossAttentions`类对象（在`return_dict=True`时），它有如下属性：\n",
    "  + `last_hidden_state`，shape=`(batch_size, sequence_length, hidden_size)`，最后一层的隐状态\n",
    "  + `pooler_output`，shape=`(batch_size, hidden_size)`，最后一层中 `[CLS]` token 对应的输出\n",
    "  + `hidden_states`，只有当`output_hidden_states=True`时才会返回.  \n",
    "  返回的是一个 长度=13 的tuple, 每个元素对应于一层的隐状态（包括了 Embedding 层），每层的隐状态 shape=`(batch_size, sequence_length, hidden_size)`\n",
    "  + `attentions`，只有当`output_attentions=True`时才会返回.  \n",
    "  返回一个 长度=12 的tuple，对应于 12 层的attention，每一层的attention.shape=`(batch_size, num_heads, sequence_length, sequence_length)`，\n",
    "  + `cross_attentions`，只有当`output_attentions=True`时才会返回.  \n",
    "  返回一个 长度=12 的tuple，对应于 12 层的attention，每一层的attention.shape=`(batch_size, num_heads, sequence_length, sequence_length)`，\n",
    "  \n",
    "如果`return_dict=False`，那么返回的就是一个 tuple，其中包含的具体元素要看配置."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gentle-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tired-metadata",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\DataAnalysis'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "useful-vacuum",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./BERT/bert-pre-trained-models/distilbert-base-uncased/ were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ./BERT/bert-pre-trained-models/distilbert-base-uncased/ and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.__class__:     <class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "tokenizer.__class__:  <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "model.__class__:      <class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"./BERT/bert-pre-trained-models/bert-base-uncased\"\n",
    "model_name = \"./BERT/bert-pre-trained-models/distilbert-base-uncased/\"\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, local_files_only=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, local_files_only=True)\n",
    "model = BertModel.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "print(\"config.__class__:    \", config.__class__)\n",
    "print(\"tokenizer.__class__: \", tokenizer.__class__)\n",
    "print(\"model.__class__:     \", model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "resistant-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1996,  2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,\n",
      "          2314,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[CLS] after stealing money from the bank vault [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] the bank robber was seen fishing on the mississippi river bank. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 单个句子\n",
    "# sentence = [\"After stealing money from the bank vault\", \"the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "# 两个句子\n",
    "sentence = [\"After stealing money from the bank vault\", \"the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "# return_tensors 指定返回的结果为 torch.Tensor，这样就不用做转换了，\n",
    "# 注意，必须要设置 padding，否则得到的 tensor 维度不一样，无法转成 tensors\n",
    "sentence_encode = tokenizer(sentence, padding=True, return_tensors='pt')\n",
    "\n",
    "print(sentence_encode)\n",
    "print(tokenizer.decode(sentence_encode['input_ids'][0]))\n",
    "print(tokenizer.decode(sentence_encode['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "anticipated-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将加载的预训练模型置于 evaluation 状态，这样会关闭其中的 dropout\n",
    "model.eval()\n",
    "\n",
    "# 预测时，使用 .no_grad() 方式会加快计算\n",
    "with torch.no_grad():\n",
    "    # 可以直接将 tokenizer 得到的输出作为输入，只要使用拆包技巧就行\n",
    "    outputs = model(**sentence_encode, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "outputs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "higher-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 两个句子-batch_size=2, 每个句子序列的长度 sequence_length = 14, 最后一层的 hidden_size = 768\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "relevant-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对应于 [CLS] token 的 embedding，2 个句子，所以返回了两个\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "frank-consolidation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 隐状态\n",
    "print(len(outputs.hidden_states))\n",
    "outputs.hidden_states.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "wrong-allergy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding 层的 隐状态\n",
    "outputs.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "isolated-order",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个 self-attention 层的 隐状态\n",
    "outputs.hidden_states[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "baking-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention 的信息\n",
    "print(len(outputs.attentions))\n",
    "outputs.attentions.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "protected-event",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 14, 14])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个 self-attention 层的 atttention shape\n",
    "# 2 个句子 batch_size=2, 12 个 heads, sequence_length=14, sequence_length=14\n",
    "outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-attitude",
   "metadata": {},
   "source": [
    "+ 也可以用 切片 的方式，直接从 `outputs` 中获取前两个的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "smoking-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 14, 768])\n",
      "torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# t1 对应于 last_hidden_state, t2 对应于 pooler_output\n",
    "t1, t2 = outputs[:2]\n",
    "print(t1.__class__)\n",
    "print(t1.shape)\n",
    "print(t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "saving-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# return_dict=False，那么返回的就是一个 tuple\n",
    "outputs = model(**sentence_encode, output_hidden_states=True, output_attentions=True, return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lined-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.__class__:  <class 'tuple'>\n",
      "outputs.len:        4\n"
     ]
    }
   ],
   "source": [
    "print(\"outputs.__class__: \", outputs.__class__)\n",
    "print(\"outputs.len:       \", len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "specific-cache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 768])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-trailer",
   "metadata": {},
   "source": [
    "### BertForPreTraining\n",
    "\n",
    "在 BertModel 的输出后，增加了一层 `BertPreTrainingHeads()`，如果要自己训练一个BERT模型，从这个类出发比较方便.\n",
    "\n",
    "+ 通过分析源码可以发现，它（并行）做了如下两件事：\n",
    "  1. 调用 `nn.Linear()` 将 BertModel 输出的 `[CLS]` token 映射成 2 维向量，用于表示二分类的值——用于 Next Sentence 的训练方式\n",
    "  2. 调用 `BertLMPredictionHead()` ，将 BertModel 最后一层输出 last_hidden_state 进行处理，内部处理逻辑如下：\n",
    "    + 调用 `BertPredictionHeadTransform()` 对 last_hidden_state 做 线性映射 + 激活函数 + LayerNormalization\n",
    "    + 调用 `nn.Linear()` 将上一步得到的向量映射成 vocab_size 维度的向量，用于做 MaskLanguageModel 的训练\n",
    "\n",
    "\n",
    "+ `forward()`方法，参数相比于 `BertModel.forward()` 多了如下两个：\n",
    "  + `label`, shape=`(batch_size, sequence_length)`，用于计算 masked language modeling 的 Loss.   \n",
    "  其中的取值范围应为 `[-100, 0, ..., config.vocab_size]`，也就是和 `input_ids` 一样，-100 表示对应位置的token 会被 mask.\n",
    "  + `next_sentence_label`, shape=`(batch_size,)`，用于计算 next sequence prediction(分类问题) 的 Loss.   \n",
    "  取值只有 `{0, 1}`\n",
    "    + 0 表示 后一句 是连着 前一句\n",
    "    + 1 表示 后一句 是随机抽取的\n",
    "\n",
    "\n",
    "+ 返回值是一个封装的 `BertForPreTrainingOutput()` 对象（`return_dict=True`时）\n",
    "  + `loss`，shape=`(1,)`当`labels`提供了时，保存了 MaskLM 的 Loss 和 next sentence prediction 的 Loss\n",
    "  + `prediction_logits`，shape=`(batch_size, sequence_length, config.vocab_size)`，记录了softmax之前的score\n",
    "  + `seq_relationship_logits`, shape=`(batch_size, 2)`，记录了 next sequence prediction 在 softmax 之前的 score\n",
    "  + `hidden_states`，就是 `BertModel` 的 hidden_states\n",
    "  + `attentions`，`BertModel`的对应输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "honey-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "latter-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at ./BERT/bert-pre-trained-models/bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.__class__:     <class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "tokenizer.__class__:  <class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n",
      "model.__class__:      <class 'transformers.models.bert.modeling_bert.BertForPreTraining'>\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./BERT/bert-pre-trained-models/bert-base-uncased\"\n",
    "# model_name = \"./BERT/bert-pre-trained-models/distilbert-base-uncased/\"\n",
    "\n",
    "config = BertConfig.from_pretrained(model_name, local_files_only=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path=model_name, local_files_only=True)\n",
    "model = BertForPreTraining.from_pretrained(model_name, local_files_only=True)\n",
    "\n",
    "print(\"config.__class__:    \", config.__class__)\n",
    "print(\"tokenizer.__class__: \", tokenizer.__class__)\n",
    "print(\"model.__class__:     \", model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "paperback-confidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1996,  2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,\n",
      "          2314,  2924,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# 两个句子\n",
    "sentence = [\"After stealing money from the bank vault\", \"the bank robber was seen fishing on the Mississippi river bank.\"]\n",
    "\n",
    "# return_tensors 指定返回的结果为 torch.Tensor，这样就不用做转换了，\n",
    "# 注意，必须要设置 padding，否则得到的 tensor 维度不一样，无法转成 tensors\n",
    "sentence_encode = tokenizer(sentence, padding=True, return_tensors='pt')\n",
    "print(sentence_encode)\n",
    "\n",
    "outputs = model(**sentence_encode, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "killing-professional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForPreTrainingOutput"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjacent-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cultural-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 30522])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.prediction_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "closing-jaguar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.seq_relationship_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-retreat",
   "metadata": {},
   "source": [
    "### BertForSequenceClassification\n",
    "\n",
    "这个类专门用于 序列分类，它的内部很简单：\n",
    "1. `BertModel()` 输出 pooled_output(`[CLS]` token 对应的 hidden_state)\n",
    "2. `nn.Dropout()` 处理 pooled_output\n",
    "3. `nn.Linear()` 映射到 多分类的类别 logits\n",
    "\n",
    "\n",
    "+ `forward()`方法\n",
    "\n",
    "+ 返回的是`SequenceClassifierOutput()`类对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-there",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow(Python3.7.11)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
