{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.15.0\n",
      "numpy version:  1.19.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "print(\"TensorFlow version: \",tf.__version__)\n",
    "print(\"numpy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量,计算图和会话\n",
    "\n",
    "TF中主要有如下三个部分：  \n",
    "\n",
    "+ 张量——数据模型  \n",
    "张量也就是多维数组，用于表示数据. 张量有三个属性：\n",
    "  1. name\n",
    "  2. shape\n",
    "  3. dtype\n",
    "  \n",
    "\n",
    "+ 计算图——计算模型  \n",
    "计算图是整个计算过程的表示，每个`tf.Graph`包括两个部分：\n",
    "  1. Graph structure.  \n",
    "    + 图中每一个节点表示运算操作，是一个`tf.Operation`\n",
    "    \n",
    "    > Operations describe calculations that consume and produce tensors.\n",
    "    \n",
    "    + 每条边表示运算操作之间的依赖关系，是一个`tf.Tensor`\n",
    "    \n",
    "    > represent the values that will flow through the graph.\n",
    "      \n",
    "  2. Graph collections  \n",
    "  用于存储图的一些元数据。\n",
    "\n",
    "\n",
    "+ 会话——运行模型  \n",
    "计算图中定义的每个运算操作必须在一个会话中执行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow计算图表示如下含义：  \n",
    "+ 每一个节点表示一个计算，或者说操作(operation)\n",
    "+ 节点之间的边描述了计算之间的依赖关系。\n",
    "\n",
    "<img src=\"images/f1.png\" width=\"30%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述的图定义的是如下的运算：  \n",
    "$add = a + b$   \n",
    "对应的TF实现如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "add = tf.add(a,b)\n",
    "print(a)\n",
    "print(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述定义的`add`是表示加法的这个操作，实际上，`a`和`b`也是表示操作——对应的是**常量赋值**这个操作，这就是每个节点是一个计算的含义。 \n",
    "\n",
    "`print`打印的是每个节点对应的操作下，对应输出的张量，可以看出，每个张量打印的时候，输出了张量的三种属性：name, shape, dtype.  \n",
    "\n",
    "节点a和b经过相加后得到节点add，这就是一个依赖关系。\n",
    "\n",
    "因为是定义的计算节点，所以打印出的结果并不是实际的值。**要想打印出计算节点对应的值，必须要开启一个会话，在会话中进行图的计算**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 使用tf.Session()创建一个会话\n",
    "with tf.Session() as sess:\n",
    "    # 使用 session.run()方法实际执行图中的计算节点，传入的参数是某个节点，返回的是该计算节点对应的值\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种执行计算图的方式是使用`tf.InteractiveSession()`开启一个默认的会话，这样就不用每次都指定对话了。  \n",
    "实际上，`tf.InteractiveSession()`相当于\n",
    "```python\n",
    "sess=tf.Session()\n",
    "with sess.as_default():\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# TF中开启用于交互下的默认会话\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 开启后，可以直接使用.eval()方法执行\n",
    "print(a.eval())\n",
    "print(b.eval())\n",
    "print(add.eval())\n",
    "\n",
    "# 关闭默认会话\n",
    "tf.InteractiveSession.close(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 以下是一个矩阵乘法的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 定义两个常量赋值操作\n",
    "a = tf.constant([ [3,3] ])\n",
    "b = tf.constant([ [2], [3] ])\n",
    "\n",
    "# 定义一个矩阵乘法操作\n",
    "prod = tf.matmul(a, b)\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> \n",
      " [[15]]\n"
     ]
    }
   ],
   "source": [
    "# 使用会话来运行上述定义的运算\n",
    "with tf.Session() as sess:\n",
    "    # 调用sess.run()方法来执行上述定义的运算\n",
    "    result = sess.run(prod)\n",
    "    print(result.__class__,\"\\n\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量的使用\n",
    "\n",
    "变量`tf.Variable`有如下作用:  \n",
    "1. 保存神经网络的参数\n",
    "2. 获取神经网络中间的结果\n",
    "\n",
    "变量和张量的区别和联系：  \n",
    "1. 变量是特殊的张量，它们之间的关系，就相当于**张量是常规的值，是不可变的**，而变量可以存储不同的张量。  \n",
    "2. 变量也有张量的三个属性：name, shape, dtype。\n",
    "3. 变量的name和dtype都是不可改变的，shape虽然可以改变，但是很少见到这种用法。\n",
    "\n",
    "\n",
    "有两种方式定义变量：  \n",
    "1. `tf.Variable()`，这是一个类的初始化方法\n",
    "2. `tf.get_variable（）`，这是类的静态方法，**推荐使用这个**。\n",
    "\n",
    "注意： \n",
    "`tf.Variable()`创建变量时，如果指定的名称对应于已存在的变量，**会直接覆盖而不提示**; `tf.get_variable()`在遇到同名变量时，**则会报错提醒。**\n",
    "\n",
    "+ `tf.get_variable(\n",
    "    name, shape=None, dtype=None, initializer=None, regularizer=None,\n",
    "    trainable=None, collections=None, caching_device=None, partitioner=None,\n",
    "    validate_shape=True, use_resource=None, custom_getter=None, constraint=None,\n",
    "    synchronization=tf.VariableSynchronization.AUTO,\n",
    "    aggregation=tf.VariableAggregation.NONE\n",
    ")`  \n",
    "  **Gets an existing variable with these parameters or create a new one.**\n",
    "  + `name`，指定变量名称,必须指定。\n",
    "  + `shape`，指定维度\n",
    "  + `dtype`，指定类型， 也可以省略，TF可以自动推断\n",
    "  + `initializer`，初始化方式，可以是一个Tensor，也可以是\n",
    "  + `regularizer`,可以传入一个正则化函数，同时将该变量加入到`tf.GraphKeys.REGULARIZATION_LOSSES`这个集合里。\n",
    "  + `trainable`，布尔值，如果为True，则会将该变量加入到`tf.GraphKeys.TRAINABLE_VARIABLES`这个集合里。\n",
    "  + `collections`，一个list，包含了想加入的collections名称. 默认为`GraphKeys.GLOBAL_VARIABLES`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 定义一个变量并初始化为0的操作，\n",
    "# 注意，这里只是定义初始化的方法（是用常量初始化还是用随机数初始化），但是并没有执行初始化\n",
    "state = tf.Variable(0, name=\"state\")\n",
    "\n",
    "# 定义一个运算，使var每次加1\n",
    "new_value = tf.add(state, 1)\n",
    "# 上面的方式等价于下面这种\n",
    "# new_value = state + 1\n",
    "\n",
    "# 定义赋值操作\n",
    "update = tf.assign(state, new_value)\n",
    "\n",
    "# 定义变量初始化操作\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 创建会话运行上述定义的操作\n",
    "with tf.Session() as sess:\n",
    "    # 执行变量的初始化操作\n",
    "    sess.run(init)\n",
    "    print(sess.run(state))\n",
    "    for _ in range(5):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有两个需要注意的地方：\n",
    "1. `new_value = tf.add(state,1)` **等价于** ` new_value = state + 1`，详见讨论[In tensorflow what is the difference between tf.add and operator (+)?\n",
    "](https://stackoverflow.com/questions/37900780/in-tensorflow-what-is-the-difference-between-tf-add-and-operator)\n",
    "2. `update = tf.assign(state, new_value)` 和 `state = new_value` 的区别. **有待讨论**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_24:0\", shape=(), dtype=int32)\n",
      "Tensor(\"add_25:0\", shape=(), dtype=int32)\n",
      "2\n",
      "2\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "new_value_1 = tf.add(state,1)\n",
    "new_value_2 = state + 1\n",
    "print(new_value_1)\n",
    "print(new_value_2)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(new_value_1))\n",
    "    print(sess.run(new_value_2))\n",
    "    \n",
    "# print('---------------------------------------')\n",
    "\n",
    "# del update_1\n",
    "# update_1 = tf.assign(state, new_value)\n",
    "# state = new_value\n",
    "# print(update_1)\n",
    "# print(state)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     print(sess.run(update_1))\n",
    "#     print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholder机制，Fetch和Feed\n",
    "\n",
    "TensorFlow 提供了 placeholder 机制用于提供输入数据。  \n",
    "placeholder 相当于定义了一个位置，这个位置中的数据在程序运行时再指定。\n",
    "\n",
    "**placeholder通常用于传入一个batch的训练数据**。\n",
    "\n",
    "Fetch是指一个session中运行多个operation。 \n",
    "\n",
    "Feed是指以字典的形式，向placeholder中传入数据。  \n",
    "\n",
    "上述两个概念都是`Session.run()`方法的参数，可以查阅该函数的帮助文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6277467 -2.845186  -0.6237103]\n",
      " [-1.3152791 -0.8063137  1.9430356]]\n",
      "[[-6.9337564]\n",
      " [ 2.9012003]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2,3]))\n",
    "\n",
    "# 定义一个placeholder需要提供类型（之后不可改变），shape不是必须的，但是提供了shape可以避免歧义和错误\n",
    "x = tf.placeholder(tf.float32, shape=(3,1), name='x')\n",
    "\n",
    "# 使用placeholder代替常量进行矩阵乘法操作的定义\n",
    "y = tf.matmul(w1, x)\n",
    "\n",
    "# 变量初始化操作\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(w1))\n",
    "    # 执行y定义的操作的时候，需要传入placeholder x对应的值\n",
    "    print(sess.run(y, feed_dict={x:[[1],[2],[3]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数\n",
    "\n",
    "[`tf.losses`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/losses)模块封装了各种损失函数的计算operation。  \n",
    "\n",
    "默认情况下，`tf.GraphKeys.LOSSES`中的变量都会当做损失函数的参数，放入到损失函数中进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'w:0' shape=(1, 1) dtype=float32_ref>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'x:0' shape=(1, 4) dtype=float32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'w:0' shape=(1, 1) dtype=float32_ref>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y:0' shape=(1, 4) dtype=float32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'y_pred:0' shape=(1, 4) dtype=float32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_squared_error/value:0' shape=() dtype=float32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3. 4.]]\n",
      "[[-1.6390375]]\n",
      "[[ 1.  4.  9. 16.]]\n",
      "[[-1.6390375 -3.278075  -4.9171124 -6.55615  ]]\n",
      "190.6002\n"
     ]
    }
   ],
   "source": [
    "#----------这是一个线性模型的示例---------\n",
    "# 首先清空默认图的节点\n",
    "tf.reset_default_graph()\n",
    "# 以下的线性模型是 y = w*x ,没有截距项b\n",
    "# x的四个样本点，4 x 1的矩阵\n",
    "x = tf.constant(value=[[1,2,3,4]], dtype=tf.float32, name='x')\n",
    "# w 是 1维 的参数， 1x1 的矩阵\n",
    "w = tf.get_variable(name='w', initializer=tf.random_normal(shape=[1,1]), trainable=True)\n",
    "# y的实际值，对应于x的四个样本点，有四个值，4 x 1 的矩阵\n",
    "y = tf.constant(value=[[1,4,9,16]], dtype=tf.float32, name='y')\n",
    "# 预测值 y_pred = x*w\n",
    "y_pred = tf.matmul(w, x, name='y_pred')\n",
    "\n",
    "# \n",
    "display(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "# display(tf.get_collection(tf.GraphKeys.LOSSES))\n",
    "# 平方损失函数的计算operation\n",
    "mse_loss = tf.losses.mean_squared_error(labels=y, predictions=y_pred)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "display(x)\n",
    "display(w)\n",
    "display(y)\n",
    "display(y_pred)\n",
    "display(mse_loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(x))\n",
    "    print(sess.run(w))\n",
    "    print(sess.run(y))\n",
    "    print(sess.run(y_pred))\n",
    "    print(sess.run(mse_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器  \n",
    "\n",
    "上面定义了损失函数之后，就需要对损失函数进行求梯度，进行梯度下降的优化。  \n",
    "[`tf.train`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train)模块封装了梯度下降的优化步骤。  \n",
    "\n",
    "其中的`tf.train.Optimizer`是所有优化器的基类，最常用的简单梯度下降优化器是`tf.train.GradientDescentOptimizer`。\n",
    "\n",
    "以`tf.train.GradientDescentOptimizer`类为例，以下说明优化器的使用。\n",
    "1. 首先优化器的必要的类构造参数是学习率，也就是梯度下降每一步的步长，以此来初始化类的实例\n",
    "```python\n",
    "tf.train.GradientDescentOptimizer(\n",
    "    learning_rate, use_locking=False, name='GradientDescent'\n",
    ")\n",
    "```\n",
    "2. 使用学习率初始化类，得到一个实例对象之后，在此对象上调用`.minimize()`方法，传入的参数是需要优化的损失函数。\n",
    "```python\n",
    "tf.train.GradientDescentOptimizer.minimize(\n",
    "    loss, global_step=None, var_list=None, gate_gradients=GATE_OP,\n",
    "    aggregation_method=None, colocate_gradients_with_ops=False, name=None,\n",
    "    grad_loss=None\n",
    ")\n",
    "```\n",
    "\n",
    "调用上面的`minimize()`方法时，其实产生了两个步骤：\n",
    "1. 计算有关参数的梯度，实际上是调用了`.compute_gradients()`方法\n",
    "2. 将梯度更新应用到有关参数上，实际上是调用了`.apply_gradients()`方法\n",
    "\n",
    "`.minimize()`方法的返回值是一个更新`var_list`的`tf.Operation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行梯度优化之前的权重和MSE：\n",
      "[[-0.6395329]]\n",
      "123.54417\n",
      "None\n",
      "运行梯度优化之后的权重和MSE：\n",
      "[[2.3401167]]\n",
      "12.565261\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# 以下的线性模型是 y = w*x ,没有截距项b\n",
    "# x的四个样本点，4 x 1的矩阵\n",
    "x = tf.constant(value=[[1,2,3,4]], dtype=tf.float32, name='x')\n",
    "# w 是 1维 的参数， 1x1 的矩阵\n",
    "w = tf.get_variable(name='w', initializer=tf.random_normal(shape=[1,1]), trainable=True)\n",
    "# y的实际值，对应于x的四个样本点，有四个值，4 x 1 的矩阵\n",
    "y = tf.constant(value=[[1,4,9,16]], dtype=tf.float32, name='y')\n",
    "# 预测值 y_pred = x*w\n",
    "y_pred = tf.matmul(w, x, name='y_pred')\n",
    "\n",
    "# 平方损失函数的计算operation\n",
    "mse_loss = tf.losses.mean_squared_error(labels=y, predictions=y_pred)\n",
    "\n",
    "# 定义优化器\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(mse_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"运行梯度优化之前的权重和MSE：\")\n",
    "    print(sess.run(w))\n",
    "    print(sess.run(mse_loss))\n",
    "    \n",
    "    print(sess.run(train_step))\n",
    "    \n",
    "    print(\"运行梯度优化之后的权重和MSE：\")\n",
    "    print(sess.run(w))\n",
    "    print(sess.run(mse_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "\n",
    "可以参见讨论 [How to add regularizations in TensorFlow?](https://stackoverflow.com/questions/37107223/how-to-add-regularizations-in-tensorflow)\n",
    "\n",
    "+ TensorFlow对参数进行正则化的方法有两个：\n",
    "  1. 使用`tf.nn.l2_loss(w)`函数  \n",
    "  这个会直接计算`w`的二范数，返回结果的Tensor\n",
    "  2. 使用` tf.contrib.layers.l2_regularizer(alpha)`方法，但是这个方法在tf1.15中被标记为“弃用”，所以不推荐。  \n",
    "  传入的参数`alpha`是正则化项前面的超参数，之后会返回一个函数，此函数接受的参数才是权重，最后计算得到的是参数的二范数乘上系数`alpha`的Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "7.0\n",
      "[1. 2. 3.]\n",
      "3.5\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 使用 tf.nn.l2_loss()\n",
    "w = tf.constant([1.0,2.0,3.0])\n",
    "reg_l2 = tf.nn.l2_loss # 注意，这里不要加括号，加了就是调用方法了\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(w))\n",
    "    print(sess.run(reg_l2(w)))\n",
    "    \n",
    "    \n",
    "reg_l2 = tf.contrib.layers.l2_regularizer(scale=0.5) #返回的是一个函数\n",
    "w = tf.constant([1.0,2.0,3.0])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(w))\n",
    "    print(sess.run(reg_l2(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述得到的结果就是对向量$(1,2,3)$使用L2正则得到的结果：$\\frac{1}{2}\\times 1.0 \\times(1^2+2^2+3^2)=\\frac{1}{2}\\times 1.0 \\times(1+4+9)=7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 对参数执行正则化的方式有两种\n",
    "1. 使用`tf.get_variable()`创建变量时，通过参数`regularizer=`指定正则化函数.  \n",
    "这种情况下，会自动将正则化后的参数加入到`tf.GraphKeys.REGULARIZATION_LOSSES`集合中.\n",
    "2. 手动计算正则化参数的项，并加入到`tf.GraphKeys.REGULARIZATION_LOSSES`集合中.  \n",
    "使用`tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)`\n",
    "\n",
    "+ 计算完参数的正则化项之和，接下来需要计算正则项的Loss，并将其加入到原本的Loss函数中  \n",
    "这里有两种：\n",
    "1. 手动取出所有的正则化参数，并计算Loss\n",
    "2. 使用` tf.losses.get_regularization_loss()`自动计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.GraphKeys.GLOBAL_VARIABLES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.GraphKeys.TRAINABLE_VARIABLES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.GraphKeys.REGULARIZATION_LOSSES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------\n",
      "tf.GraphKeys.GLOBAL_VARIABLES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'w:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.GraphKeys.TRAINABLE_VARIABLES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'w:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.GraphKeys.REGULARIZATION_LOSSES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'L2Loss:0' shape=() dtype=float32>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "print(\"tf.GraphKeys.GLOBAL_VARIABLES\")\n",
    "display(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "# print(\"tf.GraphKeys.WEIGHTS\")\n",
    "# display(tf.get_collection(tf.GraphKeys.WEIGHTS))\n",
    "print(\"tf.GraphKeys.TRAINABLE_VARIABLES\")\n",
    "display(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "print(\"tf.GraphKeys.REGULARIZATION_LOSSES\")\n",
    "display(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "print('----------------------------------------------------\\n')\n",
    "\n",
    "w = tf.constant([1.0,2.0,3.0])\n",
    "# -- 方式 1 -----------\n",
    "# 创建参数同时，自动将它加入正则化集合里\n",
    "# weight = tf.get_variable(name='w', initializer=w, regularizer=tf.nn.l2_loss)\n",
    "# --- 方式 2 ----------\n",
    "# 创建参数，手动加入正则化集合\n",
    "weight = tf.get_variable(name='w', initializer=w)\n",
    "reg_w = tf.nn.l2_loss(weight)\n",
    "tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, reg_w)\n",
    "\n",
    "# 接下来提取 正则化参数的 Loss，手动计算loss\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "reg_loss = 1 * tf.add_n(reg_variables)\n",
    "# 或者直接得到正则化参数的loss\n",
    "reg_loss_auto = tf.losses.get_regularization_loss()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    display(sess.run(weight))\n",
    "    display(sess.run(reg_loss))\n",
    "    display(sess.run(reg_loss_auto))\n",
    "\n",
    "print('\\n----------------------------------------------------')\n",
    "print(\"tf.GraphKeys.GLOBAL_VARIABLES\")\n",
    "display(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "# print(\"tf.GraphKeys.WEIGHTS\")\n",
    "# display(tf.get_collection(tf.GraphKeys.WEIGHTS))\n",
    "print(\"tf.GraphKeys.TRAINABLE_VARIABLES\")\n",
    "display(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))\n",
    "print(\"tf.GraphKeys.REGULARIZATION_LOSSES\")\n",
    "display(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.math`下的常用运算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "reduce_xx方法\n",
      "21.0\n",
      "[5. 7. 9.]\n",
      "[ 6. 15.]\n",
      "\n",
      "add_n方法\n",
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "[[ 2.  4.  6.]\n",
      " [ 8. 10. 12.]]\n",
      "\n",
      "argmax方法\n",
      "[1 1 1]\n",
      "[1 1 1]\n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "value = [[1.0, 2.0, 3.0],\n",
    "     [4.0, 5.0, 6.0]]\n",
    "x = tf.constant(value)\n",
    "\n",
    "y1 = tf.constant(value)\n",
    "y2 = tf.constant(value)\n",
    "y3 = y1+y2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(x.eval())\n",
    "    \n",
    "    # reduce 类函数\n",
    "    print(\"\\nreduce_xx方法\")\n",
    "    print( tf.reduce_sum(x).eval() )\n",
    "    print( tf.reduce_sum(x, axis=0).eval() )\n",
    "    print( tf.reduce_sum(x, axis=1).eval() )\n",
    "    \n",
    "    # add_n函数\n",
    "    print(\"\\nadd_n方法\")\n",
    "    print( tf.add_n([y1,y2]).eval() )\n",
    "    print( y3.eval())\n",
    "    \n",
    "    # argmax函数\n",
    "    print(\"\\nargmax方法\")\n",
    "    print(tf.argmax(input=x).eval())\n",
    "    print(tf.argmax(input=x, axis=0).eval())\n",
    "    print(tf.argmax(input=x, axis=1).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单使用案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.054953318, 0.10058578]\n",
      "20 [0.10441639, 0.19757646]\n",
      "40 [0.102669686, 0.19853504]\n",
      "60 [0.10161378, 0.19911446]\n",
      "80 [0.1009755, 0.19946471]\n",
      "100 [0.10058967, 0.19967642]\n",
      "120 [0.10035645, 0.1998044]\n",
      "140 [0.10021548, 0.19988175]\n",
      "160 [0.100130245, 0.19992852]\n",
      "180 [0.100078724, 0.1999568]\n",
      "200 [0.10004757, 0.1999739]\n"
     ]
    }
   ],
   "source": [
    "# 构造100个数据点\n",
    "x_data = np.random.rand(100)\n",
    "y_data = x_data * 0.1 + 0.2\n",
    "\n",
    "#构造一个带训练的线性模型\n",
    "a = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "y = a * x_data + b\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化代价函数\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 构建session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(201):\n",
    "        sess.run(train)\n",
    "        if step%20 == 0:\n",
    "            print(step, sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x2695e70b908>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取默认的计算图\n",
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的神经网络程序示例\n",
    "\n",
    "有三个步骤：  \n",
    "1. 定义神经网络的结构和前向传播的输出结果。\n",
    "2. 定义损失函数以及选择反向传播优化的算法 。\n",
    "3. 生成会话（ tf.Session ）并且在训练、数据上反复运行反向传播优化算法\n",
    "\n",
    "作为示例的神经网络如下图所示：  \n",
    "**它只有一个隐藏层，并且隐藏层没有使用激活函数。**\n",
    "\n",
    "<img src=\"images/f2.png\" width=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86375999 0.28490597]\n",
      " [0.07325639 0.7632372 ]\n",
      " [0.45271906 0.54229687]\n",
      " [0.72663578 0.84890511]\n",
      " [0.76819998 0.73314372]]\n",
      "[[0], [1], [1], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "# 使用numpy随机模拟数据\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rnd = RandomState(29)\n",
    "datasize = 128\n",
    "# 随机生成 datesize x 2 的矩阵\n",
    "X = rnd.rand(datasize,2)\n",
    "# 对于X中的每一行，计算一个对于的 Y 值，\n",
    "Y = [ [int(x1+x2<1)] for (x1,x2) in X]\n",
    "\n",
    "print(X[:5,:])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda\\envs\\tensorflow1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "w1:\n",
      "[[ 0.28952682  1.9308614   0.5419188 ]\n",
      " [-0.11188115 -2.566612   -0.41635063]]\n",
      "w2:\n",
      "[[-0.64926267]\n",
      " [-0.8263266 ]\n",
      " [ 0.8473202 ]]\n",
      "经过 0 次训练，所有数据上的交叉熵损失为：0.617352\n",
      "经过 500 次训练，所有数据上的交叉熵损失为：0.224531\n",
      "经过 1000 次训练，所有数据上的交叉熵损失为：0.0864119\n",
      "经过 1500 次训练，所有数据上的交叉熵损失为：0.04642\n",
      "经过 2000 次训练，所有数据上的交叉熵损失为：0.0299731\n",
      "经过 2500 次训练，所有数据上的交叉熵损失为：0.0212891\n",
      "经过 3000 次训练，所有数据上的交叉熵损失为：0.0159384\n",
      "经过 3500 次训练，所有数据上的交叉熵损失为：0.0123008\n",
      "经过 4000 次训练，所有数据上的交叉熵损失为：0.00967179\n",
      "经过 4500 次训练，所有数据上的交叉熵损失为：0.00769481\n",
      "w1\n",
      "[[-1.5155436  0.6918462  2.008162 ]\n",
      " [-1.7760524 -3.6705172  0.9140369]]\n",
      "w2\n",
      "[[-3.071673 ]\n",
      " [-1.686208 ]\n",
      " [ 2.6975398]]\n"
     ]
    }
   ],
   "source": [
    "#---------步骤1--------------\n",
    "# 定义训练数据batch大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络的参数\n",
    "# w1是输入层到隐藏层的权重矩阵, 2 x 3\n",
    "w1 = tf.Variable(tf.random_normal([2,3]))\n",
    "# w2是隐藏层到输出层的权重大小，3 x 1\n",
    "w2 = tf.Variable(tf.random_normal([3,1]))\n",
    "\n",
    "#定义placeholder，用于接受数据\n",
    "# shape里第一个维度留None，可以用于动态适应输入的batch大小\n",
    "x_batch = tf.placeholder(tf.float32, shape=(None, 2), name=\"x_batch\")\n",
    "y_batch = tf.placeholder(tf.float32, shape=(None, 1), name=\"y_batch\")\n",
    "\n",
    "\n",
    "#---------步骤2--------------\n",
    "#定义神经网络的前向传播过程，这里采用矩阵运算\n",
    "a = tf.matmul(x_batch, w1)\n",
    "y = tf.matmul(a, w2)  # 这个 y 是神经网络输出的 y\n",
    "\n",
    "# 定义损失函数和反向传播的优化算法\n",
    "y = tf.sigmoid(y)\n",
    "# 使用交叉熵作为损失函数\n",
    "cross_entropy = - tf.reduce_mean(\n",
    "    y_batch * tf.log(tf.clip_by_value(y, 1e-10, 10)) +\n",
    "    (1-y_batch) * tf.log(tf.clip_by_value(y,1e-10, 10))\n",
    ")\n",
    "\n",
    "# 优化算法\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
    "\n",
    "#---------步骤3--------------\n",
    "# 开启会话，进行训练\n",
    "with tf.Session() as sess:\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # 打印训练之前权重矩阵的值\n",
    "    print(\"w1:\")\n",
    "    print(sess.run(w1))\n",
    "    print(\"w2:\")\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    # 设定训练轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        # 每次选择batch_size个样本进行运算，这里生成的是样本的index范围\n",
    "        batch_start = (i * batch_size) % datasize\n",
    "        batch_end = min(batch_start+batch_size, datasize)\n",
    "        \n",
    "        # 对每个batch进行训练并更新参数\n",
    "        sess.run(train_step, feed_dict={x_batch:X[batch_start:batch_end,:], y_batch:Y[batch_start:batch_end]})\n",
    "        \n",
    "        # 每隔一段时间就打印一次训练过程中全部数据集上的交叉熵\n",
    "        if i%500 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy , feed_dict={x_batch:X, y_batch:Y})\n",
    "            print(\"经过 {} 次训练，所有数据上的交叉熵损失为：{:g}\".format(i, total_cross_entropy))\n",
    "            \n",
    "    # 打印训练之后权重矩阵的值\n",
    "    print(\"w1\")\n",
    "    print(sess.run(w1))\n",
    "    print(\"w2\")\n",
    "    print(sess.run(w2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型持久化\n",
    "\n",
    "TensorFlow提供了`tf.train.Saver`类来对计算图进行保存和恢复.\n",
    "\n",
    "保存后，会在指定文件夹下产生三个文件：\n",
    "1. `xxx.ckpt.meta`文件，保存的计算图结构\n",
    "2. `xxx.ckpt`或者`xxx.ckpt.index`文件，保存计算图中所有变量的取值\n",
    "3. `xxx.checkpoint`文件，记录的当前文件夹下所保存的所有模型文件列表\n",
    "\n",
    "+ 保存模型的函数\n",
    "`tf.train.Saver.save(\n",
    "    sess, save_path, global_step=None, latest_filename=None,\n",
    "    meta_graph_suffix='meta', write_meta_graph=True, write_state=True,\n",
    "    strip_default_attrs=False, save_debug_info=False\n",
    ")`\n",
    "  + `sess`，运行计算图的会话\n",
    "  + `save_path`，保存模型的路径，需要包含保存的模型名称\n",
    "  + `global_step`，如果提供了步长参数，则会在保存的模型后面加上步长作为后缀\n",
    "  \n",
    "+ 恢复模型的函数\n",
    "`restore(sess, save_path)`\n",
    "  + `sess`，当前使用的会话\n",
    "  + `save_path`，模型保存的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis\\\\Practice'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(tf.constant(1.0, shape=[1]), name='v1')\n",
    "v2 = tf.Variable(tf.constant(2.0, shape=[1]), name='v2')\n",
    "result = v1 + v2\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 实例化保存模型的类，相当于创建了一个保存计算图的 tf.Opeartion\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(result))\n",
    "    saver.save(sess, \"./TensorFlow-Saved-Models/model-save-test.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已保存的模型\n",
    "saver = tf.train.Saver()\n",
    "ckpt = tf.train.get_checkpoint_state(\"./TensorFlow-Saved-Models/\")\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"./TensorFlow-Models/model-save-test.ckpt.index\")\n",
    "#     print(sess.run(tf.get_default_graph().get_tensor_by_name(\"v1:0\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./TensorFlow-Saved-Models/mnist_dnn_model.cpkt-40000'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.model_checkpoint_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow提供的网络结构 \n",
    "\n",
    "TensorFlow有4个module提供网络结构：\n",
    "1. `tf.nn`——这是最低层的网络结构API\n",
    "2. `tf.layers`——它是对`tf.nn`的封装，高一级的API\n",
    "3. `tf.keras`——keras封装，\n",
    "4. `tf.contrib`——这个在1.15版本中已经被废弃了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0320586  0.08714432 0.23688284 0.6439143 ]\n",
      "[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# TF自带的softmax层\n",
    "w = tf.constant([1.,2.,3.,4.])\n",
    "res = tf.nn.softmax(w)\n",
    "\n",
    "# 手动编写的sotfmax计算过程\n",
    "w_ = np.array([1,2,3,4])\n",
    "res_ = np.exp(w_)/np.exp(w_).sum()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    res_value = sess.run(res)\n",
    "    print(res_value)\n",
    "    print(res_)\n",
    "    print(res_value.sum())\n",
    "    print(res_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层\n",
    "\n",
    "+ 卷积层函数  \n",
    "`tf.nn.conv2d(\n",
    "    input, filter=None, strides=None, padding=None, use_cudnn_on_gpu=True,\n",
    "    data_format='NHWC', dilations=[1, 1, 1, 1], name=None, filters=None\n",
    ")`\n",
    "  + `input`，输入层tensor，通常为4维, `[batch, in_height, in_width, in_channels]`\n",
    "  + `filter`，指定卷积核的尺寸，必须是一个 **4维 tensor，每个维度对应于 `[filter_height, filter_width, in_channels, out_channels]`**\n",
    "  + `strides`，指定在input上各个维度滑动的步长. 是一个 int 或者 长度为 1,2,4 的 int list.  \n",
    "    + 如果是 `a`, 表示在图像的 height 和 width 维度上使用步长为 a 的滑动；\n",
    "    + 如果是 `[a, b]`，表示在图像的 heigth 上滑动步长为 a, width 上为 b;\n",
    "    + 如果是 `[a, b, c, d]`，表示input的四个维度分别指定，不过一般 height 和 width 上都设置为 1.\n",
    "  + `padding`，表示是否边界0填充. 取值`SAME`和`VALID`，\n",
    "  + `data_format`，字符串，从\"NHWC\", \"NCHW\"中选择。指定了各个维度的顺序。\n",
    "  + `filters`，`filter`的别名\n",
    "  \n",
    "  它做了如下的步骤：\n",
    "  1. 将shape=`[filter_height, filter_width, in_channels, out_channels]`的filter压缩成一个 2维矩阵，shape=`[filter_height * filter_width * in_channels, output_channels]`\n",
    "  2. 将输入的input tensor转换成shape=`[batch, out_height, out_width, filter_height * filter_width * in_channels]`的tensor。\n",
    "  3. 对于每一个batch，执行矩阵乘法。\n",
    "  \n",
    "  \n",
    "\n",
    "+ 偏置项函数  \n",
    "`tf.nn.bias_add(value, bias, data_format=None, name=None)`用于向卷积结果里添加偏置项\n",
    "  + `value`，输入的tensor,通常是`tf.nn.conv2d`的结果\n",
    "  + `bias`，一维的tensor，长度对应于 `value`的channel维。\n",
    "  + `data_format`，维度顺序的含义\n",
    "  \n",
    "  \n",
    "+ 池化层函数  \n",
    "`tf.nn.max_pool2d(input, ksize, strides, padding, data_format='NHWC', name=None)`\n",
    "  + `input`，一个 4维 tensor，`[batch, in_height, in_width, in_channels]`\n",
    "  + `ksize`，An int or list of ints that has length 1, 2 or 4，指定池化层窗口大小。\n",
    "  + `strides`，池化层步长。\n",
    "  + `padding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始的图像数据：2张图片，图片channel=3, 大小 4x4:\n",
      "(2, 3, 4, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]],\n",
       "\n",
       "        [[16., 17., 18., 19.],\n",
       "         [20., 21., 22., 23.],\n",
       "         [24., 25., 26., 27.],\n",
       "         [28., 29., 30., 31.]],\n",
       "\n",
       "        [[32., 33., 34., 35.],\n",
       "         [36., 37., 38., 39.],\n",
       "         [40., 41., 42., 43.],\n",
       "         [44., 45., 46., 47.]]],\n",
       "\n",
       "\n",
       "       [[[48., 49., 50., 51.],\n",
       "         [52., 53., 54., 55.],\n",
       "         [56., 57., 58., 59.],\n",
       "         [60., 61., 62., 63.]],\n",
       "\n",
       "        [[64., 65., 66., 67.],\n",
       "         [68., 69., 70., 71.],\n",
       "         [72., 73., 74., 75.],\n",
       "         [76., 77., 78., 79.]],\n",
       "\n",
       "        [[80., 81., 82., 83.],\n",
       "         [84., 85., 86., 87.],\n",
       "         [88., 89., 90., 91.],\n",
       "         [92., 93., 94., 95.]]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "经过卷积后的数据：2张图片，新的channel=2, 卷积之后的图片大小 3x3\n",
      "(2, 2, 3, 3)\n",
      "[[[[108. 114. 120.]\n",
      "   [132. 138. 144.]\n",
      "   [156. 162. 168.]]\n",
      "\n",
      "  [[114. 120. 126.]\n",
      "   [138. 144. 150.]\n",
      "   [162. 168. 174.]]]\n",
      "\n",
      "\n",
      " [[[396. 402. 408.]\n",
      "   [420. 426. 432.]\n",
      "   [444. 450. 456.]]\n",
      "\n",
      "  [[402. 408. 414.]\n",
      "   [426. 432. 438.]\n",
      "   [450. 456. 462.]]]]\n",
      "\n",
      "添加bias后：2张图片，第1个channel +0, 第2个channel +1, 图片大小不变，仍为 3x3\n",
      "(2, 2, 3, 3)\n",
      "[[[[108. 114. 120.]\n",
      "   [132. 138. 144.]\n",
      "   [156. 162. 168.]]\n",
      "\n",
      "  [[115. 121. 127.]\n",
      "   [139. 145. 151.]\n",
      "   [163. 169. 175.]]]\n",
      "\n",
      "\n",
      " [[[396. 402. 408.]\n",
      "   [420. 426. 432.]\n",
      "   [444. 450. 456.]]\n",
      "\n",
      "  [[403. 409. 415.]\n",
      "   [427. 433. 439.]\n",
      "   [451. 457. 463.]]]]\n",
      "\n",
      "经过最大池化层之后：2张图片, channel=2, 大小变为 2x2\n",
      "(2, 2, 2, 2)\n",
      "[[[[138. 144.]\n",
      "   [162. 168.]]\n",
      "\n",
      "  [[145. 151.]\n",
      "   [169. 175.]]]\n",
      "\n",
      "\n",
      " [[[426. 432.]\n",
      "   [450. 456.]]\n",
      "\n",
      "  [[433. 439.]\n",
      "   [457. 463.]]]]\n"
     ]
    }
   ],
   "source": [
    "# 输入的是 2张 4x4x3 的图片，这里为了容易辨别，将通道数3提前了一点\n",
    "imgs = np.arange(2*4*4*3,dtype=np.float32).reshape((2,3,4,4))\n",
    "imgs_tensor = tf.constant(imgs)\n",
    "# display(imgs)\n",
    "\n",
    "# 卷积核为 2x2x3x2，步长为1，不做padding，过滤器数量为2\n",
    "# 2个过滤器，分别为 [[1,0], [1,0]], [[0,1],[0,1]]三个通道都是一样的\n",
    "f_w = np.array([1,1,0,0]*3 + [0,0,1,1]*3, dtype=np.float32).reshape((2,2,3,2), order='F')\n",
    "# display(f_w[:,:,0,0])\n",
    "filter_weight = tf.constant(f_w)\n",
    "\n",
    "# 卷积层\n",
    "# 因为imgs中将channel提前了，所以这里也要相应的设置data_format参数\n",
    "conv = tf.nn.conv2d(input=imgs_tensor, filter=filter_weight, strides=1, padding='VALID', data_format='NCHW')\n",
    "\n",
    "# 偏置\n",
    "# 因为卷积核中有2个filter，所以新的channel=2，这里的偏置长度也是2\n",
    "bias = tf.constant([0.0, 1.0])\n",
    "conv_bias = tf.nn.bias_add(value=conv, bias=bias, data_format='NCHW')\n",
    "\n",
    "# 最大池化层\n",
    "# 池化层窗口为2x2,\n",
    "conv_max_pool = tf.nn.max_pool2d(input=conv_bias, ksize=2, strides=1, padding='VALID', data_format='NCHW')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"原始的图像数据：2张图片，图片channel=3, 大小 4x4:\")\n",
    "print(imgs.shape)\n",
    "display(imgs)\n",
    "\n",
    "print(\"\\n经过卷积后的数据：2张图片，新的channel=2, 卷积之后的图片大小 3x3\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(conv.shape)\n",
    "    print(sess.run(conv))\n",
    "    print(\"\\n添加bias后：2张图片，第1个channel +0, 第2个channel +1, 图片大小不变，仍为 3x3\")\n",
    "    print(conv_bias.shape)\n",
    "    print(sess.run(conv_bias))\n",
    "    print(\"\\n经过最大池化层之后：2张图片, channel=2, 大小变为 2x2\")\n",
    "    print(conv_max_pool.shape)\n",
    "    print(sess.run(conv_max_pool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉熵\n",
    "\n",
    "`tf.nn.softmax_cross_entropy_with_logits_V2(labels=, logits=)`可以对实际值和预测值做如下操作：\n",
    "1. 先对预测值`logits`做softmax，转换成概率分布\n",
    "2. 然后对两者的每个样本计算交叉熵损失\n",
    "\n",
    "最终返回的是每个样本的带softmax的交叉熵损失，shape是`(batch_size,)`\n",
    "\n",
    "如果样本的真实分类`labels`是one-hot编码，也就是每个样本的labels里只有一个是1，其他是0，此时可以使用`tf.nn.sparse_softmax_cross_entropy_with_logits`这个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.407606  , 0.407606  , 0.40760598], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.2228177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([2.407606  , 0.40760595, 0.40760595], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.2228177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3个样本，每个样本可以取的分类是 3 种—— batch_size=3, class_num=3\n",
    "y_output = [[1.0, 2.0, 3.0],\n",
    "            [2.0, 3.0, 1.0],\n",
    "            [2.0, 1.0, 3.0]]\n",
    "# 三个样本的实际类别\n",
    "y_true = [[1.0, 0.0, 0.0],\n",
    "          [0.0, 1.0, 0.0],\n",
    "          [0.0, 0.0, 1.0]]\n",
    "\n",
    "# 代表实际输出的tensor\n",
    "y_output_tensor = tf.constant(y_output)\n",
    "# 代表真实样本类型的tensor\n",
    "y_true_tensor = tf.constant(y_true)\n",
    "\n",
    "# 首先对神经网络的原始输出做softmax，转换成概率，得到的也是 batch_size x class_num 的tensor\n",
    "y_soft=tf.nn.softmax(y_output_tensor)\n",
    "# 手动计算每个样本的交叉熵，\n",
    "y_entropy = y_true_tensor*tf.log(y_soft)\n",
    "# 这里本来应当是先求每个样本的交叉熵——每行求一次和，然后计算所有样本的交叉熵之和——列求和，没有指定维度，所以一口气全做了\n",
    "cross_entropy_manual = -tf.reduce_sum( y_entropy )\n",
    "cross_entropy_manual_vector = -tf.reduce_sum( y_entropy, axis=1 )\n",
    "\n",
    "# 使用softmax_cross_entropy_with_logits_V2\n",
    "# labels是真实值，logits是未经softmax之前的输出值\n",
    "# 注意，它返回的是 batch_size长度的vector，也就是每个样本的单独交叉熵\n",
    "cross_entropy_vector = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true_tensor, logits=y_output_tensor)\n",
    "# 计算所有样本的交叉熵之和\n",
    "cross_entropy_auto = tf.reduce_sum(cross_entropy_vector)\n",
    "\n",
    "# 下面可以看出，两种方式是一样的\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     display(y_soft.eval())\n",
    "#     display(y_entropy.eval())\n",
    "    display(cross_entropy_manual_vector.eval())\n",
    "    display(cross_entropy_manual.eval())\n",
    "    display(cross_entropy_vector.eval())\n",
    "    display(cross_entropy_auto.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3), Dimension(3)])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算图和会话细节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建计算图\n",
    "\n",
    "一般来说，TensorFlow会有一个默认的计算图，没有特殊说明的情况下，所有的操作都在这个默认的计算图上进行。  \n",
    "比如：\n",
    "1. `tf.constant(42.0)`这个操作，会生成一个`tf.Operation`的节点，并将这个节点加入到默认的计算图中，同时返回一个`tf.Tensor`——存储了42这个值。\n",
    "2. `tf.matmul(x,y)`这个操作，会生成一个`tf.Operation`的节点，它会将`x`和`y`的值进行矩阵相乘，并将此节点加入默认的计算图，同时返回一个`tf.Tensor`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图的命名空间\n",
    "\n",
    "每个计算图都维护了一个命名空间，其中包含了所有和此计算图相关的`tf.operation`.  \n",
    "\n",
    "计算图中的每个`tf.Operation`的名称都是唯一的，如果创建的时候有重复，就会在后面加上一个`_1`,`_2`的后缀.  \n",
    "```python\n",
    "c_0 = tf.constant(0, name=\"c\")  # => operation named \"c\"\n",
    "\n",
    "# Already-used names will be \"uniquified\".\n",
    "c_1 = tf.constant(2, name=\"c\")  # => operation named \"c_1\"\n",
    "```\n",
    "\n",
    "\n",
    "**可以通过`tf.name_scope()`来显式创建一个命名空间**，它会在operation前加上一个`/`的前缀，具体规则如下：\n",
    "```python\n",
    "# Name scopes add a prefix to all operations created in the same context.\n",
    "with tf.name_scope(\"outer\"):\n",
    "  c_2 = tf.constant(2, name=\"c\")  # => operation named \"outer/c\"\n",
    "\n",
    "  # Name scopes nest like paths in a hierarchical file system.\n",
    "  with tf.name_scope(\"inner\"):\n",
    "    c_3 = tf.constant(3, name=\"c\")  # => operation named \"outer/inner/c\"\n",
    "\n",
    "  # Exiting a name scope context will return to the previous prefix.\n",
    "  c_4 = tf.constant(4, name=\"c\")  # => operation named \"outer/c_1\"\n",
    "\n",
    "  # Already-used name scopes will be \"uniquified\".\n",
    "  with tf.name_scope(\"inner\"):\n",
    "    c_5 = tf.constant(5, name=\"c\")  # => operation named \"outer/inner_1/c\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量集合\n",
    "\n",
    "由于需要对计算图中不同的节点所表示的`tf.Opertion`进行管理，TF提供了variable collection机制——每个collection就是一系列Tensor或者变量的名称列表。\n",
    "\n",
    "默认下，每个`tf.Variable`会被加入到下面两个集合\n",
    "+ `tf.GraphKeys.GLOBAL_VARIABLES` --- variables that can be shared across multiple devices.\n",
    "+ `tf.GraphKeys.TRAINABLE_VARIABLES` --- variables for which TensorFlow will calculate gradients. 也就是进行训练的参数\n",
    "\n",
    "常用的一些变量集合如下，它们都在`tf.GraphKeys`中：\n",
    "+ `GLOBAL_VARIABLES`: 该collection默认加入所有的Variable对象，并且在分布式环境中共享。  \n",
    "一般来说，TRAINABLE_VARIABLES包含在MODEL_VARIABLES中，MODEL_VARIABLES包含在GLOBAL_VARIABLES中。\n",
    "+ `LOCAL_VARIABLES`: 与GLOBAL_VARIABLES不同的是，它只包含本机器上的Variable，即不能在分布式环境中共享。\n",
    "+ `MODEL_VARIABLES`: 顾名思义，模型中的变量，在构建模型中，所有用于正向传递的Variable都将添加到这里。\n",
    "+ `TRAINALBEL_VARIABLES`: 所有用于反向传递的Variable，即可训练(可以被optimizer优化，进行参数更新)的变量。\n",
    "+ `SUMMARIES`: 跟Tensorboard相关，这里的Variable都由tf.summary建立并将用于可视化。\n",
    "+ `QUEUE_RUNNERS`: the QueueRunner objects that are used to produce input for a computation.\n",
    "+ `MOVING_AVERAGE_VARIABLES`: the subset of Variable objects that will also keep moving averages.\n",
    "+ `REGULARIZATION_LOSSES`: regularization losses collected during graph construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'variables'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.GLOBAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'train/w1:0' shape=(784, 500) dtype=float32_ref>,\n",
       " <tf.Variable 'train/w2:0' shape=(500, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'train/b1:0' shape=(500,) dtype=float32_ref>,\n",
       " <tf.Variable 'train/b2:0' shape=(10,) dtype=float32_ref>,\n",
       " <tf.Variable 'train/Variable:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'train_1/Variable:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'train_2/Variable:0' shape=() dtype=int32_ref>,\n",
       " <tf.Variable 'train_3/Variable:0' shape=() dtype=int32_ref>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过此函数获取每个集合所包含的变量\n",
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将变量加入到某个集合\n",
    "# tf.add_to_collection(\"my_collection_name\", my_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量作用域和共享\n",
    "\n",
    "详见官方文档[https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/variables.md#sharing-variables](Sharing variables).  \n",
    "\n",
    "通过`tf.variable_scope()`创建一个变量作用域，此后生成并操作的`tf.operation`都在这个作用域中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 会话和硬件设备\n",
    "\n",
    "`tf.Session`提供了对实际硬件设备的访问，这个硬件设备可以是本地机器上的运行环境，也可以是远程的分布式集群运行环境。\n",
    "\n",
    "每个计算图都必须在一个Session中执行，使用`tf.Session().run()`方法来执行计算图中的某个子节点所表示的`tf.Opertaion`，它会返回对应的Tensor的值。\n",
    "\n",
    "**注意，一个Session一旦执行完一个计算图，关闭之后，就再也无法获取到这个计算图里的信息了，必须要将其中的内容保存下来才可以看到**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算设备管理\n",
    "\n",
    "Tensorflow为了在执行操作的时候，充分利用计算资源，可以明确指定操作在哪个设备上执行。  \n",
    "一般情况下，不需要显示指定使用CPU还是GPU，TensorFlow会自动检测。如果检测到GPU，TensorFlow会尽可能地利用第一个GPU来执行操作。  \n",
    "注意：**如果机器上有超过一个可用的GPU，那么除了第一个外其它GPU默认是不参与计算的** .  \n",
    "所以，在实际TensorFlow编程中，经常需要明确给定使用的CPU和GPU。\n",
    "\n",
    "TensorFlow给设备的命名方式为：\n",
    "1. \"/cpu:0\"：表示使用CPU计算，**默认下，即使有多个CPU，TensorFlow也不会做区分，统一使用此名称**.\n",
    "2. \"/gpu:0\"：表示使用第一个GPU运算，如果有的话\n",
    "3. \"/gpu:1\"：表示使用第二个GPU运算，以此类推\n",
    "\n",
    "\n",
    "TensorFlow提供了`tf.device()`方法对设备进行管理，设备的具体命名指定方式见官方文档[Placing operations on different devices](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/graphs.md#placing-operations-on-different-devices) :    \n",
    "`/job:<JOB_NAME>/task:<TASK_INDEX>/device:<DEVICE_TYPE>:<DEVICE_INDEX>`  \n",
    "+ `<JOB_NAME>` is an alpha-numeric string that does not start with a number.\n",
    "+ `<DEVICE_TYPE>` is a registered device type (such as GPU or CPU).\n",
    "+ `<TASK_INDEX>` is a non-negative integer representing the index of the task in the job named <JOB_NAME>. See tf.train.ClusterSpec for an explanation of jobs and tasks.\n",
    "+ `<DEVICE_INDEX>` is a non-negative integer representing the index of the device, for example, to distinguish between different GPU devices used in the same process.\n",
    "\n",
    "\n",
    "+ 管理设备\n",
    "  + 要验证TensorFlow正在使用的设备，可以在创建会话时，指定配置为`config=tf.ConfigProto(log_device_placement=True)`，运行的时候就会打印出参与运算的设备；\n",
    "  + 要指定使用某个设备，可以使用`with tf.device():`上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "[2. 4. 6.]\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "[]\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "[2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# 打印设备使用日志\n",
    "a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')\n",
    "c = a + b\n",
    "with tf.Session( config=tf.ConfigProto(log_device_placement=True) ) as sess:\n",
    "    print( sess.run(c) )\n",
    "\n",
    "print(\"\\n-----------------------------------\\n\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "# config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "# 指定使用CPU进行计算\n",
    "with tf.device('/CPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0], shape=[3])\n",
    "    b = tf.constant([1.0, 2.0, 3.0], shape=[3])\n",
    "    c = a + b\n",
    "    with tf.Session( config=tf.ConfigProto(log_device_placement=True) ) as sess:\n",
    "#     with tf.Session( config=tf.ConfigProto(log_device_placement=True, device_count={'CPU':0}) ) as sess:\n",
    "        print( sess.run(c) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "[2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "a = tf.constant([1.0, 2.0, 3.0], shape=[3])\n",
    "b = tf.constant([1.0, 2.0, 3.0], shape=[3])\n",
    "c = a + b\n",
    "with tf.Session( config=tf.ConfigProto(log_device_placement=True) ) as sess:\n",
    "# with tf.Session( config=tf.ConfigProto(log_device_placement=True, device_count={'CPU':0}) ) as sess:\n",
    "    print( sess.run(c) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "add = tf.add(a,b)\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs\", tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 常用API结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [`tf.layers`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)\n",
    "  + [`class Dense`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/Dense): 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [`tf.lossess`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/losses) 损失函数的模块.  \n",
    "  **注意，以下都是Function，不是Class**\n",
    "  + `get_losses(...)`: Gets the list of losses from the loss_collection.\n",
    "  + `get_regularization_loss(...)`: Gets the total regularization loss.\n",
    "  + `get_regularization_losses(...)`: Gets the list of regularization losses.\n",
    "  + `get_total_loss(...)`: Returns a tensor whose value represents the total loss.\n",
    "  + `absolute_difference(...)`: Adds an Absolute Difference loss to the training procedure.\n",
    "  + `hinge_loss(...)`: Adds a hinge loss to the training procedure.\n",
    "  + `huber_loss(...)`: Adds a Huber Loss term to the training procedure.\n",
    "  + `log_loss(...)`: Adds a Log Loss term to the training procedure.\n",
    "  + `mean_pairwise_squared_error(...)`: Adds a pairwise-errors-squared loss to the training procedure.\n",
    "  + `mean_squared_error(...)`: Adds a Sum-of-Squares loss to the training procedure.\n",
    "  + `sigmoid_cross_entropy(...)`: Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.\n",
    "  + `softmax_cross_entropy(...)`: Creates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST手写数据集识别练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Project-Workspace\\\\Python-Projects\\\\DataAnalysis\\\\Practice'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] 系统找不到指定的文件。: './Practice'\n",
      "D:\\Project-Workspace\\Python-Projects\\DataAnalysis\\Practice\n"
     ]
    }
   ],
   "source": [
    "cd ./Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-6a5f265156f6>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda\\envs\\tensorflow1.15\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda\\envs\\tensorflow1.15\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda\\envs\\tensorflow1.15\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda\\envs\\tensorflow1.15\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\MyProgramFiles\\Anaconda\\envs\\tensorflow1.15\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./datasets/MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tranning examples:    55000\n",
      "number of validation examples:  5000\n",
      "number of test examples:        10000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of tranning examples:   \",mnist.train.num_examples)\n",
    "print(\"number of validation examples: \", mnist.validation.num_examples)\n",
    "print(\"number of test examples:       \", mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape:         (55000, 784)\n",
      "train_labels.shape:  (55000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 每一张图片是 28*28 像素，被拉成了一个 28*28=784的行向量\n",
    "print(\"train.shape:        \",mnist.train.images.shape)\n",
    "\n",
    "# 每张图片对应的分类是一个10维向量，对应于10个分类\n",
    "print(\"train_labels.shape: \",mnist.train.labels.shape)\n",
    "\n",
    "# print(mnist.train.images[0])\n",
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.isinteractive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAHSCAYAAADlm6P3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\nbGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsT\nAAALEwEAmpwYAAA0p0lEQVR4nO3de3zU9ZX/8fchBJCLCiIUEAEvLN5RU1rrpa6u1p/bFu1WLeta\n62rjVvDS+tvVum31t7Xqukpti62iUmjretmq1VZ7UddqrVdA5K4iRUQQ8AboViHJ+f3BuI2Y802Y\nzHxnJp/X8/HgkWTemfmeDDmZwzfDGXN3AQAAACnoVukCAAAAgLww/AIAACAZDL8AAABIBsMvAAAA\nksHwCwAAgGQw/AIAACAZnRp+zewYM3vOzJaY2YWlKgpAedCzQO2gX4HysGL3/JpZnaTnJR0laYWk\npyVNcPeF0XV6WE/vpT5FHQ/oijbozdfcfcc8jrW1PUu/Ah9Uzf0q0bPAlqKe7d6J2xwnaYm7L5Uk\nM7tV0nhJYWP2Uh99zI7sxCGBruUB//lLOR5uq3qWfgU+qJr7VaJngS1FPduZpz0Mk/Ryq49XFC4D\nUJ3oWaB20K9AmXTmzK+1cdmHnkNhZo2SGiWpl3p34nAAOqndnqVfgarBYyxQJp0587tC0vBWH+8k\naeWWn+TuU929wd0b6tWzE4cD0Ent9iz9ClQNHmOBMunM8Pu0pN3NbJSZ9ZD0BUn3lKYsAGVAzwK1\ng34FyqTopz24e5OZTZL0W0l1kqa5+4KSVQagpOhZoHbQr0D5dOY5v3L3+yTdV6JaAJQZPQvUDvoV\nKA9e4Q0AAADJYPgFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJYPgFAABAMhh+AQAAkAyGXwAAACSD\n4RcAAADJYPgFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJ6F7pAlAm3eriqFfPzKu+fPbYMPvo8fPC\nbM5P9wmzQdc+lnlMAACAPHDmFwAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAy\nWHXWRb347+PC7Lm/v7adaz9a1DHHHTMkDts7JAAANaL78J3C7K2D4kySRn9tQZgd0G95mN25cmyY\nLXtxcJiNmbI+zFrmLw6zrowzvwAAAEgGwy8AAACSwfALAACAZDD8AgAAIBkMvwAAAEhGp7Y9mNky\nSRskNUtqcveGUhQFoDzoWaB20K9AeZRi1dlfu/trJbgdtKFuhwFh1vRXw8PssvH/WfQxL399zzC7\n+c4jwmyXH78cZk1FV4MyoGdrSPeRO2fmn/v1U2F26e8/G2ajvxJfD1WFfq2Q7qNGhNnL3+0dZs98\n9IflKEdnbf+nOIwftrXgmI1hds7Es8Os571Pd6SsmsTTHgAAAJCMzg6/Lul3ZjbLzBpLURCAsqJn\ngdpBvwJl0NmnPRzs7ivNbJCk+81ssbs/0voTCg3bKEm9FP+aAEAuMnuWfgWqCo+xQBl06syvu68s\nvF0j6S5JH3pNXXef6u4N7t5Qr56dORyATmqvZ+lXoHrwGAuUR9HDr5n1MbN+778v6WhJ80tVGIDS\nomeB2kG/AuXTmac9DJZ0l5m9fzv/6e6/KUlVAMqBngVqB/0KlEnRw6+7L5W0XwlrSVbWKqOTfvNY\nmJ3c74Ew+6+3dwizfadMyqxnxH+tCrOdl8T1sM6sutGztal5WnNm/qVtV4bZpV7qapAX+jUf1j0e\ng9653sJs7t63hNnkN3bLPObtlx8dZgN+uTDMNu27S5jt+O8vhdlPRz4YZpd8/6Ywu/zefcOs1rHq\nDAAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyOvvyxuggq+8RZs+dPTTMTu63\nJszWt7wbZpfc9oUwG3F5vK5MkrIXKwHI04i+b1S6BKDLajo0Xuf14F43htlh8/4uzPpOzD7mdkue\nCLOsx99uj84Js8ef/dCL//1Fxqqz3t3eyzhi18WZXwAAACSD4RcAAADJYPgFAABAMhh+AQAAkAyG\nXwAAACSD4RcAAADJYNVZTl66qCHMnvvClKJu86O3fy3Mdv3W40XdJgAAqVh6XLyGNEuPy/uHWfOS\n2cWWU7SD9nshzF5v+XOYfWX+aWE2UM93qqZqxplfAAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQDIZf\nAAAAJIPhFwAAAMlg1VkJ2YF7hdnFf39LUbd58dr9wmz09WvDrLmoowEAkI7Jx/6sqOvV/c+mElfS\nvtVnfyLM7hgxOcw+s+jvw2zgZ7ruOrMsnPkFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJYPgFAABA\nMhh+AQAAkIx2V52Z2TRJn5a0xt33Llw2QNJtkkZKWibpRHd/s3xlVg+r7xFmb337vTA7oe/rYZa1\nzmz2iaPDrPn5F8OsIsbtE0bNferD7JVP9gqzYQ+/G2bdH1sQZv5e/HfR1dGzXUudeTt5xjkMK3Ex\nKDn6teO69YofK169bWSYXbznr8Ls6G3eyDhi/ttgrXt8zEO+OCvMelr8GPv2LUPDbIBe6lhhXUxH\nzvxOl3TMFpddKOlBd99d0oOFjwFUh+miZ4FaMV30K5Crdodfd39E0pb/NBovaUbh/RmSjittWQCK\nRc8CtYN+BfJX7HN+B7v7KkkqvB1UupIAlAE9C9QO+hUoo7I/ocXMGiU1SlIv9S734QB0Av0K1BZ6\nFth6xZ75XW1mQySp8HZN9InuPtXdG9y9oV49izwcgE7qUM/Sr0BV4DEWKKNih997JJ1aeP9USXeX\nphwAZULPArWDfgXKqCOrzm6RdLikgWa2QtLFkq6QdLuZnS5puaQTyllkNakbNDDM/rjf7UXd5q+m\nHRpmg59/rKjb7Iy3vnhQmO1wWrwW5ZpdrguzXbtvU1wxX46jk5f9TZit/2yfMGt+PWu1Te2jZ2tP\nt33HhNlFg2/MvG6zZ/yqO3tLGqoA/dpxa0/ZP8xmNlxb5K0W9+zP5cf0C7OdnyqyFEnP/eCAMLt3\naPwYu8cfTguzUdMeL76gLqrdv3V3nxBER5a4FgAlQM8CtYN+BfLHK7wBAAAgGQy/AAAASAbDLwAA\nAJLB8AsAAIBkMPwCAAAgGWV/hbeuZukZI4u63hPvxdmwX60Ms6aijiatn/DxMBs+8YXM607eaXKY\njanPWqIerzO7aE28vmXNe/HKmEO2i2u9eeQDYbb3xElhtvO/5b8+DsjiPevDbHBdkWsCJfVewY94\ndB2D7no+zE497Ygwu3bnX4dZL4t7pLvqwuxnp10TZl9/sDHM2rP4s/HKtuvW7RJmu31jQ5g1F11N\n18WZXwAAACSD4RcAAADJYPgFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJYA/OVnp3541FXe+Ld58V\nZrstfaKo28xaZ3b95deE2V71Pdq55Xid2YWrDwyz/74+rmfwbQvDrPmtdWF2zT9/Lsy+dN6UMCv2\n7wnoSob/bn2YeY51AKXQ/NrrYbb2E/H1TtRBYbbpb+LHtGtujNeOje0RP45e+rMb4mIk1WV037qW\nTWF218Sj4ttcMjvzmPggzvwCAAAgGQy/AAAASAbDLwAAAJLB8AsAAIBkMPwCAAAgGQy/AAAASAar\nznLS+5Xi/p3Rbb89wqzYdWbzNsarVCTp9CvOC7PBty8Os4FvPh5mzZlHjPV7uSXM1re8W+StAgAg\n1T8wK8zOnnROmF035ZowO7BHr6LrafjOeWE26KHHir5dfBBnfgEAAJAMhl8AAAAkg+EXAAAAyWD4\nBQAAQDIYfgEAAJAMhl8AAAAko93h18ymmdkaM5vf6rJLzOwVM5tT+HNsecsE0FH0LFA76Fcgfx3Z\n8ztd0hRJP9ni8u+6+1UlrwgfsPjcPmGWtct3waaNYXbWhedlHnPgbaXf11us3qvjr+OtlngHcOKm\ni56tKc+dvk2YdZNlXrfO+AVejZsu+rUq1W2MH2M2lekX50PvfTnMmspyxDS1+7fn7o9IeiOHWgCU\nAD0L1A76FchfZ/7pMsnM5hZ+ZdO/ZBUBKBd6Fqgd9CtQJsUOvz+StKuksZJWSbo6+kQzazSzmWY2\nc5PeK/JwADqpQz1LvwJVgcdYoIyKGn7dfbW7N7t7i6QbJI3L+Nyp7t7g7g316llsnQA6oaM9S78C\nlcdjLFBeRQ2/Zjak1YfHS5offS6AyqNngdpBvwLl1e62BzO7RdLhkgaa2QpJF0s63MzGSnJJyySd\nWb4SAWwNehaoHfQrkL92h193n9DGxTeVoZYu7diTHwuzh1/9eJj99sjwqV5asKkuzP7pgvPCrN9t\nT4RZtXlrt/jXeIPr4lVvKaNna5DHUUtWKOniNfuEmc1fUswhkSP6tbK69eoVZnt+Z16YZa0a7Yy9\nfxGvOpt/3PAwa3opvh4+jAWRAAAASAbDLwAAAJLB8AsAAIBkMPwCAAAgGQy/AAAASAbDLwAAAJLR\n7qozfFC/hRnrTY6Jo2/uGK8Xa/zOo2E2snvvMNvn8S+G2fAaWmf2xj8eFGbf+/q1YdbT4m/fzL8n\noAtZu7FvmLW8++ccKwFqz+t3xOvDfjX0tjB7ftO7Yfa3vz8785gvHHVDmF02aHaYjTsmfqwcfGdc\nT/PatZn1pIgzvwAAAEgGwy8AAACSwfALAACAZDD8AgAAIBkMvwAAAEgGwy8AAACSwaqzrTT81mVx\n+LU42sbi1Vsjuxe3luu9FfGKo2rz+pfjFS2Tzr8jzD7eM77NA54+Ocx2mrYgzJrjmwQqom7bTUVf\n93dz9wqz0ZpZ9O0CXcVrjfHjz6/3vSrM5mysD7PzJ54bZmOeXpZZz25XNIbZkmOmhtlT34pXf+4x\namKYjbqQVWdb4swvAAAAksHwCwAAgGQw/AIAACAZDL8AAABIBsMvAAAAksHwCwAAgGSw6qyE6qy4\nf0s0e0tR13v289eE2WWfHBdm9//g4MzbHTDt8aLqee3MeJ3M1f9yfZgd2qspzPZ78pQw27nx1TBr\nfmtdmAHV5ujRi4q+br9Fxa1KBLqS7qNGhNlvvxmvM+vfbZswO/LaSWE29L7Hwqy9dZp7fPW9MJt1\nRHztA3vUhdmUz98YZt+99YQwa5mzMMy6Ms78AgAAIBkMvwAAAEgGwy8AAACSwfALAACAZDD8AgAA\nIBntDr9mNtzMHjKzRWa2wMzOLVw+wMzuN7MXCm/7l79cAFnoV6C20LNA/jqy6qxJ0vnuPtvM+kma\nZWb3S/qSpAfd/Qozu1DShZIuKF+p1aFp1eow2/v7Z4XZs2dPKXkt21i84ujbg+aE2UX/9lTm7S79\nVnH17NY9vt2fbRgZZudce1yY7fT9+Dabm+IVaQmjX2vQ6QMfCbNu7fyYdit1NcgZPVsCz00cEmZZ\n68zWt7wbZoOeiVeSdUbz+vVh9s0Jp4fZfXdMD7Mjt4lrPXNS7zAbfUYYdWntnvl191XuPrvw/gZJ\niyQNkzRe0ozCp82QdFyZagTQQfQrUFvoWSB/W/WcXzMbKWl/SU9KGuzuq6TNzStpUMmrA1A0+hWo\nLfQskI8OD79m1lfSHZLOc/f4nP2Hr9doZjPNbOYmledXCAA+iH4Fags9C+SnQ8OvmdVrc1Pe7O53\nFi5ebWZDCvkQSWvauq67T3X3BndvqFfPUtQMIAP9CtQWehbIV0e2PZikmyQtcvfJraJ7JJ1aeP9U\nSXeXvjwAW4N+BWoLPQvkryPbHg6WdIqkeWY2p3DZRZKukHS7mZ0uabmkE8pSIYCtQb8CtYWeBXLW\n7vDr7o9KipbpHFnacmpAS3MYDbvyyTD76LqJYbbh0D+H2eJPTutYXVsha0WaJO1VX9ztTl8/NMx+\ncdwnwmzIc4+FmRdXSrLo19rUHP6VSS3tdMHwGS9k3C6qHT1bGjceP7Wo623brVeYXXzDTWG2bOPA\nMLt0zrGZx7QX+oTZZSfdnHndYowcsbbkt1nreIU3AAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQDIZf\nAAAAJIPhFwAAAMnoyJ5fdFTGGrQdf/R4nF0Xrzn6zG6fD7PFk+KXej9k3MIwe/SpPcOsPWOmvhlm\nLc//Kcx805Kijwmk7K53BmTmzWtZYwRccs7pYXbCf/wmzP5pu5fC7OCeLRlZmy+4J0k6+dDpYSZJ\nOjQ7LrUVa/uH2S6Kv/6ujDO/AAAASAbDLwAAAJLB8AsAAIBkMPwCAAAgGQy/AAAASAbDLwAAAJLB\nqrNq4B5GzS8sDbPdz42z1RmH211PdKSqtusp+poAivGN2eMz81Gam1MlQPXqee/TYXbvk6PD7Bdj\njgyz5Z/aJszuPGVymI2p7xlmknTI3BPCbM3r24bZ1IN+EmZn/P60MNvjgngNaaqP6Zz5BQAAQDIY\nfgEAAJAMhl8AAAAkg+EXAAAAyWD4BQAAQDIYfgEAAJAMVp0BQIWdfPs5YWap7iICSqT5tdfDrNuj\ncTby0fg2v/bNg4quZ1u9mJHFrtQ+YTZaM8OMHyEfxplfAAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQ\nDIZfAAAAJKPd4dfMhpvZQ2a2yMwWmNm5hcsvMbNXzGxO4c+x5S8XQBb6Fagt9CyQv46sOmuSdL67\nzzazfpJmmdn9hey77n5V+coDsJXo1xq0ywWPV7oEVA49C+Ss3eHX3VdJWlV4f4OZLZI0rNyFAdh6\n9CtQW+hZIH9b9ZxfMxspaX9JTxYummRmc81smpn1L3VxAIpHvwK1hZ4F8tHh4dfM+kq6Q9J57r5e\n0o8k7SpprDb/q/Xq4HqNZjbTzGZu0nudrxhAu+hXoLbQs0B+OjT8mlm9Njflze5+pyS5+2p3b3b3\nFkk3SBrX1nXdfaq7N7h7Q716lqpuAAH6Fagt9CyQr45sezBJN0la5O6TW10+pNWnHS9pfunLA7A1\n6FegttCzQP46su3hYEmnSJpnZnMKl10kaYKZjZXkkpZJOrMM9QHYOvQrUFvoWSBnHdn28KgkayO6\nr/TlAOgM+hWoLfQskD9e4Q0AAADJYPgFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJYPgFAABAMhh+\nAQAAkAyGXwAAACSD4RcAAADJYPgFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJMHfP72BmayW9VPhw\noKTXcjt4+6qpHmppW1esZYS771iC2ym5LfpV6pr3fylQS9uqqRapNPVUbb9KVf0YSy2xaqqnK9bS\nZs/mOvx+4MBmM929oSIHb0M11UMtbaOWyqqmr5la2kYtsWqrp9yq6eulllg11ZNSLTztAQAAAMlg\n+AUAAEAyKjn8Tq3gsdtSTfVQS9uopbKq6WumlrZRS6za6im3avp6qSVWTfUkU0vFnvMLAAAA5I2n\nPQAAACAZFRl+zewYM3vOzJaY2YWVqKFVLcvMbJ6ZzTGzmRU4/jQzW2Nm81tdNsDM7jezFwpv+1ew\nlkvM7JXC/TPHzI7NqZbhZvaQmS0yswVmdm7h8tzvm4xaKnLf5K2a+rVQT8V6ln4Na6Ffq0g19Sz9\nmlkL/Vqhfs39aQ9mVifpeUlHSVoh6WlJE9x9Ya6F/KWeZZIa3L0iu+3M7DBJb0v6ibvvXbjsSklv\nuPsVhR9c/d39ggrVcomkt939qnIff4tahkga4u6zzayfpFmSjpP0JeV832TUcqIqcN/kqdr6tVDT\nMlWoZ+nXsBb6tUpUW8/Sr5m1XCL6tSL9Wokzv+MkLXH3pe6+UdKtksZXoI6q4O6PSHpji4vHS5pR\neH+GNn8jVKqWinD3Ve4+u/D+BkmLJA1TBe6bjFpSQL+2Qr+2jX6tKvRsAf3aNvq1MsPvMEkvt/p4\nhSr7g8kl/c7MZplZYwXraG2wu6+SNn9jSBpU4Xommdncwq9tcvkVUWtmNlLS/pKeVIXvmy1qkSp8\n3+Sg2vpVqr6epV9boV8rrtp6ln7NRr+2XYtUxvumEsOvtXFZJVdOHOzuB0j6P5ImFn41gb/4kaRd\nJY2VtErS1Xke3Mz6SrpD0nnuvj7PY3egloreNzmptn6V6Nks9GtcSwr9KlVfz9KvMfo1rqWs900l\nht8Vkoa3+ngnSSsrUIckyd1XFt6ukXSXNv/KqNJWF54H8/7zYdZUqhB3X+3uze7eIukG5Xj/mFm9\nNjfDze5+Z+Hiitw3bdVSyfsmR1XVr1JV9iz9Kvq1ilRVz9KvMfo1rqXc900lht+nJe1uZqPMrIek\nL0i6pwJ1yMz6FJ5gLTPrI+loSfOzr5WLeySdWnj/VEl3V6qQ9xuh4HjldP+YmUm6SdIid5/cKsr9\nvolqqdR9k7Oq6VepanuWfqVfq0nV9Cz9mo1+rWC/unvufyQdq83/G/VFSf9aiRoKdewi6dnCnwWV\nqEXSLdp8Sn+TNv+L/XRJO0h6UNILhbcDKljLTyXNkzRXmxtjSE61HKLNv6qbK2lO4c+xlbhvMmqp\nyH1Tge/RqujXQi0V7Vn6NayFfq2iP9XSs/Rru7XQrxXqV17hDQAAAMngFd4AAACQDIZfAAAAJIPh\nFwAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAA\nAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg\n+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMno1PBrZseY2XNmtsTMLixVUQDKg54Fagf9CpSHuXtx\nVzSrk/S8pKMkrZD0tKQJ7r4wuk4P6+m91Keo4wFd0Qa9+Zq775jHsba2Z+lX4IOquV8lehbYUtSz\n3Ttxm+MkLXH3pZJkZrdKGi8pbMxe6qOP2ZGdOCTQtTzgP38px8NtVc/Sr8AHVXO/SvQssKWoZzvz\ntIdhkl5u9fGKwmUAqhM9C9QO+hUok86c+bU2LvvQcyjMrFFSoyT1Uu9OHA5AJ7Xbs/QrUDV4jAXK\npDNnfldIGt7q450krdzyk9x9qrs3uHtDvXp24nAAOqndnqVfgarBYyxQJp0Zfp+WtLuZjTKzHpK+\nIOme0pQFoAzoWaB20K9AmRT9tAd3bzKzSZJ+K6lO0jR3X1CyygCUFD0L1A76FSifzjznV+5+n6T7\nSlQLgDKjZ4HaQb8C5cErvAEAACAZDL8AAABIBsMvAAAAksHwCwAAgGQw/AIAACAZDL8AAABIBsMv\nAAAAksHwCwAAgGQw/AIAACAZDL8AAABIBsMvAAAAksHwCwAAgGQw/AIAACAZDL8AAABIBsMvAAAA\nksHwCwAAgGQw/AIAACAZDL8AAABIBsMvAAAAksHwCwAAgGR0r3QBKI/uw3cKs7cOijNJGv21BWF2\nQL/lYXbnyrFhtuzFwWE2Zsr6MGuZvzjMkI66HQaE2eJLdi/58XoPezvMnvnYT8Jsrz+cFmZNa7cJ\nsx2eic9D7Hj3c2HWrubmOHprXfG3C1SZbvuOCbPFZ20bZv9+xG1hdnyfN4qqZcztE7Pzq+LH0aZX\nVhZ1TGwdzvwCAAAgGQy/AAAASAbDLwAAAJLB8AsAAIBkMPwCAAAgGQy/AAAASEanVp2Z2TJJGyQ1\nS2py94ZSFIWO6T5qRJi9/N3eYfbMR39YjnJ01vZ/isM942jBMRvD7JyJZ4dZz3uf7khZaKVWe3af\nB+KVQ3cNmpJjJVJLRjbv0JuKu9HPZWTfzr5qt4xzGCua/hxmR/z3uWG28x11YdbrV09lF4SSqdV+\nLZd1J388zP5w5bVh1pLZtbHiriUtPjGuRZIWHNcUZmeff06Y9b7zySIrwpZKsef3r939tRLcDoB8\n0LNA7aBfgRLjaQ8AAABIRmeHX5f0OzObZWaNpSgIQFnRs0DtoF+BMujs0x4OdveVZjZI0v1mttjd\nH2n9CYWGbZSkXoqfhwogF5k9S78CVYXHWKAMOnXm191XFt6ukXSXpHFtfM5Ud29w94Z69ezM4QB0\nUns9S78C1YPHWKA8ih5+zayPmfV7/31JR0uaX6rCAJQWPQvUDvoVKJ/OPO1hsKS7zOz92/lPd/9N\nSarC/7Lu8V/RO9dbmM3d+5Ywm/zGbpnHvP3yo8NswC8XhtmmfXcJsx3//aUw++nIB8Psku/Hq6Mu\nv3ffMEObarZnJ2yfteKH/7cbGdo9PhO4+OjrwuzZw+PbnHBkvH5w9LcWhFnLhg3xjaItNduvnVG3\n26gwe/1v3w2zm9btHGbfu3V8mA1YXOxCs9i5l8aPv5I0vk+8vOPqq+PVjRc/f0qYtcxf3H5h+F9F\nD7/uvlTSfiWsBUAZ0bNA7aBfgfLhlAkAAACSwfALAACAZDD8AgAAIBkMvwAAAEgGwy8AAACS0dlX\neEOZNR0ar/N6cK8bw+yweX8XZn0nZh9zuyVPhFlzxvW6PTonzB5/9kO72f8iY9VZ727vZRwRqZjw\n46+G2Zwzv1fy461oir/vfvzmQWHWOODxMBtcVzsvQLBfjzhbeOIPwuzUcZ8Ks9X/1hBmPX47s0N1\noWtY+c+fCLNvN/4kzC64LV719csL9g+znV9+rGOFlcilO52cmY8/P/6ZldV7qw/tH2Y7sgF6q3Dm\nFwAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyWHVW5ZYel7H3JEOPy+OVKM1L\nZhdbTtEO2u+FMHu95c9h9pX5p4XZQD3fqZpQO0ZcHq/CGrPTWWHWY238I27AfA+zXm80xbeZsZbr\n7v/7L2H29l7x+rSP/K4+zNrz6qEtYXb4AYvC7B92jNc/HdLr3aJqmTHyt2H28LW9w+wb3z4jzHa4\n7Zkwa3m3uDpRYRZHF//wi2E28ntPhllTS9Yizny9Mzzuyc7Y7k/xzyVsHc78AgAAIBkMvwAAAEgG\nwy8AAACSwfALAACAZDD8AgAAIBkMvwAAAEgGq86q3ORjf1bU9er+Z1OJK2nf6rM/EWZ3jJgcZp9Z\n9PdhNvAzrDOD5Js2htnoxqdzrCTb0Kvi9WHlsu0tcbYy43qXfipeI7jL/1scZj8c/lAHqvqwT27z\nP2H2h8u+H2afWhuvsut5X/X83aPjhl6Zf5/kamC81lCSuhV53rH72/k/rndVnPkFAABAMhh+AQAA\nkAyGXwAAACSD4RcAAADJYPgFAABAMhh+AQAAkIx2V52Z2TRJn5a0xt33Llw2QNJtkkZKWibpRHd/\ns3xl1oZuvXqF2au3jQyzi/f8VZgdvc0bGUfMf1OddY+PecgXZ4VZT6sPs7dvGRpmA/RSxwrD/6Jn\n0RE9fjszzFY9vm2Y7XP9GWE277AbO1VTW74z5fowu3S/T4ZZy4YNJa+lHOjX2vTqefFqz8VH/CDz\nui1qCbNT/nRMmPVYsirMmjKPiC115MzvdElb/m1cKOlBd99d0oOFjwFUh+miZ4FaMV30K5Crdodf\nd39E0panH8dLmlF4f4ak40pbFoBi0bNA7aBfgfwV+5zfwe6+SpIKbweVriQAZUDPArWDfgXKqOxP\nGjWzRkmNktRLvct9OACdQL8CtYWeBbZesWd+V5vZEEkqvF0TfaK7T3X3BndvqFfPIg8HoJM61LP0\nK1AVeIwFyqjY4fceSacW3j9V0t2lKQdAmdCzQO2gX4Ey6siqs1skHS5poJmtkHSxpCsk3W5mp0ta\nLumEchZZK9aesn+YzWy4tshbLe6ZKcuP6RdmOz9VZCmSnvvBAWF279DrwmyPP5wWZqOmPV58QfgQ\nera81v3Dx8Nsh4eWh9nqY0eE2cZ+VnxBGVcd8se34/CJuWHUvH59mO3auDTM9rnh9DCbd+hNcS0Z\nGno2h9nSf9k7zEZ+szZ+rtCvldV9WLxqc+mXR4bZw6f/R8atxmtPJemWDcPC7J2T4rP3Ta+uzLxd\ndFy7k5W7TwiiI0tcC4ASoGeB2kG/AvnjFd4AAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQ\njLK/wltKBt31fJidetoRYXbtzr8Os14W/xV1V12Y/ey0a8Ls6w82hll7Fn82Xtl23bpdwmy3b2wI\ns3iREVA+r371E2E2edL1YbZfjz+G2ZJN8YqjPXpsDLOsPm9Pt4xzGEvOfi/MVjbH6xAb7/lymO32\n1SfCbNd/XBJm1zy5Z5idN2BhmGXZtF1LUddDWt799LgwG/L1+Ht2zsjvZdxqjzBpfPnwzHpWfWXn\nMPNXFmReF6XBmV8AAAAkg+EXAAAAyWD4BQAAQDIYfgEAAJAMhl8AAAAkg+EXAAAAyWDVWQk1v/Z6\nmK2NtyrpRB0UZpv+5sAwu+bGeO3Y2B7xGpZLf3ZDXIykOnmYrWvZFGZ3TTwqvs0lszOPCZTD6nPi\nxvv9164Ks97d6jNuNe6t/Xtmrd7K/8ftLvXx17FL/bth9uwJ14TZjUePCbPffW63MLv1urFhdt5F\nxa06A963ZPLHw+zhz8e9vmNdz5LXMv+1IZn5gGdYZ1ZpnPkFAABAMhh+AQAAkAyGXwAAACSD4RcA\nAADJYPgFAABAMhh+AQAAkAxWnVW5+gdmhdnZk84Js+umXBNmB/boVXQ9Dd85L8wGPfRY0bcLFKv7\nsKFhduIZD4ZZ326lX3FUCfVWF2ab4q2FmXpafF5k4vYvhtmAu98Os5E9XguzbsWeh7HirobSsO7x\nCNHysb3D7CNXLQ2zswY/FGYHZrRsvc0Js02+TXzFMnj6gNsz802vNIfZXe8MCLML/vukMBvzw/Vh\n1m3dO2HW9NLLYdaVceYXAAAAyWD4BQAAQDIYfgEAAJAMhl8AAAAkg+EXAAAAyWD4BQAAQDLaHX7N\nbJqZrTGz+a0uu8TMXjGzOYU/x5a3TAAdRc8CtYN+BfLXkT2/0yVNkfSTLS7/rrtfVfKK0GF1G1vC\nbFOZTuoPvTfeCdhUliOiCNOVUM8uvHRImN25w91hFndPtjea3wuzt1rivnu5abswm/RfZ4RZ93ey\nl9n+4LTrw6wu4+fAQb3ir6NYJ/VbVdT1iv27UJF7jKvMdNVovy79t4+G2bxTv1/y42V9n2TttL7n\nnf5hNv/POxVfUKDOsr+jP93v2TAb3yfehz3+M9fGN/qZOMr6mXXUzDPDrP/P+oZZ7zufjA9YA9qd\nkNz9EUlv5FALgBKgZ4HaQb8C+evM6cFJZja38Cub+J9VAKoFPQvUDvoVKJNih98fSdpV0lhJqyRd\nHX2imTWa2Uwzm7lJpf81G4AO6VDP0q9AVeAxFiijooZfd1/t7s3u3iLpBknjMj53qrs3uHtDvTJe\nmBtA2XS0Z+lXoPJ4jAXKq6jh18xa/w+T4yXNjz4XQOXRs0DtoF+B8mp324OZ3SLpcEkDzWyFpIsl\nHW5mY7X5/9oukxT/d0EAuaJngdpBvwL5a3f4dfcJbVx8UxlqQRu69eoVZnt+Z16Y7VXfoxzlaO9f\nxKvO5h83PMyaXoqvh9KiZ8vrb370L2E29A//E2bdHp0TZqP0eNH1XHnpPmFWt+22Ybbk63uF2bwv\nln5NVTnUr6v912mq5X7dfr94LVexznnlsDC7//H9wmzIH+Pb3H7mq2HWtHRZR8oqqUc+0Rhm7+y0\nTZitn7A+zCb+1cNhdvp2y8Ns1semh9mz+4eR/qHhnDAbdVHxP8/yUvs/OQAAAIAOYvgFAABAMhh+\nAQAAkAyGXwAAACSD4RcAAADJYPgFAABAMtpddYbKev2OeH3Yr4beFmbPb3o3zP7292dnHvOFo24I\ns8sGzQ6zccccFGaD74zraV67NrMeoFu/fmH2jY/fm2Ml0sB5TWGWtc6sIj6yYxhtvzjHOspk16sX\nhllzjnWkasezN4XZPl+OV2H1XxTf5oC74tfz2H3DEx2qa0txx1aGPfZsmPXNuF7f2+Psl8PjvWTf\n/9L4MLvw5PhGT+q3Ksxun3BNmJ37eDxj9PrlU2GWJ878AgAAIBkMvwAAAEgGwy8AAACSwfALAACA\nZDD8AgAAIBkMvwAAAEgGq86qwGuN8YqwX+97VZjN2VgfZudPPDfMxjy9LLOe3a5oDLMlx0wNs6e+\ndW2Y7TFqYpiNupBVZ8hmPeLv9X/Y9uUcK5Fe2zf+sTnij/3DrPnNN4s6Xt1f7ZaZP//lgWF22Wdv\nCbPxfV4rqp68HTX/xDDr/c4rOVaCLTUtXRZmo74eZ1laiisleU0vrwiz4d+Os2kzjw+zk278YZjt\n0SM+d7r8b8NIo38ZZ3nizC8AAACSwfALAACAZDD8AgAAIBkMvwAAAEgGwy8AAACSwfALAACAZLDq\nLCfdR40Is99+M15n1r/bNmF25LWTwmzofY+FWXOYbLbHV98Ls1lHxNc+sEddmE35/I1h9t1bTwiz\nljkLwwzpaFm3Psw++eyEMHt4v3jVV7FmT/xemO1v8YrBpr4eZof+9bwwO6r/g5n1/F3feGVZSxdY\nHFV/5YAw801/yrESVNq6kz8eZhtGxOfydrosfjxMgR88Nszuv/G6jGvG9+lrzX8Os488Uv3nVau/\nQgAAAKBEGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAy2h1+zWy4mT1kZovMbIHZ5v/ObGYDzOx+\nM3uh8LZ/+csFkIV+BWoLPQvkryOrzpokne/us82sn6RZZna/pC9JetDdrzCzCyVdKOmC8pVa256b\nOCTMstaZrW95N8wGPROvJOuM5vXxWqlvTjg9zO67Y3qYHblNXOuZk3qH2egzwght65L96k1NYbb9\nt3qF2c9v/kiYfb7vq52qqS3PnBWvQSuffH+B9+XlR4bZcz/cK8y2XR7/LMtS/8T8MIuXx9WULtmz\nxXr30+PC7OeXx2tBT9/5kHKUU1XswLi/lnytR5gtOjxeNVrsOsRP/PqrYTb6P58o6jbz1O5PTXdf\n5e6zC+9vkLRI0jBJ4yXNKHzaDEnHlalGAB1EvwK1hZ4F8rdVpwzMbKSk/SU9KWmwu6+SNjevpEEl\nrw5A0ehXoLbQs0A+Ojz8mllfSXdIOs/d49+Lf/h6jWY208xmblJ5fk0P4IPoV6C20LNAfjo0/JpZ\nvTY35c3ufmfh4tVmNqSQD5G0pq3ruvtUd29w94Z69SxFzQAy0K9AbaFngXx1ZNuDSbpJ0iJ3n9wq\nukfSqYX3T5V0d+nLA7A16FegttCzQP46su3hYEmnSJpnZnMKl10k6QpJt5vZ6ZKWSzqhLBUC2Br0\nK1Bb6FkgZ+0Ov+7+qCQL4njnDT7gxuOnFnW9bbvFa5wuvuGmMFu2cWCYXTrn2Mxj2gt9wuyyk27O\nvG4xRo5YW/LbTFWK/eoz41VY3342/l7//MHTylFOzfinl48Is0VT4pVKA+5ZGGbbr3+8UzW1pYus\nMwul2LNZ1u0SjyU71sVP61h65UHx9WYV9120/QPPh9mGw3YPs+Ye0V/nZn/eMf6l+4gTXgyzK0fG\nc8SI7vGqs6xf8r/UtDHM/m7KP4fZHtMWh1lzRiXVgld4AwAAQDIYfgEAAJAMhl8AAAAkg+EXAAAA\nyWD4BQAAQDIYfgEAAJCMjuz5RQlccs7pYXbCf/wmzP5pu5fC7OCeLRlZmy8GJEk6+dDpYSZJOjQ7\nLrUVa/uH2S6Kv36gPbv8v3iNzzk/PizM/nq7eI3P8X3j3qqE0b/8SpjtecWrYdby+pthtt2GJ8Ks\nFtYYoXZt/+KmMFvbHL9888KTp4RZy8nxY2WWH68bGWYn9ftVmPXtlv1Key0qrh4pXmf2/TfHhNlP\nf/ypMBv63+vi7JnHwqzWfw5w5hcAAADJYPgFAABAMhh+AQAAkAyGXwAAACSD4RcAAADJYPgFAABA\nMlh1lpOe9z4dZvc+OTrMfjHmyDBb/qltwuzOUyaH2Zj67DUsh8w9IczWvL5tmE096CdhdsbvTwuz\nPS74U5jV+joVVFbzgufCbPlhvcJsRt2eYfbtcyaE2bVnXBdmD78dryKa8XC8X3CPy5aFmSSNfvWp\nMGvKvCZQfbIeK888+Ath1rJd3zBbPCl+3Bq401th9uj+N4eZVJ+RZfv04s+F2ZIXhoTZ7jPiVW/d\nFy8PsyFvxivLPEy6Ns78AgAAIBkMvwAAAEgGwy8AAACSwfALAACAZDD8AgAAIBkMvwAAAEiGuee3\n6GJbG+Afs3h1F5CaB/zns9y9odJ1tIV+BT6omvtVomeBLUU9y5lfAAAAJIPhFwAAAMlg+AUAAEAy\nGH4BAACQDIZfAAAAJKPd4dfMhpvZQ2a2yMwWmNm5hcsvMbNXzGxO4c+x5S8XQBb6Fagt9CyQv+4d\n+JwmSee7+2wz6ydplpndX8i+6+5Xla88AFuJfgVqCz0L5Kzd4dfdV0laVXh/g5ktkjSs3IUB2Hr0\nK1Bb6Fkgf1v1nF8zGylpf0lPFi6aZGZzzWyamfUvdXEAike/ArWFngXy0eHh18z6SrpD0nnuvl7S\njyTtKmmsNv+r9ergeo1mNtPMZm7Se52vGEC76FegttCzQH46NPyaWb02N+XN7n6nJLn7andvdvcW\nSTdIGtfWdd19qrs3uHtDvXqWqm4AAfoVqC30LJCvjmx7MEk3SVrk7pNbXT6k1acdL2l+6csDsDXo\nV6C20LNA/jqy7eFgSadImmdmcwqXXSRpgpmNleSSlkk6swz1Adg69CtQW+hZIGcd2fbwqCRrI7qv\n9OUA6Az6Fagt9CyQP17hDQAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyGH4B\nAACQDIZfAAAAJIPhFwAAAMlg+AUAAEAyGH4BAACQDIZfAAAAJIPhFwAAAMkwd8/vYGZrJb1U+HCg\npNdyO3j7qqkeamlbV6xlhLvvWILbKbkt+lXqmvd/KVBL26qpFqk09VRtv0pV/RhLLbFqqqcr1tJm\nz+Y6/H7gwGYz3b2hIgdvQzXVQy1to5bKqqavmVraRi2xaqun3Krp66WWWDXVk1ItPO0BAAAAyWD4\nBQAAQDIqOfxOreCx21JN9VBL26ilsqrpa6aWtlFLrNrqKbdq+nqpJVZN9SRTS8We8wsAAADkjac9\nAAAAIBkVGX7N7Bgze87MlpjZhZWooVUty8xsnpnNMbOZFTj+NDNbY2bzW102wMzuN7MXCm/7V7CW\nS8zslcL9M8fMjs2pluFm9pCZLTKzBWZ2buHy3O+bjFoqct/krZr6tVBPxXqWfg1roV+rSDX1LP2a\nWQv9WqF+zf1pD2ZWJ+l5SUdJWiHpaUkT3H1hroX8pZ5lkhrcvSK77czsMElvS/qJu+9duOxKSW+4\n+xWFH1z93f2CCtVyiaS33f2qch9/i1qGSBri7rPNrJ+kWZKOk/Ql5XzfZNRyoipw3+Sp2vq1UNMy\nVahn6dewFvq1SlRbz9KvmbVcIvq1Iv1aiTO/4yQtcfel7r5R0q2Sxlegjqrg7o9IemOLi8dLmlF4\nf4Y2fyNUqpaKcPdV7j678P4GSYskDVMF7puMWlJAv7ZCv7aNfq0q9GwB/do2+rUyw+8wSS+3+niF\nKvuDySX9zsxmmVljBetobbC7r5I2f2NIGlTheiaZ2dzCr21y+RVRa2Y2UtL+kp5Uhe+bLWqRKnzf\n5KDa+lWqvp6lX1uhXyuu2nqWfs1Gv7Zdi1TG+6YSw6+1cVklV04c7O4HSPo/kiYWfjWBv/iRpF0l\njZW0StLVeR7czPpKukPSee6+Ps9jd6CWit43Oam2fpXo2Sz0a1xLCv0qVV/P0q8x+jWupaz3TSWG\n3xWShrf6eCdJKytQhyTJ3VcW3q6RdJc2/8qo0lYXngfz/vNh1lSqEHdf7e7N7t4i6QbleP+YWb02\nN8PN7n5n4eKK3Ddt1VLJ+yZHVdWvUlX2LP0q+rWKVFXP0q8x+jWupdz3TSWG36cl7W5mo8ysh6Qv\nSLqnAnXIzPoUnmAtM+sj6WhJ87OvlYt7JJ1aeP9USXdXqpD3G6HgeOV0/5iZSbpJ0iJ3n9wqyv2+\niWqp1H2Ts6rpV6lqe5Z+pV+rSdX0LP2ajX6tYL+6e+5/JB2rzf8b9UVJ/1qJGgp17CLp2cKfBZWo\nRdIt2nxKf5M2/4v9dEk7SHpQ0guFtwMqWMtPJc2TNFebG2NITrUcos2/qpsraU7hz7GVuG8yaqnI\nfVOB79Gq6NdCLRXtWfo1rIV+raI/1dKz9Gu7tdCvFepXXuENAAAAyeAV3gAAAJAMhl8AAAAkg+EX\nAAAAyWD4BQAAQDIYfgEAAJAMhl8AAAAkg+EXAAAAyWD4BQAAQDL+P6z5gc2K7poyAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(mnist.train.images[0].reshape(28,28))\n",
    "# px.imshow(mnist.train.images[2].reshape(28,28))\n",
    "\n",
    "nrow=2; ncol=3\n",
    "fig, axes = plt.subplots(nrow,ncol,figsize=(4*ncol,4*nrow))\n",
    "for i in range(nrow):\n",
    "    for j in range(ncol):\n",
    "        axes[i,j].imshow(mnist.train.images[ i*nrow+(j+1) ].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN模型\n",
    "\n",
    "这里构建一个 **输入层 + 隐藏层1 + 隐藏层2 + softmax层**，一共4层的神经网络，使用的内容如下：  \n",
    "+ 每一层的激活函数使用ReLU函数\n",
    "+ 损失函数使用L2正则项\n",
    "+ 使用衰减的指数学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------配置相关参数------\n",
    "# 输入层节点数\n",
    "INPUT_NODE = 784\n",
    "# 输出层节点数——对应于10个分类\n",
    "OUTPUT_NODE = 10\n",
    "# 隐藏层1的节点数\n",
    "LAYER_1_NODE = 500\n",
    "# 隐藏层2的节点数\n",
    "LAYER_2_NODE = 10\n",
    "\n",
    "# batch大小\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 训练轮次\n",
    "TRAIN_STEPS = 3000\n",
    "\n",
    "# 基础学习率和衰减率\n",
    "LEARN_RATE = 0.8\n",
    "DECAY_RATE = 0.9\n",
    "\n",
    "# 正则化系数\n",
    "LAMBDA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_calculate(input_tensor, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    计算网络前向传播得到的结果,不包括softmax层\n",
    "    这里的网络是全连接层\n",
    "    \"\"\"\n",
    "    # 隐藏层1的结果，使用了ReLU激活函数\n",
    "    layer1 = tf.nn.relu(tf.matmul(input_tensor, w1) + b1)\n",
    "    # 隐藏层2的结果，仍然使用ReLU\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "    # softmax层\n",
    "    # softmax = tf.nn.softmax(layer2)\n",
    "    # 返回softmax层\n",
    "    #return softmax\n",
    "    # 这里不直接返回softmax层的结果，而是返回进行softmax之前的值\n",
    "    return layer2\n",
    "\n",
    "\n",
    "def network_predict(input_tensor, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    用于计算网络前向传播得到的结果,这个返回的是softmax层之后的结果\n",
    "    \"\"\"\n",
    "    # 隐藏层1的结果，使用了ReLU激活函数\n",
    "    layer1 = tf.nn.relu(tf.matmul(input_tensor, w1) + b1)\n",
    "    # 隐藏层2的结果，仍然使用ReLU\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "    # softmax层\n",
    "    softmax = tf.nn.softmax(layer2)\n",
    "    # 返回softmax层\n",
    "    return softmax\n",
    "\n",
    "\n",
    "def network_train(mnist):\n",
    "    \"\"\"\n",
    "    用于训练网络的函数\n",
    "    \"\"\"\n",
    "    x_batch = tf.placeholder(tf.float32, shape=[None, INPUT_NODE], name='x_batch')\n",
    "    y_batch = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE], name='y_batch')\n",
    "    \n",
    "    # 定义正则化函数\n",
    "    reg_l2 = tf.contrib.layers.l2_regularizer(LAMBDA)\n",
    "    \n",
    "    # 定义两个隐藏层的参数，并将这两个隐藏层的参数加入到losses集合里\n",
    "    w1 = tf.get_variable(name='w1', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([INPUT_NODE, LAYER_1_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    w2 = tf.get_variable(name='w2', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([LAYER_1_NODE, LAYER_2_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    # 注意，一般不对偏置项做正则化，所以不用将它们加入到对应的loss集合里\n",
    "    b1 = tf.get_variable(name='b1', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_1_NODE]))\n",
    "    b2 = tf.get_variable(name='b2', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_2_NODE]))\n",
    "    \n",
    "    # 定义网络前向传播结果的计算，注意，这里得到的结果不包括softmax层\n",
    "    y_pred = network_calculate(x_batch,w1,b1,w2,b2)\n",
    "    \n",
    "    # 定义交叉熵损失函数的计算，这里使用TF自带的损失函数，它在计算的时候会包括softmax这一步\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=y_pred)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失\n",
    "    # loss = cross_entropy_mean + reg_l2(w1) + reg_l2(w2)\n",
    "    # 提取出经过正则化的变量集合\n",
    "    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    # 总的损失函数\n",
    "    loss = cross_entropy_mean + tf.add_n(reg_variables)\n",
    "    \n",
    "    # 定义指数衰减的学习率\n",
    "    # 定义全局的步数，这个变量不需要训练，不加入train集合, 所以trainable=False\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=LEARN_RATE, global_step=global_step,\n",
    "                                               decay_steps=mnist.train.num_examples/BATCH_SIZE,\n",
    "                                               decay_rate=DECAY_RATE)\n",
    "    \n",
    "    # 定义梯度学习的优化器\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "                   .minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 初始化会话并开始训练\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        # 初始化所有变量\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # 准备验证集\n",
    "        feed_valid = {x_batch:mnist.validation.images, y_batch:mnist.validation.labels}\n",
    "        # 迭代训练\n",
    "        for step in range(TRAIN_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_step, feed_dict={x_batch:xs, y_batch:ys})\n",
    "            # 每 200 轮输出一次在验证集上的loss\n",
    "            if (step+1) % 200 == 0:\n",
    "                valid_loss = sess.run(loss, feed_dict=feed_valid)\n",
    "                print(\"经过 {} 轮训练后的在验证集上的交叉熵误差为：{:g}\".format(step+1, valid_loss))\n",
    "                \n",
    "        # 由于整个计算图的数据只有在运行的Session中才能获得，所以需要在Session关闭前保存最终得到的两层神经网络的权重值并返回\n",
    "        weights_1, weights_2, bias_1, bias_2 = sess.run((w1,w2,b1,b2))\n",
    "    \n",
    "    # 不能返回 w1,w2,b1,b2，它们只是variable,其中存储的最终的张量值只在上面的Session中存在，上面的Session一旦关闭，其中存储的张量值就消失了。\n",
    "    # return w1,w2,b1,b2\n",
    "    return weights_1,weights_2,bias_1,bias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-49-6db2d627dee3>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "经过 200 轮训练后的在验证集上的交叉熵误差为：100.664\n",
      "经过 400 轮训练后的在验证集上的交叉熵误差为：6.45011\n",
      "经过 600 轮训练后的在验证集上的交叉熵误差为：1.95566\n",
      "经过 800 轮训练后的在验证集上的交叉熵误差为：1.71977\n",
      "经过 1000 轮训练后的在验证集上的交叉熵误差为：1.67968\n",
      "经过 1200 轮训练后的在验证集上的交叉熵误差为：1.84289\n",
      "经过 1400 轮训练后的在验证集上的交叉熵误差为：1.67013\n",
      "经过 1600 轮训练后的在验证集上的交叉熵误差为：1.83942\n",
      "经过 1800 轮训练后的在验证集上的交叉熵误差为：1.68078\n",
      "经过 2000 轮训练后的在验证集上的交叉熵误差为：1.68035\n",
      "经过 2200 轮训练后的在验证集上的交叉熵误差为：1.68546\n",
      "经过 2400 轮训练后的在验证集上的交叉熵误差为：1.66816\n",
      "经过 2600 轮训练后的在验证集上的交叉熵误差为：1.66437\n",
      "经过 2800 轮训练后的在验证集上的交叉熵误差为：1.68067\n",
      "经过 3000 轮训练后的在验证集上的交叉熵误差为：1.65914\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 这里需要限定变量的scope\n",
    "# 首先重置整个计算图——主要是为了清除scope里的变量\n",
    "tf.reset_default_graph()\n",
    "# 输出当前计算图中的全局变量\n",
    "# print(\"训练前的变量有：\")\n",
    "# display(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "# print(\"----------------------------------------------\")\n",
    "# 创建一个scope，执行网络训练\n",
    "with tf.variable_scope(name_or_scope='train', reuse=tf.AUTO_REUSE)  as train_scope:\n",
    "#     network_train(mnist)\n",
    "    (w1,w2,b1,b2) = network_train(mnist)\n",
    "# 输出训练后的变量\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"训练之后的变量为：\")\n",
    "# display(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "\n",
      "经过 200 轮训练后的在验证集上的交叉熵误差为：100.219\n",
      "经过 400 轮训练后的在验证集上的交叉熵误差为：7.01822\n",
      "经过 600 轮训练后的在验证集上的交叉熵误差为：2.55724\n",
      "经过 800 轮训练后的在验证集上的交叉熵误差为：2.31795\n",
      "经过 1000 轮训练后的在验证集上的交叉熵误差为：2.30363\n",
      "经过 1200 轮训练后的在验证集上的交叉熵误差为：2.30268\n",
      "经过 1400 轮训练后的在验证集上的交叉熵误差为：2.30261\n",
      "经过 1600 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 1800 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 2000 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 2200 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 2400 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 2600 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 2800 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "经过 3000 轮训练后的在验证集上的交叉熵误差为：2.3026\n",
      "Wall time: 6.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 使用CPU进行计算\n",
    "# 这里需要限定变量的scope\n",
    "# 首先重置整个计算图——主要是为了清除scope里的变量\n",
    "tf.reset_default_graph()\n",
    "# 输出当前计算图中的全局变量\n",
    "# print(\"训练前的变量有：\")\n",
    "# display(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n",
    "# print(\"----------------------------------------------\")\n",
    "# 创建一个scope，执行网络训练\n",
    "with tf.device('/cpu:0'):\n",
    "    with tf.variable_scope(name_or_scope='train', reuse=tf.AUTO_REUSE)  as train_scope:\n",
    "    #     network_train(mnist)\n",
    "        (w1,w2,b1,b2) = network_train(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN模型-重构版\n",
    "\n",
    "重构版会将上面的文件拆成三部分：\n",
    "1. `mnist_inference.py`，定义前向传播过程和神经网络中的参数——也就是网络的结构\n",
    "2. `mnist_train.py`，定义了神经网络的训练过程\n",
    "3. `mnist_eval.py`，定义了神经网络的预测或者测试过程\n",
    "\n",
    "上述这种分离的方式，**可以灵活的更改网络结构，因为网络结构只是定义在`mnist_inference.py`中，后两个文件只是调用最外面的接口**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.constant([[1.0, 2.0, 3.0]\n",
    "                 [4.0, 5.0, 6.0]])\n",
    "x = tf.constant([1.0, 1.0])\n",
    "res = tf.matmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN：LeNet-5模型\n",
    "\n",
    "原始的LeNet-5模型总共有7层（不包含输入层）\n",
    "1. 输入层：32x32x1\n",
    "2. 卷积层：6 个过滤器，步长为 1，不使用0填充，卷积核为 5x5x1x6，输出尺寸为 32-5+1=28，即 28x28x6\n",
    "3. 池化层：窗口大小为 2x2，步长为 2，输出尺寸为 14x14x6\n",
    "4. 卷积层：16 个过滤器，步长为 1，不使用0填充，卷积核为 5x5x6x16，输出尺寸为 14-5+1=10，即 10x10x16\n",
    "5. 池化层：窗口大小为 2x2，步长为 2，输出尺寸为 5x5x6\n",
    "6. 全连接层：输出节点个数为 120\n",
    "7. 全连接层：输出节点个数为 84\n",
    "8. 全连接层；输出节点个数为 10\n",
    "\n",
    "在MNIST数据集上的应用就是：\n",
    "1. 输入层：28x28x1\n",
    "2. 卷积层：32 个过滤器，步长为 1，**使用0填充**，卷积核为 5x5x1x32，输出尺寸为 28x28x32\n",
    "3. 池化层：窗口大小为 2x2，步长为 2，**使用0填充**，输出尺寸为 14x14x32\n",
    "4. 卷积层：64 个过滤器，步长为 1，不使用0填充，卷积核为 5x5x32x64，输出尺寸为 14-5+1=10，即 10x10x64\n",
    "5. 池化层：窗口大小为 2x2，步长为 2，输出尺寸为 5x5x64\n",
    "6. 全连接层：节点个数为 512\n",
    "7. 全连接层：节点个数为 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = mnist.validation.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 草稿  \n",
    "下面这一段是草稿，用于练习正则化和variable scope的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_variable_scope().reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用variable_scope并设置reuse = tf.AUTO_REUSE是为了重复使用tf.get_variable()定义变量\n",
    "with tf.variable_scope(name_or_scope='params', reuse=tf.AUTO_REUSE):\n",
    "    x_batch = tf.placeholder(tf.float32, shape=[None, INPUT_NODE], name='x_batch')\n",
    "    y_batch = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE], name='y_batch')\n",
    "\n",
    "    # 定义正则化函数\n",
    "    reg_l2 = tf.contrib.layers.l2_regularizer(tf.constant(LAMBDA))\n",
    "    # 生成两个隐藏层的参数，并将这两个隐藏层的参数加入到losses集合里\n",
    "    w1 = tf.get_variable(name='w1', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([INPUT_NODE, LAYER_1_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    w2 = tf.get_variable(name='w2', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([LAYER_1_NODE, LAYER_2_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    b1 = tf.get_variable(name='b1', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_1_NODE]))\n",
    "    b2 = tf.get_variable(name='b2', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_2_NODE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(tf.global_variables_initializer())\n",
    "# w1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：\n",
    "> 如何获取variable所属的集合？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "print(reg_variables.__class__)\n",
    "reg_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除某个collection里的内容\n",
    "graph = tf.get_default_graph()\n",
    "graph.clear_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "reg_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-1.15(Python3.6.12)",
   "language": "python",
   "name": "tensorflow1.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
