{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量,计算图和会话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF中主要有如下三个部分：  \n",
    "+ 张量——数据模型  \n",
    "张量也就是多维数组，用于表示数据\n",
    "+ 计算图——计算模型  \n",
    "计算图是整个计算过程的表示，图中的节点表示运算操作，边表示运算操作之间的依赖关系\n",
    "+ 会话——运行模型  \n",
    "计算图中定义的每个运算操作必须在会话中执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow计算图表示如下含义：  \n",
    "+ 每一个节点表示一个计算，或者说操作(operation)\n",
    "+ 节点之间的边描述了计算之间的依赖关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/f1.png\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述的图定义的是如下的运算：  \n",
    "$add = a + b$   \n",
    "对应的TF实现如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_12:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add_8:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "add = tf.add(a,b)\n",
    "print(a)\n",
    "print(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述定义的add是表示加法的这个操作，实际上，a和b也是表示操作——对应于常量赋值这个操作，这就是每个节点是一个计算的含义。 \n",
    "\n",
    "节点a和b经过相加后得到节点add，这就是一个依赖关系。\n",
    "\n",
    "因为是定义的计算节点，所以打印出的结果并不是实际的值。**要想打印出计算节点对应的值，必须要开启一个会话，在会话中进行图的计算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 使用tf.Session()创建一个会话\n",
    "with tf.Session() as sess:\n",
    "    # 使用 session.run()方法实际执行图中的计算节点，传入的参数是某个节点，返回的是该计算节点对应的值\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 以下是一个矩阵乘法的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 定义两个常量赋值操作\n",
    "a = tf.constant([ [3,3] ])\n",
    "b = tf.constant([ [2], [3] ])\n",
    "\n",
    "# 定义一个矩阵乘法操作\n",
    "prod = tf.matmul(a, b)\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> \n",
      " [[15]]\n"
     ]
    }
   ],
   "source": [
    "# 使用会话来运行上述定义的运算\n",
    "with tf.Session() as sess:\n",
    "    # 调用sess.run()方法来执行上述定义的运算\n",
    "    result = sess.run(prod)\n",
    "    print(result.__class__,\"\\n\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量有如下作用:  \n",
    "1. 保存神经网络的参数\n",
    "2. 获取神经网络中间的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两种方式定义变量：  \n",
    "1. `tf.Variable()`，这是一个类的初始化方法\n",
    "2. `tf.get_variable（）`，这是类的静态方法，**推荐使用这个**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意： \n",
    "> `tf.Variable()`创建变量时，如果指定的名称对应于已存在的变量，**会直接覆盖而不提示**；\n",
    "`tf.get_variable()`在遇到同名变量时，**则会报错提醒。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.get_variable()`的参数里有一个`regularizer=`,这个可以传入一个正则化函数，同时将该变量加入到`tf.GraphKeys.REGULARIZATION_LOSSES`这个集合里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 定义一个变量并初始化为0的操作，\n",
    "# 注意，这里只是定义初始化的方法（是用常量初始化还是用随机数初始化），但是并没有执行初始化\n",
    "state = tf.Variable(0, name=\"state\")\n",
    "# 定义一个运算，使var每次加1\n",
    "new_value = tf.add(state, 1)\n",
    "# 定义赋值操作\n",
    "update = tf.assign(state, new_value)\n",
    "# 定义变量初始化操作\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 创建会话运行上述定义的操作\n",
    "with tf.Session() as sess:\n",
    "    # 执行变量的初始化操作\n",
    "    sess.run(init)\n",
    "    print(sess.run(state))\n",
    "    for _ in range(5):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 变量的集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'variables'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.GLOBAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local_variables'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.LOCAL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'losses'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.LOSSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'regularization_losses'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GraphKeys.REGULARIZATION_LOSSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholder机制，Fetch和Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 提供了 placeholder 机制用于提供输入数据。placeholder 相当于定义了一个位置，这个位置中的数据在程序运行时再指定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**placeholder通常用于传入一个batch的训练数据**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch是指一个session中运行多个operation。 \n",
    "Feed是指以字典的形式传入多个数据。  \n",
    "\n",
    "上述两个概念都是`Session.run()`方法的参数，可以查阅该函数的帮助文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.601076   -0.45815668 -0.31606627]\n",
      " [ 0.6450111  -0.37396577 -0.03890442]]\n",
      "[[-3.465588  ]\n",
      " [-0.21963367]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2,3]))\n",
    "\n",
    "# 定义一个placeholder需要提供类型（之后不可改变），shape不是必须的，但是提供了shape可以避免歧义和错误\n",
    "x = tf.placeholder(tf.float32, shape=(3,1), name='x')\n",
    "\n",
    "# 使用placeholder代替常量进行矩阵乘法操作的定义\n",
    "y = tf.matmul(w1, x)\n",
    "\n",
    "# 变量初始化操作\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(w1))\n",
    "    # 执行y定义的操作的时候，需要传入placeholder x对应的值\n",
    "    print(sess.run(y, feed_dict={x:[[1],[2],[3]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_l2 = tf.contrib.layers.l2_regularizer(1.0)\n",
    "w = tf.constant([1.0,2.0,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(w))\n",
    "    print(sess.run(reg_l2(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述得到的结果就是对向量$(1,2,3)$使用L2正则得到的结果：$\\frac{1}{2}\\times 1.0 \\times(1^2+2^2+3^2)=\\frac{1}{2}\\times 1.0 \\times(1+4+9)=7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0320586  0.08714432 0.23688284 0.6439143 ]\n",
      "[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant([1.,2.,3.,4.])\n",
    "res = tf.nn.softmax(w)\n",
    "w_ = np.array([1,2,3,4])\n",
    "res_ = np.exp(w_)/np.exp(w_).sum()\n",
    "\n",
    "print(res.eval())\n",
    "print(res_)\n",
    "print(res.eval().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单使用案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.04929078, 0.098659955]\n",
      "20 [0.099961706, 0.20001897]\n",
      "40 [0.09997764, 0.20001116]\n",
      "60 [0.099986926, 0.20000653]\n",
      "80 [0.09999237, 0.20000382]\n",
      "100 [0.09999553, 0.20000224]\n",
      "120 [0.09999739, 0.20000131]\n",
      "140 [0.09999846, 0.20000076]\n",
      "160 [0.099999085, 0.20000045]\n",
      "180 [0.099999465, 0.20000027]\n",
      "200 [0.09999967, 0.20000017]\n"
     ]
    }
   ],
   "source": [
    "# 构造100个数据点\n",
    "x_data = np.random.rand(100)\n",
    "y_data = x_data * 0.1 + 0.2\n",
    "\n",
    "#构造一个带训练的线性模型\n",
    "a = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "y = a * x_data + b\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化代价函数\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 构建session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(201):\n",
    "        sess.run(train)\n",
    "        if step%20 == 0:\n",
    "            print(step, sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x1463f1b90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取默认的计算图\n",
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的神经网络程序示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三个步骤：  \n",
    "1. 定义神经网络的结构和前向传播的输出结果。\n",
    "2. 定义损失函数以及选择反向传播优化的算法 。\n",
    "3. 生成会话（ tf.Session ）并且在训练、数据上反复运 行反向传播优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为示例的神经网络如下图所示：  \n",
    "**它只有一个隐藏层，并且隐藏层没有使用激活函数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/f2.png\" width=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86375999 0.28490597]\n",
      " [0.07325639 0.7632372 ]\n",
      " [0.45271906 0.54229687]\n",
      " [0.72663578 0.84890511]\n",
      " [0.76819998 0.73314372]]\n",
      "[[0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "# 使用numpy随机模拟数据\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rnd = RandomState(29)\n",
    "datasize = 128\n",
    "X = rnd.rand(datasize,2)\n",
    "Y = [ [int(x1+x2<1)] for (x1,x2) in X]\n",
    "\n",
    "print(X[:5,:])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1\n",
      "[[ 0.06869038  0.49156898  0.11386791]\n",
      " [ 0.8700317  -0.44391954  0.4978401 ]]\n",
      "w2\n",
      "[[-1.084994  ]\n",
      " [ 0.19721599]\n",
      " [-0.9770192 ]]\n",
      "经过 0 次训练，所有数据上的交叉熵损失为：1.19841\n",
      "经过 500 次训练，所有数据上的交叉熵损失为：0.502654\n",
      "经过 1000 次训练，所有数据上的交叉熵损失为：0.195012\n",
      "经过 1500 次训练，所有数据上的交叉熵损失为：0.0943532\n",
      "经过 2000 次训练，所有数据上的交叉熵损失为：0.0556156\n",
      "经过 2500 次训练，所有数据上的交叉熵损失为：0.0371082\n",
      "经过 3000 次训练，所有数据上的交叉熵损失为：0.0267609\n",
      "经过 3500 次训练，所有数据上的交叉熵损失为：0.0202841\n",
      "经过 4000 次训练，所有数据上的交叉熵损失为：0.0158909\n",
      "经过 4500 次训练，所有数据上的交叉熵损失为：0.0127333\n",
      "w1\n",
      "[[-0.86306465  2.570241   -1.2016271 ]\n",
      " [-0.22396241  1.8466288  -1.067801  ]]\n",
      "w2\n",
      "[[-1.0842199]\n",
      " [ 2.594463 ]\n",
      " [-2.4313126]]\n"
     ]
    }
   ],
   "source": [
    "#---------步骤1--------------\n",
    "# 定义训练数据batch大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络的参数\n",
    "# w1是输入层到隐藏层的权重矩阵, 2 x 3\n",
    "w1 = tf.Variable(tf.random_normal([2,3]))\n",
    "# w2是隐藏层到输出层的权重大小，3 x 1\n",
    "w2 = tf.Variable(tf.random_normal([3,1]))\n",
    "\n",
    "#定义placeholder，用于接受数据\n",
    "# shape里第一个维度留None，可以用于动态适应输入的batch大小\n",
    "x_batch = tf.placeholder(tf.float32, shape=(None, 2), name=\"x_batch\")\n",
    "y_batch = tf.placeholder(tf.float32, shape=(None, 1), name=\"y_batch\")\n",
    "\n",
    "\n",
    "#---------步骤2--------------\n",
    "#定义神经网络的前向传播过程，这里采用矩阵运算\n",
    "a = tf.matmul(x_batch, w1)\n",
    "y = tf.matmul(a, w2)  # 这个 y 是神经网络输出的 y\n",
    "\n",
    "# 定义损失函数和反向传播的优化算法\n",
    "y = tf.sigmoid(y)\n",
    "# 使用交叉熵作为损失函数\n",
    "cross_entropy = - tf.reduce_mean(\n",
    "    y_batch * tf.log(tf.clip_by_value(y, 1e-10, 10)) +\n",
    "    (1-y_batch) * tf.log(tf.clip_by_value(y,1e-10, 10))\n",
    ")\n",
    "# 优化算法\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
    "\n",
    "#---------步骤3--------------\n",
    "with tf.Session() as sess:\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # 打印训练之前权重矩阵的值\n",
    "    print(\"w1\")\n",
    "    print(sess.run(w1))\n",
    "    print(\"w2\")\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    # 设定训练轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        # 每次选择batch_size个样本进行运算，这里生成的是样本的index范围\n",
    "        batch_start = (i * batch_size) % datasize\n",
    "        batch_end = min(batch_start+batch_size, datasize)\n",
    "        \n",
    "        # 对每个batch进行训练并更新参数\n",
    "        sess.run(train_step, feed_dict={x_batch:X[batch_start:batch_end,:], y_batch:Y[batch_start:batch_end]})\n",
    "        \n",
    "        # 每隔一段时间就打印一次训练过程中全部数据集上的交叉熵\n",
    "        if i%500 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy , feed_dict={x_batch:X, y_batch:Y})\n",
    "            print(\"经过 {} 次训练，所有数据上的交叉熵损失为：{:g}\".format(i, total_cross_entropy))\n",
    "            \n",
    "    # 打印训练之后权重矩阵的值\n",
    "    print(\"w1\")\n",
    "    print(sess.run(w1))\n",
    "    print(\"w2\")\n",
    "    print(sess.run(w2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST手写数据集识别练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/danielzhang/Documents/Python-Projects/DataAnalysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/danielzhang/Documents/Python-Projects/DataAnalysis/Practice\n"
     ]
    }
   ],
   "source": [
    "cd ./Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-400bd63c3c13>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/danielzhang/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/danielzhang/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/danielzhang/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/danielzhang/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/danielzhang/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./datasets/MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.num_examples)\n",
    "print(mnist.validation.num_examples)\n",
    "print(mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape (55000, 784)\n",
      "train_labels.shape (55000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 每一张图片是 28*28 像素，被拉成了一个 28*28=784的行向量\n",
    "print(\"train.shape\",mnist.train.images.shape)\n",
    "\n",
    "# 每张图片对应的分类是一个10维向量，对应于10个分类\n",
    "print(\"train_labels.shape\",mnist.train.labels.shape)\n",
    "\n",
    "# print(mnist.train.images[0])\n",
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a40499f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[2].reshape(28,28))\n",
    "# px.imshow(mnist.train.images[2].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里构建一个 **输入层 + 隐藏层1 + 隐藏层2 + softmax层**，一共4层的神经网络，使用的内容如下：  \n",
    "+ 每一层的激活函数使用ReLU函数\n",
    "+ 损失函数使用L2正则项\n",
    "+ 使用衰减的指数学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF中用于交互下的默认会话\n",
    "sess = tf.InteractiveSession()\n",
    "# tf.InteractiveSession.close(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置相关参数\n",
    "\n",
    "# 输入层节点数\n",
    "INPUT_NODE = 784\n",
    "# 输出层节点数\n",
    "OUTPUT_NODE = 10\n",
    "# 隐藏层1的节点数\n",
    "LAYER_1_NODE = 500\n",
    "# 隐藏层2的节点数\n",
    "LAYER_2_NODE = 10\n",
    "\n",
    "# batch大小\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# 训练轮次\n",
    "TRAIN_STEPS = 3000\n",
    "\n",
    "# 基础学习率和衰减率\n",
    "LEARN_RATE = 0.8\n",
    "DECAY_RATE = 0.9\n",
    "\n",
    "# 正则化系数\n",
    "LAMBDA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_calculate(input_tensor, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    用于计算网络前向传播得到的结果,不包括softmax层\n",
    "    \"\"\"\n",
    "    # 隐藏层1的结果，使用了ReLU激活函数\n",
    "    layer1 = tf.nn.relu(tf.matmul(input_tensor, w1) + b1)\n",
    "    # 隐藏层2的结果，仍然使用ReLU\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "    # softmax层\n",
    "    # softmax = tf.nn.softmax(layer2)\n",
    "    # 返回softmax层\n",
    "    #return softmax\n",
    "    # 这里不直接返回softmax层的结果，而是返回进行softmax之前的值\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_predict(input_tensor, w1, b1, w2, b2):\n",
    "    \"\"\"\n",
    "    用于计算网络前向传播得到的结果,这个返回的是softmax层之后的结果\n",
    "    \"\"\"\n",
    "    # 隐藏层1的结果，使用了ReLU激活函数\n",
    "    layer1 = tf.nn.relu(tf.matmul(input_tensor, w1) + b1)\n",
    "    # 隐藏层2的结果，仍然使用ReLU\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "    # softmax层\n",
    "    softmax = tf.nn.softmax(layer2)\n",
    "    # 返回softmax层\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_train(mnist):\n",
    "    \"\"\"\n",
    "    用于训练网络的函数\n",
    "    \"\"\"\n",
    "    x_batch = tf.placeholder(tf.float32, shape=[None, INPUT_NODE], name='x_batch')\n",
    "    y_batch = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE], name='y_batch')\n",
    "    \n",
    "    # 定义正则化函数\n",
    "    reg_l2 = tf.contrib.layers.l2_regularizer(LAMBDA)\n",
    "    # 定义两个隐藏层的参数，并将这两个隐藏层的参数加入到losses集合里\n",
    "    w1 = tf.get_variable(name='w1', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([INPUT_NODE, LAYER_1_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    w2 = tf.get_variable(name='w2', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([LAYER_1_NODE, LAYER_2_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    # 注意，一般不对偏置项做正则化，所以不用将它们加入到对应的loss集合里\n",
    "    b1 = tf.get_variable(name='b1', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_1_NODE]))\n",
    "    b2 = tf.get_variable(name='b2', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_2_NODE]))\n",
    "    \n",
    "    # 定义网络前向传播结果的计算，注意，这里得到的结果不包括softmax层\n",
    "    y_pred = network_calculate(x_batch,w1,b1,w2,b2)\n",
    "    \n",
    "    # 定义交叉熵损失函数的计算，这里使用TF自带的损失函数，它在计算的时候会包括softmax这一步\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_batch, logits=y_pred)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失\n",
    "    # loss = cross_entropy_mean + reg_l2(w1) + reg_l2(w2)\n",
    "    # 提取出经过正则化的变量集合\n",
    "    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    # 总的损失函数\n",
    "    loss = cross_entropy_mean + tf.add_n(reg_variables)\n",
    "    \n",
    "    # 定义指数衰减的学习率\n",
    "    # 定义全局的步数，这个变量不需要训练，所以不加入train集合\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=LEARN_RATE, global_step=global_step,\n",
    "                                               decay_steps=mnist.train.num_examples/BATCH_SIZE,\n",
    "                                               decay_rate=DECAY_RATE)\n",
    "    \n",
    "    # 定义梯度学习的优化器\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "                   .minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 初始化会话并开始训练\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for step in range(TRAIN_STEPS):\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_step, feed_dict={x_batch:xs, y_batch:ys})\n",
    "            \n",
    "            # 每 500 轮输出一次在验证集上的loss\n",
    "            feed_valid = {x_batch:mnist.validation.images, y_batch:mnist.validation.labels}\n",
    "            if (step+1) % 200 == 0:\n",
    "                valid_loss = sess.run(loss, feed_dict=feed_valid)\n",
    "                print(\"经过 {} 轮训练后的在验证集上的交叉熵误差为：{:g}\".format(step+1, valid_loss))\n",
    "                \n",
    "    return w1,w2,b1,b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经过 200 轮训练后的在验证集上的交叉熵误差为：44.8363\n",
      "经过 400 轮训练后的在验证集上的交叉熵误差为：43.5702\n",
      "经过 600 轮训练后的在验证集上的交叉熵误差为：42.3873\n",
      "经过 800 轮训练后的在验证集上的交叉熵误差为：41.2809\n",
      "经过 1000 轮训练后的在验证集上的交叉熵误差为：40.245\n",
      "经过 1200 轮训练后的在验证集上的交叉熵误差为：39.274\n",
      "经过 1400 轮训练后的在验证集上的交叉熵误差为：38.3629\n",
      "经过 1600 轮训练后的在验证集上的交叉熵误差为：37.5074\n",
      "经过 1800 轮训练后的在验证集上的交叉熵误差为：36.7031\n",
      "经过 2000 轮训练后的在验证集上的交叉熵误差为：35.9464\n",
      "经过 2200 轮训练后的在验证集上的交叉熵误差为：35.2339\n",
      "经过 2400 轮训练后的在验证集上的交叉熵误差为：34.5625\n",
      "经过 2600 轮训练后的在验证集上的交叉熵误差为：33.9292\n",
      "经过 2800 轮训练后的在验证集上的交叉熵误差为：33.3314\n",
      "经过 3000 轮训练后的在验证集上的交叉熵误差为：32.7668\n"
     ]
    }
   ],
   "source": [
    "# 这里必须要限定scope，否则只能运行一次\n",
    "with tf.variable_scope(name_or_scope='train', reuse=tf.AUTO_REUSE):\n",
    "    (w1,w2,b1,b2) = network_train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value train/w1\n\t [[{{node _retval_train/w1_0_0}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value train/w1\n\t [[{{node _retval_train/w1_0_0}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0edcf7ea8cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value train/w1\n\t [[{{node _retval_train/w1_0_0}}]]"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 草稿  \n",
    "下面这一段是草稿，用于练习正则化和variable scope的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_variable_scope().reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用variable_scope并设置reuse = tf.AUTO_REUSE是为了重复使用tf.get_variable()定义变量\n",
    "with tf.variable_scope(name_or_scope='params', reuse=tf.AUTO_REUSE):\n",
    "    x_batch = tf.placeholder(tf.float32, shape=[None, INPUT_NODE], name='x_batch')\n",
    "    y_batch = tf.placeholder(tf.float32, shape=[None, OUTPUT_NODE], name='y_batch')\n",
    "\n",
    "    # 定义正则化函数\n",
    "    reg_l2 = tf.contrib.layers.l2_regularizer(tf.constant(LAMBDA))\n",
    "    # 生成两个隐藏层的参数，并将这两个隐藏层的参数加入到losses集合里\n",
    "    w1 = tf.get_variable(name='w1', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([INPUT_NODE, LAYER_1_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    w2 = tf.get_variable(name='w2', dtype=tf.float32, \n",
    "                        initializer=tf.random_normal([LAYER_1_NODE, LAYER_2_NODE]),\n",
    "                        regularizer=reg_l2)\n",
    "    b1 = tf.get_variable(name='b1', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_1_NODE]))\n",
    "    b2 = tf.get_variable(name='b2', dtype=tf.float32,\n",
    "                        initializer=tf.constant(0.1, shape=[LAYER_2_NODE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(tf.global_variables_initializer())\n",
    "# w1.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问题：\n",
    "> 如何获取variable所属的集合？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "print(reg_variables.__class__)\n",
    "reg_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除某个collection里的内容\n",
    "graph = tf.get_default_graph()\n",
    "graph.clear_collection(tf.GraphKeys.REGULARIZATION_LOSSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "reg_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
