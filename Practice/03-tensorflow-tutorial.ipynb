{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow的基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量,计算图和会话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF中主要有如下三个部分：  \n",
    "+ 张量——数据模型  \n",
    "张量也就是多维数组，用于表示数据\n",
    "+ 计算图——计算模型  \n",
    "计算图是整个计算过程的表示，图中的节点表示运算操作，边表示运算操作之间的依赖关系\n",
    "+ 会话——运行模型  \n",
    "计算图中定义的每个运算操作必须在会话中执行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow计算图表示如下含义：  \n",
    "+ 每一个节点表示一个计算，或者说操作(operation)\n",
    "+ 节点之间的边描述了计算之间的依赖关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/f1.png\" width=\"40%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述的图定义的是如下的运算：  \n",
    "$add = a + b$   \n",
    "对应的TF实现如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_12:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Add_8:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "add = tf.add(a,b)\n",
    "print(a)\n",
    "print(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述定义的add是表示加法的这个操作，实际上，a和b也是表示操作——对应于常量赋值这个操作，这就是每个节点是一个计算的含义。 \n",
    "\n",
    "节点a和b经过相加后得到节点add，这就是一个依赖关系。\n",
    "\n",
    "因为是定义的计算节点，所以打印出的结果并不是实际的值。**要想打印出计算节点对应的值，必须要开启一个会话，在会话中进行图的计算**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 使用tf.Session()创建一个会话\n",
    "with tf.Session() as sess:\n",
    "    # 使用 session.run()方法实际执行图中的计算节点，传入的参数是某个节点，返回的是该计算节点对应的值\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 以下是一个矩阵乘法的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 定义两个常量赋值操作\n",
    "a = tf.constant([ [3,3] ])\n",
    "b = tf.constant([ [2], [3] ])\n",
    "\n",
    "# 定义一个矩阵乘法操作\n",
    "prod = tf.matmul(a, b)\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> \n",
      " [[15]]\n"
     ]
    }
   ],
   "source": [
    "# 使用会话来运行上述定义的运算\n",
    "with tf.Session() as sess:\n",
    "    # 调用sess.run()方法来执行上述定义的运算\n",
    "    result = sess.run(prod)\n",
    "    print(result.__class__,\"\\n\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量有如下作用:  \n",
    "1. 保存神经网络的参数\n",
    "2. 获取神经网络中间的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 定义一个变量并初始化为0的操作，\n",
    "# 注意，这里只是定义初始化的方法（是用常量初始化还是用随机数初始化），但是并没有执行初始化\n",
    "state = tf.Variable(0, name=\"state\")\n",
    "# 定义一个运算，使var每次加1\n",
    "new_value = tf.add(state, 1)\n",
    "# 定义赋值操作\n",
    "update = tf.assign(state, new_value)\n",
    "# 定义变量初始化操作\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 创建会话运行上述定义的操作\n",
    "with tf.Session() as sess:\n",
    "    # 执行变量的初始化操作\n",
    "    sess.run(init)\n",
    "    print(sess.run(state))\n",
    "    for _ in range(5):\n",
    "        sess.run(update)\n",
    "        print(sess.run(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## placeholder机制，Fetch和Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 提供了 placeholder 机制用于提供输入数据。placeholder 相当于定义了一个位置，这个位置中的数据在程序运行时再指定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**placeholder通常用于传入一个batch的训练数据**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch是指一个session中运行多个operation。 \n",
    "Feed是指以字典的形式传入多个数据。  \n",
    "\n",
    "上述两个概念都是`Session.run()`方法的参数，可以查阅该函数的帮助文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.601076   -0.45815668 -0.31606627]\n",
      " [ 0.6450111  -0.37396577 -0.03890442]]\n",
      "[[-3.465588  ]\n",
      " [-0.21963367]]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(tf.random_normal([2,3]))\n",
    "\n",
    "# 定义一个placeholder需要提供类型（之后不可改变），shape不是必须的，但是提供了shape可以避免歧义和错误\n",
    "x = tf.placeholder(tf.float32, shape=(3,1), name='x')\n",
    "\n",
    "# 使用placeholder代替常量进行矩阵乘法操作的定义\n",
    "y = tf.matmul(w1, x)\n",
    "\n",
    "# 变量初始化操作\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(w1))\n",
    "    # 执行y定义的操作的时候，需要传入placeholder x对应的值\n",
    "    print(sess.run(y, feed_dict={x:[[1],[2],[3]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单使用案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.04929078, 0.098659955]\n",
      "20 [0.099961706, 0.20001897]\n",
      "40 [0.09997764, 0.20001116]\n",
      "60 [0.099986926, 0.20000653]\n",
      "80 [0.09999237, 0.20000382]\n",
      "100 [0.09999553, 0.20000224]\n",
      "120 [0.09999739, 0.20000131]\n",
      "140 [0.09999846, 0.20000076]\n",
      "160 [0.099999085, 0.20000045]\n",
      "180 [0.099999465, 0.20000027]\n",
      "200 [0.09999967, 0.20000017]\n"
     ]
    }
   ],
   "source": [
    "# 构造100个数据点\n",
    "x_data = np.random.rand(100)\n",
    "y_data = x_data * 0.1 + 0.2\n",
    "\n",
    "#构造一个带训练的线性模型\n",
    "a = tf.Variable(0.)\n",
    "b = tf.Variable(0.)\n",
    "y = a * x_data + b\n",
    "\n",
    "# 二次代价函数\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.2)\n",
    "\n",
    "#最小化代价函数\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 构建session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in range(201):\n",
    "        sess.run(train)\n",
    "        if step%20 == 0:\n",
    "            print(step, sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x1463f1b90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取默认的计算图\n",
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的神经网络程序示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三个步骤：  \n",
    "1. 定义神经网络的结构和前向传播的输出结果。\n",
    "2. 定义损失函数以及选择反向传播优化的算法 。\n",
    "3. 生成会话（ tf.Session ）并且在训练、数据上反复运 行反向传播优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为示例的神经网络如下图所示：  \n",
    "**它只有一个隐藏层，并且隐藏层没有使用激活函数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/f2.png\" width=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.86375999 0.28490597]\n",
      " [0.07325639 0.7632372 ]\n",
      " [0.45271906 0.54229687]\n",
      " [0.72663578 0.84890511]\n",
      " [0.76819998 0.73314372]]\n",
      "[[0], [1], [1], [0], [0], [1], [1], [1], [1], [0], [1], [0], [0], [0], [0], [1], [0], [1], [0], [1], [0], [0], [0], [0], [0], [1], [1], [1], [0], [0], [1], [0], [1], [0], [1], [1], [0], [1], [1], [1], [0], [0], [0], [0], [1], [0], [1], [1], [1], [1], [0], [0], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [0], [1], [0], [1], [0], [0], [1], [1], [0], [0], [0], [1], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [1], [0], [0], [0], [1], [1], [0], [1], [1], [0], [1], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1], [0], [0], [0], [1], [1], [1], [1], [1], [0], [0], [0], [0], [1], [1], [1], [0], [1], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "# 使用numpy随机模拟数据\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rnd = RandomState(29)\n",
    "datasize = 128\n",
    "X = rnd.rand(datasize,2)\n",
    "Y = [ [int(x1+x2<1)] for (x1,x2) in X]\n",
    "\n",
    "print(X[:5,:])\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1\n",
      "[[ 0.06869038  0.49156898  0.11386791]\n",
      " [ 0.8700317  -0.44391954  0.4978401 ]]\n",
      "w2\n",
      "[[-1.084994  ]\n",
      " [ 0.19721599]\n",
      " [-0.9770192 ]]\n",
      "经过 0 次训练，所有数据上的交叉熵损失为：1.19841\n",
      "经过 500 次训练，所有数据上的交叉熵损失为：0.502654\n",
      "经过 1000 次训练，所有数据上的交叉熵损失为：0.195012\n",
      "经过 1500 次训练，所有数据上的交叉熵损失为：0.0943532\n",
      "经过 2000 次训练，所有数据上的交叉熵损失为：0.0556156\n",
      "经过 2500 次训练，所有数据上的交叉熵损失为：0.0371082\n",
      "经过 3000 次训练，所有数据上的交叉熵损失为：0.0267609\n",
      "经过 3500 次训练，所有数据上的交叉熵损失为：0.0202841\n",
      "经过 4000 次训练，所有数据上的交叉熵损失为：0.0158909\n",
      "经过 4500 次训练，所有数据上的交叉熵损失为：0.0127333\n",
      "w1\n",
      "[[-0.86306465  2.570241   -1.2016271 ]\n",
      " [-0.22396241  1.8466288  -1.067801  ]]\n",
      "w2\n",
      "[[-1.0842199]\n",
      " [ 2.594463 ]\n",
      " [-2.4313126]]\n"
     ]
    }
   ],
   "source": [
    "#---------步骤1--------------\n",
    "# 定义训练数据batch大小\n",
    "batch_size = 8\n",
    "\n",
    "# 定义神经网络的参数\n",
    "# w1是输入层到隐藏层的权重矩阵, 2 x 3\n",
    "w1 = tf.Variable(tf.random_normal([2,3]))\n",
    "# w2是隐藏层到输出层的权重大小，3 x 1\n",
    "w2 = tf.Variable(tf.random_normal([3,1]))\n",
    "\n",
    "#定义placeholder，用于接受数据\n",
    "# shape里第一个维度留None，可以用于动态适应输入的batch大小\n",
    "x_batch = tf.placeholder(tf.float32, shape=(None, 2), name=\"x_batch\")\n",
    "y_batch = tf.placeholder(tf.float32, shape=(None, 1), name=\"y_batch\")\n",
    "\n",
    "\n",
    "#---------步骤2--------------\n",
    "#定义神经网络的前向传播过程，这里采用矩阵运算\n",
    "a = tf.matmul(x_batch, w1)\n",
    "y = tf.matmul(a, w2)  # 这个 y 是神经网络输出的 y\n",
    "\n",
    "# 定义损失函数和反向传播的优化算法\n",
    "y = tf.sigmoid(y)\n",
    "# 使用交叉熵作为损失函数\n",
    "cross_entropy = - tf.reduce_mean(\n",
    "    y_batch * tf.log(tf.clip_by_value(y, 1e-10, 10)) +\n",
    "    (1-y_batch) * tf.log(tf.clip_by_value(y,1e-10, 10))\n",
    ")\n",
    "# 优化算法\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cross_entropy)\n",
    "\n",
    "#---------步骤3--------------\n",
    "with tf.Session() as sess:\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    # 打印训练之前权重矩阵的值\n",
    "    print(\"w1\")\n",
    "    print(sess.run(w1))\n",
    "    print(\"w2\")\n",
    "    print(sess.run(w2))\n",
    "    \n",
    "    # 设定训练轮数\n",
    "    STEPS = 5000\n",
    "    for i in range(STEPS):\n",
    "        # 每次选择batch_size个样本进行运算，这里生成的是样本的index范围\n",
    "        batch_start = (i * batch_size) % datasize\n",
    "        batch_end = min(batch_start+batch_size, datasize)\n",
    "        \n",
    "        # 对每个batch进行训练并更新参数\n",
    "        sess.run(train_step, feed_dict={x_batch:X[batch_start:batch_end,:], y_batch:Y[batch_start:batch_end]})\n",
    "        \n",
    "        # 每隔一段时间就打印一次训练过程中全部数据集上的交叉熵\n",
    "        if i%500 == 0:\n",
    "            total_cross_entropy = sess.run(cross_entropy , feed_dict={x_batch:X, y_batch:Y})\n",
    "            print(\"经过 {} 次训练，所有数据上的交叉熵损失为：{:g}\".format(i, total_cross_entropy))\n",
    "            \n",
    "    # 打印训练之后权重矩阵的值\n",
    "    print(\"w1\")\n",
    "    print(sess.run(w1))\n",
    "    print(\"w2\")\n",
    "    print(sess.run(w2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 深层的神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST手写数据集识别练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./datasets/MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n",
      "5000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.num_examples)\n",
    "print(mnist.validation.num_examples)\n",
    "print(mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape (55000, 784)\n",
      "train_labels.shape (55000, 10)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 每一张图片是 28*28 像素，被拉成了一个 28*28=784的行向量\n",
    "print(\"train.shape\",mnist.train.images.shape)\n",
    "\n",
    "# 每张图片对应的分类是一个10维向量，对应于10个分类\n",
    "print(\"train_labels.shape\",mnist.train.labels.shape)\n",
    "\n",
    "# print(mnist.train.images[0])\n",
    "print(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.train.next_batch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
